{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0b2ad9ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pygad\n",
    "import wandb\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.client import device_lib\n",
    "# print(device_lib.list_local_devices())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b264bf",
   "metadata": {},
   "source": [
    "# Creating the Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b390b206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (25050, 10)\n",
      "y_train shape: (25050,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Set the number of classes, dimensions, and samples per class\n",
    "num_classes = 5\n",
    "num_dimensions = 10  # Set the dataset to 3 dimensions\n",
    "num_samples_per_class = 5000\n",
    "num_outliers = 10\n",
    "cluster_std_factor = 1.0  # Adjust this factor as needed\n",
    "outlier_magnitude_factor = 10\n",
    "\n",
    "# Generate random data for each class\n",
    "datasets = []\n",
    "for i in range(num_classes):\n",
    "    data, _ = make_blobs(n_samples=num_samples_per_class, n_features=num_dimensions, centers=1, cluster_std=cluster_std_factor, random_state=i)\n",
    "    datasets.append(data)\n",
    "\n",
    "# Add outliers\n",
    "outliers = []\n",
    "outlier_data = []\n",
    "for i in range(num_classes):\n",
    "    # Generate outliers close to one class\n",
    "    outlier_class = np.random.randint(0, num_classes)\n",
    "    outlier_samples = np.random.rand(num_outliers, num_dimensions) * outlier_magnitude_factor\n",
    "    outlier_samples += datasets[outlier_class][:num_outliers]  # Add outliers close to a class\n",
    "    outlier_data.append(outlier_samples)\n",
    "outliers.append(outlier_data)\n",
    "\n",
    "# Combine data and outliers, and add binary outlier column\n",
    "final_datasets = []\n",
    "for i in range(num_classes):\n",
    "    data = datasets[i]\n",
    "    outliers_data = outliers[0][i]\n",
    "    \n",
    "    # Add binary outlier column (1 for outliers, 0 for non-outliers)\n",
    "    data = np.column_stack((data, np.zeros(len(data))))\n",
    "    outliers_data = np.column_stack((outliers_data, np.ones(len(outliers_data))))\n",
    "    \n",
    "    data_with_outliers = np.vstack((data, outliers_data))\n",
    "    final_datasets.append(data_with_outliers)\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "combined_dataset = np.vstack(final_datasets)\n",
    "combined_dataset = shuffle(combined_dataset, random_state=42)\n",
    "\n",
    "# Create the training set (all samples) and target set (classification assignments)\n",
    "X_train = combined_dataset[:, :-1]  # Features (all dimensions except the outlier flag)\n",
    "y_train = combined_dataset[:, -1]   # Target (outlier flag)\n",
    "\n",
    "# Print the shapes of the training set and target set\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087b86a8",
   "metadata": {},
   "source": [
    "# Training the RAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "31cf2a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n",
      "196/196 [==============================] - 2s 5ms/step - loss: 29.4841 - reconstruction_loss: 28.9224 - z_loss: 0.0416 - REG_loss: 0.0023\n",
      "Epoch 2/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 28.5334 - reconstruction_loss: 28.4489 - z_loss: 0.0065 - REG_loss: 0.0018\n",
      "Epoch 3/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 28.3763 - reconstruction_loss: 28.3930 - z_loss: 0.0944 - REG_loss: 0.0469\n",
      "Epoch 4/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 26.9336 - reconstruction_loss: 24.9834 - z_loss: 0.3885 - REG_loss: 0.9135\n",
      "Epoch 5/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 20.4569 - reconstruction_loss: 19.8884 - z_loss: 0.4399 - REG_loss: 1.7412\n",
      "Epoch 6/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 19.3836 - reconstruction_loss: 19.2830 - z_loss: 0.4431 - REG_loss: 0.6808\n",
      "Epoch 7/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 19.3272 - reconstruction_loss: 19.2510 - z_loss: 0.4437 - REG_loss: 0.3133\n",
      "Epoch 8/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 19.3125 - reconstruction_loss: 19.2486 - z_loss: 0.4439 - REG_loss: 0.1631\n",
      "Epoch 9/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 19.3139 - reconstruction_loss: 19.2509 - z_loss: 0.4439 - REG_loss: 0.0752\n",
      "Epoch 10/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 19.3123 - reconstruction_loss: 19.2490 - z_loss: 0.4437 - REG_loss: 0.0357\n",
      "Epoch 11/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 19.2595 - reconstruction_loss: 19.2500 - z_loss: 0.4433 - REG_loss: 0.0207\n",
      "Epoch 12/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 19.2881 - reconstruction_loss: 19.2486 - z_loss: 0.4427 - REG_loss: 0.0144\n",
      "Epoch 13/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 19.2409 - reconstruction_loss: 19.2497 - z_loss: 0.4418 - REG_loss: 0.0113\n",
      "Epoch 14/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 19.3131 - reconstruction_loss: 19.2503 - z_loss: 0.4405 - REG_loss: 0.0090\n",
      "Epoch 15/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 19.3021 - reconstruction_loss: 19.2484 - z_loss: 0.4384 - REG_loss: 0.0073\n",
      "Epoch 16/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 19.2314 - reconstruction_loss: 19.2497 - z_loss: 0.4347 - REG_loss: 0.0064\n",
      "Epoch 17/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 19.2722 - reconstruction_loss: 19.2492 - z_loss: 0.4260 - REG_loss: 0.0057\n",
      "Epoch 18/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 19.2496 - reconstruction_loss: 19.2497 - z_loss: 0.3820 - REG_loss: 0.0092\n",
      "Epoch 19/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 19.2414 - reconstruction_loss: 19.1993 - z_loss: 0.3133 - REG_loss: 0.1675\n",
      "Epoch 20/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 18.8212 - reconstruction_loss: 18.5187 - z_loss: 0.3696 - REG_loss: 0.4047\n",
      "Epoch 21/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 17.4705 - reconstruction_loss: 17.1137 - z_loss: 0.3831 - REG_loss: 1.2054\n",
      "Epoch 22/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 16.4981 - reconstruction_loss: 16.1950 - z_loss: 0.3849 - REG_loss: 2.8868\n",
      "Epoch 23/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 16.0659 - reconstruction_loss: 15.6903 - z_loss: 0.3861 - REG_loss: 3.3856\n",
      "Epoch 24/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 15.4186 - reconstruction_loss: 15.2801 - z_loss: 0.3867 - REG_loss: 0.5630\n",
      "Epoch 25/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 15.0673 - reconstruction_loss: 14.9375 - z_loss: 0.3872 - REG_loss: 1.3160\n",
      "Epoch 26/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 14.7127 - reconstruction_loss: 14.6803 - z_loss: 0.3877 - REG_loss: 0.5940\n",
      "Epoch 27/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 14.9430 - reconstruction_loss: 14.5078 - z_loss: 0.3875 - REG_loss: 3.9907\n",
      "Epoch 28/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 14.7065 - reconstruction_loss: 14.3928 - z_loss: 0.3882 - REG_loss: 9.6382\n",
      "Epoch 29/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 14.7312 - reconstruction_loss: 14.3446 - z_loss: 0.3896 - REG_loss: 4.7907\n",
      "Epoch 30/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 14.3191 - reconstruction_loss: 14.2420 - z_loss: 0.3896 - REG_loss: 0.8445\n",
      "Epoch 31/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 14.2234 - reconstruction_loss: 14.1720 - z_loss: 0.3898 - REG_loss: 0.7345\n",
      "Epoch 32/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 14.1029 - reconstruction_loss: 14.1145 - z_loss: 0.3899 - REG_loss: 0.6529\n",
      "Epoch 33/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 14.1043 - reconstruction_loss: 14.0655 - z_loss: 0.3900 - REG_loss: 0.6128\n",
      "Epoch 34/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9329 - reconstruction_loss: 14.0237 - z_loss: 0.3901 - REG_loss: 0.6025\n",
      "Epoch 35/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 14.1361 - reconstruction_loss: 13.9920 - z_loss: 0.3900 - REG_loss: 1.2791\n",
      "Epoch 36/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9946 - reconstruction_loss: 13.9655 - z_loss: 0.3901 - REG_loss: 1.2102\n",
      "Epoch 37/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 14.0114 - reconstruction_loss: 13.9461 - z_loss: 0.3900 - REG_loss: 0.5780\n",
      "Epoch 38/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9519 - reconstruction_loss: 13.9302 - z_loss: 0.3900 - REG_loss: 3.0112\n",
      "Epoch 39/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 14.0349 - reconstruction_loss: 13.9256 - z_loss: 0.3897 - REG_loss: 1.1915\n",
      "Epoch 40/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.9990 - reconstruction_loss: 13.9149 - z_loss: 0.3897 - REG_loss: 0.5408\n",
      "Epoch 41/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 14.0544 - reconstruction_loss: 13.9080 - z_loss: 0.3898 - REG_loss: 0.4740\n",
      "Epoch 42/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.9107 - reconstruction_loss: 13.9021 - z_loss: 0.3898 - REG_loss: 0.4376\n",
      "Epoch 43/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.9487 - reconstruction_loss: 13.8968 - z_loss: 0.3898 - REG_loss: 0.3982\n",
      "Epoch 44/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9933 - reconstruction_loss: 13.8952 - z_loss: 0.3899 - REG_loss: 0.6921\n",
      "Epoch 45/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9458 - reconstruction_loss: 13.8932 - z_loss: 0.3897 - REG_loss: 0.7601\n",
      "Epoch 46/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8588 - reconstruction_loss: 13.8921 - z_loss: 0.3897 - REG_loss: 1.4210\n",
      "Epoch 47/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8880 - reconstruction_loss: 13.8920 - z_loss: 0.3896 - REG_loss: 0.4436\n",
      "Epoch 48/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9065 - reconstruction_loss: 13.8910 - z_loss: 0.3896 - REG_loss: 0.3119\n",
      "Epoch 49/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8985 - reconstruction_loss: 13.8925 - z_loss: 0.3896 - REG_loss: 0.3554\n",
      "Epoch 50/100\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 13.8981 - reconstruction_loss: 13.8911 - z_loss: 0.3896 - REG_loss: 0.3807\n",
      "Epoch 51/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9866 - reconstruction_loss: 13.8908 - z_loss: 0.3895 - REG_loss: 1.7126\n",
      "Epoch 52/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 1s 4ms/step - loss: 13.9334 - reconstruction_loss: 13.8899 - z_loss: 0.3893 - REG_loss: 0.2747\n",
      "Epoch 53/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.8826 - reconstruction_loss: 13.8908 - z_loss: 0.3894 - REG_loss: 0.2454\n",
      "Epoch 54/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9432 - reconstruction_loss: 13.8897 - z_loss: 0.3894 - REG_loss: 0.2443\n",
      "Epoch 55/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8984 - reconstruction_loss: 13.8893 - z_loss: 0.3893 - REG_loss: 0.4244\n",
      "Epoch 56/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8700 - reconstruction_loss: 13.8912 - z_loss: 0.3893 - REG_loss: 0.2149\n",
      "Epoch 57/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8967 - reconstruction_loss: 13.8895 - z_loss: 0.3892 - REG_loss: 0.8423\n",
      "Epoch 58/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8797 - reconstruction_loss: 13.8899 - z_loss: 0.3892 - REG_loss: 0.3065\n",
      "Epoch 59/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8841 - reconstruction_loss: 13.8901 - z_loss: 0.3891 - REG_loss: 0.1885\n",
      "Epoch 60/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9049 - reconstruction_loss: 13.8889 - z_loss: 0.3891 - REG_loss: 0.4224\n",
      "Epoch 61/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9422 - reconstruction_loss: 13.8900 - z_loss: 0.3890 - REG_loss: 0.5889\n",
      "Epoch 62/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.8922 - reconstruction_loss: 13.8906 - z_loss: 0.3890 - REG_loss: 0.1632\n",
      "Epoch 63/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8579 - reconstruction_loss: 13.8904 - z_loss: 0.3890 - REG_loss: 0.1595\n",
      "Epoch 64/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9908 - reconstruction_loss: 13.8899 - z_loss: 0.3889 - REG_loss: 0.5626\n",
      "Epoch 65/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.9556 - reconstruction_loss: 13.8898 - z_loss: 0.3888 - REG_loss: 0.1429\n",
      "Epoch 66/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9399 - reconstruction_loss: 13.8906 - z_loss: 0.3888 - REG_loss: 0.2490\n",
      "Epoch 67/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9220 - reconstruction_loss: 13.8905 - z_loss: 0.3888 - REG_loss: 0.1608\n",
      "Epoch 68/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 14.1015 - reconstruction_loss: 13.8898 - z_loss: 0.3885 - REG_loss: 0.9435\n",
      "Epoch 69/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9916 - reconstruction_loss: 13.8884 - z_loss: 0.3885 - REG_loss: 0.1364\n",
      "Epoch 70/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.9248 - reconstruction_loss: 13.8903 - z_loss: 0.3885 - REG_loss: 0.1241\n",
      "Epoch 71/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.8754 - reconstruction_loss: 13.8894 - z_loss: 0.3885 - REG_loss: 0.1225\n",
      "Epoch 72/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9095 - reconstruction_loss: 13.8898 - z_loss: 0.3885 - REG_loss: 0.1549\n",
      "Epoch 73/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9371 - reconstruction_loss: 13.8905 - z_loss: 0.3884 - REG_loss: 0.3847\n",
      "Epoch 74/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8616 - reconstruction_loss: 13.8893 - z_loss: 0.3883 - REG_loss: 0.1132\n",
      "Epoch 75/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9166 - reconstruction_loss: 13.8904 - z_loss: 0.3882 - REG_loss: 0.5782\n",
      "Epoch 76/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.8668 - reconstruction_loss: 13.8903 - z_loss: 0.3881 - REG_loss: 0.1036\n",
      "Epoch 77/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.9791 - reconstruction_loss: 13.8895 - z_loss: 0.3881 - REG_loss: 0.0978\n",
      "Epoch 78/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.9655 - reconstruction_loss: 13.8892 - z_loss: 0.3880 - REG_loss: 0.3840\n",
      "Epoch 79/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8811 - reconstruction_loss: 13.8900 - z_loss: 0.3879 - REG_loss: 0.0968\n",
      "Epoch 80/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9229 - reconstruction_loss: 13.8901 - z_loss: 0.3878 - REG_loss: 0.0843\n",
      "Epoch 81/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9215 - reconstruction_loss: 13.8901 - z_loss: 0.3877 - REG_loss: 0.2088\n",
      "Epoch 82/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9875 - reconstruction_loss: 13.8915 - z_loss: 0.3875 - REG_loss: 0.5254\n",
      "Epoch 83/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9313 - reconstruction_loss: 13.8918 - z_loss: 0.3874 - REG_loss: 0.0893\n",
      "Epoch 84/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9485 - reconstruction_loss: 13.8907 - z_loss: 0.3873 - REG_loss: 0.0822\n",
      "Epoch 85/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.8852 - reconstruction_loss: 13.8893 - z_loss: 0.3872 - REG_loss: 0.0805\n",
      "Epoch 86/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9612 - reconstruction_loss: 13.8900 - z_loss: 0.3870 - REG_loss: 0.3992\n",
      "Epoch 87/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.9443 - reconstruction_loss: 13.8900 - z_loss: 0.3864 - REG_loss: 0.0778\n",
      "Epoch 88/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.9606 - reconstruction_loss: 13.8903 - z_loss: 0.3857 - REG_loss: 0.0741\n",
      "Epoch 89/100\n",
      "196/196 [==============================] - 1s 4ms/step - loss: 13.8070 - reconstruction_loss: 13.8892 - z_loss: 0.3847 - REG_loss: 0.1096\n",
      "Epoch 90/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9024 - reconstruction_loss: 13.8897 - z_loss: 0.3843 - REG_loss: 0.2057\n",
      "Epoch 91/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9751 - reconstruction_loss: 13.8887 - z_loss: 0.3839 - REG_loss: 0.0818\n",
      "Epoch 92/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9757 - reconstruction_loss: 13.8893 - z_loss: 0.3832 - REG_loss: 0.3483\n",
      "Epoch 93/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9026 - reconstruction_loss: 13.8887 - z_loss: 0.3835 - REG_loss: 0.0683\n",
      "Epoch 94/100\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 13.8509 - reconstruction_loss: 13.8906 - z_loss: 0.3827 - REG_loss: 0.0647\n",
      "Epoch 95/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9279 - reconstruction_loss: 13.8895 - z_loss: 0.3816 - REG_loss: 0.5373\n",
      "Epoch 96/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9318 - reconstruction_loss: 13.8897 - z_loss: 0.3827 - REG_loss: 0.0910\n",
      "Epoch 97/100\n",
      "196/196 [==============================] - 1s 6ms/step - loss: 13.8315 - reconstruction_loss: 13.8895 - z_loss: 0.3826 - REG_loss: 0.0677\n",
      "Epoch 98/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9515 - reconstruction_loss: 13.8888 - z_loss: 0.3818 - REG_loss: 0.0629\n",
      "Epoch 99/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.8985 - reconstruction_loss: 13.8889 - z_loss: 0.3808 - REG_loss: 0.0603\n",
      "Epoch 100/100\n",
      "196/196 [==============================] - 1s 5ms/step - loss: 13.9162 - reconstruction_loss: 13.8894 - z_loss: 0.3798 - REG_loss: 0.0625\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 4\n",
    "# num_dimensions = 10\n",
    "\n",
    "# normalizing \n",
    "\n",
    "\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(num_dimensions,))\n",
    "# x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "# x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Flatten()(x)\n",
    "x = layers.Dense(20, activation=\"sigmoid\")(encoder_inputs)\n",
    "x = layers.Dense(18, activation=\"sigmoid\")(x)\n",
    "x = layers.Dense(16, activation=\"sigmoid\")(x)\n",
    "encoder_output = layers.Dense(latent_dim, activation=\"sigmoid\")(x)\n",
    "# z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "# z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "# z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, encoder_output, name=\"encoder\")\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "# x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "# x = layers.Reshape((7, 7, 64))(x)\n",
    "# x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "x = layers.Dense(16, activation=\"sigmoid\")(latent_inputs)\n",
    "x = layers.Dense(18, activation=\"sigmoid\")(x)\n",
    "x = layers.Dense(20, activation=\"sigmoid\")(x)\n",
    "# x = layers.Dense(512, activation=\"relu\")(x)\n",
    "decoder_outputs = layers.Dense(num_dimensions, activation=\"linear\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "\n",
    "class RAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.z_tracker = keras.metrics.Mean(name=\"z_loss\")\n",
    "        self.REG_tracker = keras.metrics.Mean(name=\"REG_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.z_tracker,\n",
    "            self.REG_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "#             reconstruction_loss = tf.reduce_mean(\n",
    "#                 tf.reduce_sum(\n",
    "#                     keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "#                 )\n",
    "#             )\n",
    "            reconstruction_loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(data,reconstruction)\n",
    "            # reconstruction_loss = tf.keras.losses.BinaryCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(data, reconstruction)\n",
    "#             \n",
    "            z_loss = K.mean(K.square(z), axis=[1])\n",
    "    \n",
    "\n",
    "#             gradients = tape.gradient(reconstruction, self.encoder(data))\n",
    "    \n",
    "\n",
    "#             gradient_norm = 0.0\n",
    "#             for gradient in gradients:\n",
    "#                 if gradient is not None:\n",
    "#                     gradient_norm += tf.norm(gradient, ord=2)\n",
    "    \n",
    "\n",
    "            REG_loss = K.mean(K.square(K.gradients(K.square(reconstruction), z)))\n",
    "\n",
    "            z_loss_w = 0.05\n",
    "            REG_loss_w = 0.05\n",
    "\n",
    "            total_loss = reconstruction_loss +  z_loss_w * z_loss + REG_loss_w * REG_loss\n",
    "            # total_loss = reconstruction_loss\n",
    "        \n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.z_tracker.update_state(z_loss)\n",
    "            self.REG_tracker.update_state(REG_loss)\n",
    "            del tape\n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"z_loss\": self.z_tracker.result(),\n",
    "                \"REG_loss\": self.REG_tracker.result(),\n",
    "            }\n",
    "        \n",
    "\n",
    "# mnist_data = mat_data['X']\n",
    "tdata = np.concatenate([X_train], axis=0)\n",
    "tdata = np.expand_dims(tdata, -1).astype(\"float32\")\n",
    "\n",
    "rae = RAE(encoder, decoder)\n",
    "rae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "history = rae.fit(tdata, epochs=100, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58403e51",
   "metadata": {},
   "source": [
    "# Testing the Rec Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "956a3ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.792164 7.735719\n",
      "61.199318 31.762594\n"
     ]
    }
   ],
   "source": [
    "outlier_indices = np.argwhere(y_train)\n",
    "inlier_indices  = np.argwhere(1 - y_train)\n",
    "\n",
    "outliers = X_train[outlier_indices,:]\n",
    "inliers  = X_train[inlier_indices, :]\n",
    "\n",
    "inliers_mean = []\n",
    "outliers_mean= []\n",
    "\n",
    "for i in range(inliers.shape[0]):\n",
    "    \n",
    "    sample = inliers[i,0,:].reshape([1,num_dimensions])\n",
    "\n",
    "    z = rae.encoder(sample)\n",
    "    reconstruction = rae.decoder(z)\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample,reconstruction)\n",
    "    \n",
    "    inliers_mean.append(reconstruction_loss)\n",
    "\n",
    "for i in range(outliers.shape[0]):\n",
    "    \n",
    "    sample = outliers[i,0,:].reshape([1,num_dimensions])\n",
    "\n",
    "    z = rae.encoder(sample)\n",
    "    reconstruction = rae.decoder(z)\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample,reconstruction)\n",
    "    \n",
    "    outliers_mean.append(reconstruction_loss)\n",
    "\n",
    "inliers_mean = np.array(inliers_mean)\n",
    "outliers_mean = np.array(outliers_mean)\n",
    "\n",
    "i_mean = np.mean(inliers_mean)\n",
    "o_mean = np.mean(outliers_mean)\n",
    "\n",
    "i_std = np.std(inliers_mean)\n",
    "o_std = np.std(outliers_mean)\n",
    "\n",
    "print(i_mean, i_std)\n",
    "print(o_mean, o_std)\n",
    "#     print(reconstruction_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7d2eb56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.99932146072388"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_mean + 3*i_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8dfc2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = i_mean + 3*i_std\n",
    "\n",
    "classes = []\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    \n",
    "    sample = X_train[i,:].reshape([1,num_dimensions])\n",
    "\n",
    "    z = rae.encoder(sample)\n",
    "    reconstruction = rae.decoder(z)\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample,reconstruction)\n",
    "    \n",
    "    if reconstruction_loss > threshold:\n",
    "        \n",
    "        classes.append(1)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        classes.append(0)\n",
    "\n",
    "classes = np.array(classes)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5a87d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "detected_outliers_indices = np.argwhere(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c8df16ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641775b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
