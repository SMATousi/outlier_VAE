{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91d009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 12:17:27.882389: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 12:17:38.726394: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/macula/SMATousi/.conda/envs/mac-deep/lib/:/home/macula/SMATousi/.conda/envs/mac-deep/lib/python3.7/site-packages/nvidia/cudnn/lib\n",
      "2023-06-12 12:17:38.728248: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/macula/SMATousi/.conda/envs/mac-deep/lib/:/home/macula/SMATousi/.conda/envs/mac-deep/lib/python3.7/site-packages/nvidia/cudnn/lib\n",
      "2023-06-12 12:17:38.728273: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51199e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9469744745569415091\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10088022016\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14572668863915254932\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:19:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10088022016\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16271735185002732189\n",
      "physical_device_desc: \"device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 2144165316\n",
      ", name: \"/device:GPU:2\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 10088022016\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1446188156585758040\n",
      "physical_device_desc: \"device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:67:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 1651660799\n",
      ", name: \"/device:GPU:3\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9954656256\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12982715034557206105\n",
      "physical_device_desc: \"device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 878896533\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 12:18:05.432943: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 12:18:11.508283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 9620 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:19:00.0, compute capability: 7.5\n",
      "2023-06-12 12:18:11.511080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:1 with 9620 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:1a:00.0, compute capability: 7.5\n",
      "2023-06-12 12:18:11.512196: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:2 with 9620 MB memory:  -> device: 2, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "2023-06-12 12:18:11.513270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:3 with 9493 MB memory:  -> device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "651470eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3516357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 10:33:57.707973: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 14, 14, 32)   320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 7, 7, 64)     18496       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 3136)         0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           50192       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 69,076\n",
      "Trainable params: 69,076\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb6afd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3136)              9408      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 14, 14, 64)       36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,089\n",
      "Trainable params: 65,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37049328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e22a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "Epoch 1/2\n",
      "547/547 [==============================] - 66s 115ms/step - loss: 248.3755 - reconstruction_loss: 205.1570 - kl_loss: 3.4305\n",
      "Epoch 2/2\n",
      "547/547 [==============================] - 69s 126ms/step - loss: 178.6454 - reconstruction_loss: 167.3625 - kl_loss: 5.1407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb91645a450>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(mnist_digits, epochs=2, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a421872f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADER: \"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
      "EXAMPLE FEATURES: [0.0, -1.3598071336738, -0.0727811733098497, 2.53634673796914, 1.37815522427443, -0.338320769942518, 0.462387777762292, 0.239598554061257, 0.0986979012610507, 0.363786969611213, 0.0907941719789316, -0.551599533260813, -0.617800855762348, -0.991389847235408, -0.311169353699879, 1.46817697209427, -0.470400525259478, 0.207971241929242, 0.0257905801985591, 0.403992960255733, 0.251412098239705, -0.018306777944153, 0.277837575558899, -0.110473910188767, 0.0669280749146731, 0.128539358273528, -0.189114843888824, 0.133558376740387, -0.0210530534538215, 149.62]\n",
      "features.shape: (284807, 30)\n",
      "targets.shape: (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/\n",
    "fname = \"/home/macula/SMATousi/VAE_outlier/archive/creditcard.csv\"\n",
    "\n",
    "all_features = []\n",
    "all_targets = []\n",
    "with open(fname) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(\"HEADER:\", line.strip())\n",
    "            continue  # Skip header\n",
    "        fields = line.strip().split(\",\")\n",
    "        all_features.append([float(v.replace('\"', \"\")) for v in fields[:-1]])\n",
    "        all_targets.append([int(fields[-1].replace('\"', \"\"))])\n",
    "        if i == 1:\n",
    "            print(\"EXAMPLE FEATURES:\", all_features[-1])\n",
    "\n",
    "features = np.array(all_features, dtype=\"float32\")\n",
    "targets = np.array(all_targets, dtype=\"uint8\")\n",
    "print(\"features.shape:\", features.shape)\n",
    "print(\"targets.shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676f201a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 227846\n",
      "Number of validation samples: 56961\n"
     ]
    }
   ],
   "source": [
    "num_val_samples = int(len(features) * 0.2)\n",
    "train_features = features[:-num_val_samples]\n",
    "train_targets = targets[:-num_val_samples]\n",
    "val_features = features[-num_val_samples:]\n",
    "val_targets = targets[-num_val_samples:]\n",
    "\n",
    "print(\"Number of training samples:\", len(train_features))\n",
    "print(\"Number of validation samples:\", len(val_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b91d1a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples in training data: 417 (0.18% of total)\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(train_targets[:, 0])\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(train_targets)\n",
    "    )\n",
    ")\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "270cc1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(train_features, axis=0)\n",
    "train_features -= mean\n",
    "val_features -= mean\n",
    "std = np.std(train_features, axis=0)\n",
    "train_features /= std\n",
    "val_features /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17e5ed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 12:53:40.370089: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-30 12:53:40.370667: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(30,))\n",
    "# x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "# x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Flatten()(x)\n",
    "x = layers.Dense(30, activation=\"tanh\")(encoder_inputs)\n",
    "x = layers.Dense(20, activation=\"tanh\")(x)\n",
    "x = layers.Dense(18, activation=\"tanh\")(x)\n",
    "x = layers.Dense(16, activation=\"tanh\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "# x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "# x = layers.Reshape((7, 7, 64))(x)\n",
    "# x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "x = layers.Dense(16, activation=\"tanh\")(latent_inputs)\n",
    "x = layers.Dense(18, activation=\"tanh\")(x)\n",
    "x = layers.Dense(20, activation=\"tanh\")(x)\n",
    "decoder_outputs = layers.Dense(30, activation=\"tanh\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8459cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "#             reconstruction_loss = tf.reduce_mean(\n",
    "#                 tf.reduce_sum(\n",
    "#                     keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "#                 )\n",
    "#             )\n",
    "            reconstruction_loss = tf.keras.losses.MeanSquaredError()(data,reconstruction)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d68e9af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2226/2226 [==============================] - 4s 1ms/step - loss: 1.0310 - reconstruction_loss: 1.0054 - kl_loss: 0.0035\n",
      "Epoch 2/10\n",
      "2226/2226 [==============================] - 2s 1ms/step - loss: 1.0149 - reconstruction_loss: 1.0047 - kl_loss: 2.8087e-05\n",
      "Epoch 3/10\n",
      "2226/2226 [==============================] - 3s 1ms/step - loss: 0.9983 - reconstruction_loss: 1.0048 - kl_loss: 7.8637e-06\n",
      "Epoch 4/10\n",
      "2226/2226 [==============================] - 3s 1ms/step - loss: 0.9902 - reconstruction_loss: 1.0045 - kl_loss: 3.5634e-06\n",
      "Epoch 5/10\n",
      "2226/2226 [==============================] - 3s 1ms/step - loss: 0.9902 - reconstruction_loss: 1.0044 - kl_loss: 2.1467e-06\n",
      "Epoch 6/10\n",
      "2226/2226 [==============================] - 3s 1ms/step - loss: 0.9975 - reconstruction_loss: 1.0044 - kl_loss: 1.5636e-06\n",
      "Epoch 7/10\n",
      "2226/2226 [==============================] - 3s 1ms/step - loss: 0.9956 - reconstruction_loss: 1.0043 - kl_loss: 1.2261e-06\n",
      "Epoch 8/10\n",
      "2226/2226 [==============================] - 3s 1ms/step - loss: 1.0107 - reconstruction_loss: 1.0043 - kl_loss: 8.2702e-07\n",
      "Epoch 9/10\n",
      "2226/2226 [==============================] - 3s 1ms/step - loss: 1.0029 - reconstruction_loss: 1.0044 - kl_loss: 7.6706e-07\n",
      "Epoch 10/10\n",
      "2226/2226 [==============================] - 3s 1ms/step - loss: 0.9945 - reconstruction_loss: 1.0044 - kl_loss: 7.9487e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9d1421ed10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "creditdata = np.concatenate([train_features, val_features], axis=0)\n",
    "creditdata = np.expand_dims(creditdata, -1).astype(\"float32\")\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(creditdata, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "565eba6f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'python' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_412818/1510789129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'python' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1406bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_features[1,:]\n",
    "\n",
    "test = test.reshape([1,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3f7b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_log_var, z = vae.encoder(test)\n",
    "reconstruction = vae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8cc1db9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41741496"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18959388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41229054,  0.02760492,  0.00990844, -0.13348393, -0.03093056,\n",
       "         0.04025487, -0.02018991,  0.01525421, -0.0075708 ,  0.0047708 ,\n",
       "         0.01919036, -0.07557505,  0.04402744, -0.0281549 , -0.02722586,\n",
       "        -0.04202577,  0.0039289 , -0.01101142,  0.02211242,  0.00563244,\n",
       "        -0.01162461,  0.01399416,  0.0240715 ,  0.01843469, -0.0130402 ,\n",
       "        -0.07930604, -0.01005057, -0.0105467 ,  0.00252189, -0.00742954]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "321b2d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0008564 ,  0.64784056,  0.16869006, -0.01075485,  0.28652602,\n",
       "         0.09012804, -0.08543706, -0.04230526,  0.0670673 , -0.2293523 ,\n",
       "        -0.15108545,  1.4866744 ,  1.0515386 ,  0.4633228 , -0.18291788,\n",
       "         0.62796277,  0.5300628 , -0.14867269, -0.1904353 , -0.16889915,\n",
       "        -0.10145202, -0.29439253, -0.8689419 ,  0.17637353, -0.56267697,\n",
       "         0.25427756,  0.25284111, -0.0230066 ,  0.03819847, -0.3517778 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c1b318f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec Loss =  7.5094476  -------- Label =  [1]  Case No.  1866\n",
      "Rec Loss =  3.3196588  -------- Label =  [1]  Case No.  1884\n",
      "Rec Loss =  10.030133  -------- Label =  [1]  Case No.  2230\n",
      "Rec Loss =  6.4608707  -------- Label =  [1]  Case No.  2630\n",
      "Rec Loss =  5.0940776  -------- Label =  [1]  Case No.  4132\n",
      "Rec Loss =  5.3070607  -------- Label =  [1]  Case No.  5412\n",
      "Rec Loss =  12.009609  -------- Label =  [1]  Case No.  6728\n",
      "Rec Loss =  5.2277074  -------- Label =  [1]  Case No.  6786\n",
      "Rec Loss =  5.2294545  -------- Label =  [1]  Case No.  6787\n",
      "Rec Loss =  6.815138  -------- Label =  [1]  Case No.  6859\n",
      "Rec Loss =  8.662293  -------- Label =  [1]  Case No.  7770\n",
      "Rec Loss =  8.746953  -------- Label =  [1]  Case No.  7788\n",
      "Rec Loss =  8.010825  -------- Label =  [1]  Case No.  7798\n",
      "Rec Loss =  7.5274982  -------- Label =  [1]  Case No.  9261\n",
      "Rec Loss =  4.9370375  -------- Label =  [1]  Case No.  9580\n",
      "Rec Loss =  15.51634  -------- Label =  [1]  Case No.  10376\n",
      "Rec Loss =  4.067612  -------- Label =  [1]  Case No.  10520\n",
      "Rec Loss =  1.3866913  -------- Label =  [1]  Case No.  10620\n",
      "Rec Loss =  3.4350863  -------- Label =  [1]  Case No.  11653\n",
      "Rec Loss =  3.3325393  -------- Label =  [1]  Case No.  11655\n",
      "Rec Loss =  2.2309318  -------- Label =  [1]  Case No.  12376\n",
      "Rec Loss =  2.5541432  -------- Label =  [1]  Case No.  13408\n",
      "Rec Loss =  7.936458  -------- Label =  [1]  Case No.  13599\n",
      "Rec Loss =  12.206465  -------- Label =  [1]  Case No.  15547\n",
      "Rec Loss =  11.587795  -------- Label =  [1]  Case No.  15701\n",
      "Rec Loss =  17.210018  -------- Label =  [1]  Case No.  15853\n",
      "Rec Loss =  17.072529  -------- Label =  [1]  Case No.  15903\n",
      "Rec Loss =  16.806295  -------- Label =  [1]  Case No.  16002\n",
      "Rec Loss =  16.594913  -------- Label =  [1]  Case No.  16158\n",
      "Rec Loss =  16.426771  -------- Label =  [1]  Case No.  16487\n",
      "Rec Loss =  0.9376562  -------- Label =  [1]  Case No.  17501\n",
      "Rec Loss =  0.81206065  -------- Label =  [1]  Case No.  17710\n",
      "Rec Loss =  6.0854635  -------- Label =  [1]  Case No.  19827\n",
      "Rec Loss =  3.5899422  -------- Label =  [1]  Case No.  20149\n",
      "Rec Loss =  7.440956  -------- Label =  [1]  Case No.  20450\n",
      "Rec Loss =  2.1242206  -------- Label =  [1]  Case No.  21125\n",
      "Rec Loss =  6.2652135  -------- Label =  [1]  Case No.  21321\n",
      "Rec Loss =  1.8157153  -------- Label =  [1]  Case No.  21393\n",
      "Rec Loss =  7.891714  -------- Label =  [1]  Case No.  21761\n",
      "Rec Loss =  9.848066  -------- Label =  [1]  Case No.  21982\n",
      "Rec Loss =  12.240511  -------- Label =  [1]  Case No.  22117\n",
      "Rec Loss =  11.998058  -------- Label =  [1]  Case No.  22915\n",
      "Rec Loss =  11.87045  -------- Label =  [1]  Case No.  23631\n",
      "Rec Loss =  8.578803  -------- Label =  [1]  Case No.  24020\n",
      "Rec Loss =  1.6055672  -------- Label =  [1]  Case No.  24035\n",
      "Rec Loss =  1.5776365  -------- Label =  [1]  Case No.  24045\n",
      "Rec Loss =  9.069529  -------- Label =  [1]  Case No.  24058\n",
      "Rec Loss =  11.88171  -------- Label =  [1]  Case No.  24278\n",
      "Rec Loss =  11.723037  -------- Label =  [1]  Case No.  24928\n",
      "Rec Loss =  0.54398257  -------- Label =  [1]  Case No.  26498\n",
      "Rec Loss =  0.978151  -------- Label =  [1]  Case No.  26549\n",
      "Rec Loss =  5.245369  -------- Label =  [1]  Case No.  27557\n",
      "Rec Loss =  8.04893  -------- Label =  [1]  Case No.  27710\n",
      "Rec Loss =  6.704536  -------- Label =  [1]  Case No.  30557\n",
      "Rec Loss =  3.4096606  -------- Label =  [1]  Case No.  33210\n",
      "Rec Loss =  3.9158437  -------- Label =  [1]  Case No.  33627\n",
      "Rec Loss =  3.7353394  -------- Label =  [1]  Case No.  34079\n",
      "Rec Loss =  11.862329  -------- Label =  [1]  Case No.  34714\n",
      "Rec Loss =  14.05816  -------- Label =  [1]  Case No.  34980\n",
      "Rec Loss =  0.92143786  -------- Label =  [1]  Case No.  35234\n",
      "Rec Loss =  13.818744  -------- Label =  [1]  Case No.  35428\n",
      "Rec Loss =  10.551589  -------- Label =  [1]  Case No.  35478\n",
      "Rec Loss =  10.203214  -------- Label =  [1]  Case No.  36031\n",
      "Rec Loss =  9.1083555  -------- Label =  [1]  Case No.  40529\n",
      "Rec Loss =  12.134983  -------- Label =  [1]  Case No.  44675\n",
      "Rec Loss =  7.0281706  -------- Label =  [1]  Case No.  46536\n",
      "Rec Loss =  2.024425  -------- Label =  [1]  Case No.  46629\n",
      "Rec Loss =  1.1876132  -------- Label =  [1]  Case No.  48146\n",
      "Rec Loss =  0.71580446  -------- Label =  [1]  Case No.  48225\n",
      "Rec Loss =  3.5066657  -------- Label =  [1]  Case No.  49018\n",
      "Rec Loss =  6.0349917  -------- Label =  [1]  Case No.  52017\n",
      "Rec Loss =  3.3941805  -------- Label =  [1]  Case No.  52297\n",
      "Rec Loss =  3.1298482  -------- Label =  [1]  Case No.  52303\n",
      "Rec Loss =  5.8462167  -------- Label =  [1]  Case No.  53298\n",
      "Rec Loss =  0.68649834  -------- Label =  [1]  Case No.  53828\n",
      "mean =  7.058716\n",
      "std =  4.6576095\n"
     ]
    }
   ],
   "source": [
    "rec_loss = []\n",
    "\n",
    "for i in range(val_features.shape[0]):\n",
    "    \n",
    "    if val_targets[i] == 1:\n",
    "        \n",
    "        test = val_features[i,:]\n",
    "\n",
    "        test = test.reshape([1,30])\n",
    "\n",
    "        z_mean, z_log_var, z = vae.encoder(test)\n",
    "        reconstruction = vae.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)\n",
    "        \n",
    "        rec_loss.append(reconstruction_loss.numpy())\n",
    "\n",
    "\n",
    "        print(\"Rec Loss = \", reconstruction_loss.numpy(), \" -------- Label = \", val_targets[i], \" Case No. \", i)\n",
    "\n",
    "rec_loss = np.array(rec_loss)\n",
    "print(\"mean = \", np.mean(rec_loss))\n",
    "print(\"std = \", np.std(rec_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8bfe2279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.7462541 ,  -0.50769436,   2.6397295 ,  -4.164056  ,\n",
       "         3.7277195 ,   1.124849  ,  -1.683369  ,  -1.2245529 ,\n",
       "         0.51499057,  -3.5956087 ,  -5.744471  ,   3.3477724 ,\n",
       "        -7.4962306 ,  -1.657718  , -12.934678  ,  -0.2816221 ,\n",
       "        -1.6018841 ,  -3.7281365 ,   0.09827992,  -2.1891088 ,\n",
       "         0.669623  ,   0.54346186,  -1.3017883 ,  -0.5808547 ,\n",
       "        -0.36568624,   0.4449785 ,   0.09471209,   2.115266  ,\n",
       "         1.5862252 ,  -0.36251542], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_features[6728,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f3b2326",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cba35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e3cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac95280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5310fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a173c37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c6a4505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygad\n",
    "\n",
    "\n",
    "def fitness_func(ga_instance, solution, solution_idx):\n",
    "    \n",
    "    particle = val_features[6728,:]\n",
    "    particle = particle.reshape([1,30])\n",
    "    \n",
    "#     abn_subspace = solution * val_features[6728,:]\n",
    "    \n",
    "#     abn_subspace = abn_subspace.reshape([1,30])\n",
    "    \n",
    "    \n",
    "\n",
    "    z_mean, z_log_var, z = vae.encoder(particle)\n",
    "    reconstruction_1 = vae.decoder(z)\n",
    "    \n",
    "    replace = reconstruction_1 * solution\n",
    "    \n",
    "    abn_subspace = 1 - solution\n",
    "    \n",
    "    particle_abn = particle * abn_subspace\n",
    "    \n",
    "    particle_rec = particle_abn + replace\n",
    "    \n",
    "    \n",
    "    z_mean, z_log_var, z = vae.encoder(particle_rec)\n",
    "    reconstruction_1 = vae.decoder(z)\n",
    "    \n",
    "    rec_loss = tf.keras.losses.MeanSquaredError()(particle,particle_rec)\n",
    "    fitness = rec_loss.numpy()\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "\n",
    "def on_generation(ga):\n",
    "    print(\"Generation\", ga.generations_completed)\n",
    "    \n",
    "    solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "    \n",
    "    print(solution_fitness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "737d3f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_func_avg(ga_instance, solution, solution_idx):\n",
    "    \n",
    "    inliers = val_features[10:15,:]\n",
    "    \n",
    "    avg_ins = np.mean(val_features[10:15,:], axis=0)\n",
    "    avg_ins = avg_ins.reshape([1,30])\n",
    "    \n",
    "    particle = val_features[6728,:]\n",
    "    particle = particle.reshape([1,30])\n",
    "    \n",
    "#     abn_subspace = solution * val_features[6728,:]\n",
    "    \n",
    "#     abn_subspace = abn_subspace.reshape([1,30])\n",
    "\n",
    "    avg_in_rec = []\n",
    "    \n",
    "    for index in range(inliers.shape[0]):\n",
    "        \n",
    "        candidate_inlier = inliers[index,:]\n",
    "        candidate_inlier = candidate_inlier.reshape([1,30])\n",
    "        \n",
    "        in_remain = candidate_inlier * solution\n",
    "        \n",
    "        in_normal_subspace = 1 - solution\n",
    "        \n",
    "        in_replace = in_normal_subspace * avg_ins\n",
    "        \n",
    "        in_candidate = in_remain + in_replace\n",
    "        \n",
    "        z_mean, z_log_var, z = vae.encoder(in_candidate)\n",
    "        in_candidate_rec = vae.decoder(z)\n",
    "        \n",
    "        \n",
    "        rec_loss = tf.keras.losses.MeanSquaredError()(in_candidate,in_candidate_rec)\n",
    "        \n",
    "        avg_in_rec.append(rec_loss.numpy())\n",
    "    \n",
    "    avg_in_rec = np.array(avg_in_rec)\n",
    "    avg_in_rec = np.mean(avg_in_rec)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "#     z_mean, z_log_var, z = vae.encoder(particle)\n",
    "#     reconstruction_1 = vae.decoder(z)\n",
    "    \n",
    "    out_remain = particle * solution\n",
    "    \n",
    "    out_normal_subspace = 1 - solution\n",
    "    \n",
    "    out_replace = avg_ins * out_normal_subspace\n",
    "    \n",
    "    out_candidate = out_remain + out_replace\n",
    "    \n",
    "    \n",
    "    z_mean, z_log_var, z = vae.encoder(out_candidate)\n",
    "    out_candidate_rec = vae.decoder(z)\n",
    "    \n",
    "    rec_loss = tf.keras.losses.MeanSquaredError()(out_candidate,out_candidate_rec)\n",
    "    rec_loss = rec_loss.numpy()\n",
    "    \n",
    "    fitness = rec_loss / avg_in_rec\n",
    "    \n",
    "    return fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "67171a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitness_function = fitness_func_avg\n",
    "\n",
    "num_generations = 50\n",
    "num_parents_mating = 4\n",
    "\n",
    "sol_per_pop = 100\n",
    "num_genes = val_features[6728,:].shape[0]\n",
    "\n",
    "init_range_low = -2\n",
    "init_range_high = 5\n",
    "\n",
    "parent_selection_type = \"sss\"\n",
    "keep_parents = 1\n",
    "\n",
    "space = [[0,1] for i in range(num_genes)]\n",
    "\n",
    "crossover_type = \"single_point\"\n",
    "\n",
    "mutation_type = \"random\"\n",
    "mutation_percent_genes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "46dc9472",
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                       num_parents_mating=num_parents_mating,\n",
    "                       fitness_func=fitness_function,\n",
    "                       sol_per_pop=sol_per_pop,\n",
    "                       num_genes=num_genes,\n",
    "                       init_range_low=init_range_low,\n",
    "                       init_range_high=init_range_high,\n",
    "                       parent_selection_type=parent_selection_type,\n",
    "                       keep_parents=keep_parents,\n",
    "                       crossover_type=crossover_type,\n",
    "                       mutation_type=mutation_type,\n",
    "                       mutation_percent_genes=mutation_percent_genes,\n",
    "                       on_generation=on_generation,\n",
    "                       gene_space = space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e868f986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1\n",
      "37.11245\n",
      "Generation 2\n",
      "41.05748\n",
      "Generation 3\n",
      "45.728626\n",
      "Generation 4\n",
      "45.678024\n",
      "Generation 5\n",
      "45.5867\n",
      "Generation 6\n",
      "45.5867\n",
      "Generation 7\n",
      "45.998726\n",
      "Generation 8\n",
      "46.43835\n",
      "Generation 9\n",
      "46.453487\n",
      "Generation 10\n",
      "46.453487\n",
      "Generation 11\n",
      "46.453487\n",
      "Generation 12\n",
      "46.453487\n",
      "Generation 13\n",
      "46.453487\n",
      "Generation 14\n",
      "46.453487\n",
      "Generation 15\n",
      "47.00801\n",
      "Generation 16\n",
      "47.113224\n",
      "Generation 17\n",
      "47.113224\n",
      "Generation 18\n",
      "47.113224\n",
      "Generation 19\n",
      "47.503582\n",
      "Generation 20\n",
      "47.38894\n",
      "Generation 21\n",
      "47.38894\n",
      "Generation 22\n",
      "47.38894\n",
      "Generation 23\n",
      "47.38894\n",
      "Generation 24\n",
      "47.38894\n",
      "Generation 25\n",
      "47.38894\n",
      "Generation 26\n",
      "47.409275\n",
      "Generation 27\n",
      "47.38894\n",
      "Generation 28\n",
      "47.38894\n",
      "Generation 29\n",
      "47.38894\n",
      "Generation 30\n",
      "47.38894\n",
      "Generation 31\n",
      "47.38894\n",
      "Generation 32\n",
      "47.38894\n",
      "Generation 33\n",
      "47.38894\n",
      "Generation 34\n",
      "47.644253\n",
      "Generation 35\n",
      "47.424953\n",
      "Generation 36\n",
      "47.424953\n",
      "Generation 37\n",
      "47.424953\n",
      "Generation 38\n",
      "47.424953\n",
      "Generation 39\n",
      "47.424953\n",
      "Generation 40\n",
      "47.424953\n",
      "Generation 41\n",
      "47.424953\n",
      "Generation 42\n",
      "47.424953\n",
      "Generation 43\n",
      "47.424953\n",
      "Generation 44\n",
      "47.424953\n",
      "Generation 45\n",
      "47.424953\n",
      "Generation 46\n",
      "47.424953\n",
      "Generation 47\n",
      "47.424953\n",
      "Generation 48\n",
      "47.424953\n",
      "Generation 49\n",
      "47.91089\n",
      "Generation 50\n",
      "47.81949\n"
     ]
    }
   ],
   "source": [
    "ga_instance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6c8ab1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of the best solution : [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 0.]\n",
      "Fitness value of the best solution = 47.819488525390625\n"
     ]
    }
   ],
   "source": [
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "print(\"Parameters of the best solution : {solution}\".format(solution=solution))\n",
    "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101bd600",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7208b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff54b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eca93444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.67609   ,  0.29879466, -0.51317704,  0.03845291, -0.26599112,\n",
       "       -0.4089473 , -0.11063142, -0.08701693,  0.02064687,  0.32344753,\n",
       "       -0.24708238, -0.94346046,  0.38582838,  0.5944903 , -0.40336066,\n",
       "       -0.1687354 ,  0.26231766, -0.567821  ,  0.17933348, -0.24484256,\n",
       "        0.30691004,  0.12367544,  0.04679886, -0.19573799,  0.12336917,\n",
       "       -0.37529045, -0.22803795, -0.13733166, -0.05003484,  0.4847347 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(val_features[10:15,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0d354c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 30)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_features[10:15,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f34c3dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05065055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6637555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd4e09f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee4f37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = np.random.rand(200, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56f0c838",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "num_dimensions = 30\n",
    "\n",
    "# Generate random samples\n",
    "random_samples = np.random.rand(num_samples, num_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2205011b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.67743024 0.25345555 0.13721269 ... 0.52092047 0.40750846 0.20464901]\n",
      " [0.76032286 0.74206023 0.42532838 ... 0.71720405 0.98045262 0.39438384]\n",
      " [0.30459255 0.2779774  0.51034756 ... 0.64490894 0.89239919 0.15052649]\n",
      " ...\n",
      " [0.41204152 0.71944016 0.47328146 ... 0.40802448 0.39963379 0.85432651]\n",
      " [0.73030401 0.79193762 0.20377598 ... 0.58925518 0.05544492 0.89040338]\n",
      " [0.79458781 0.72340085 0.50311506 ... 0.22426583 0.99445266 0.70633326]]\n"
     ]
    }
   ],
   "source": [
    "print(random_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a46df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dataset = random_samples\n",
    "\n",
    "\n",
    "out_dataset[0,0] = 20\n",
    "out_dataset[0,3] = 20\n",
    "out_dataset[0,4] = 20\n",
    "out_dataset[0,7] = 20\n",
    "out_dataset[0,9] = 20\n",
    "out_dataset[0,14] = 20\n",
    "out_dataset[0,22] = 20\n",
    "\n",
    "\n",
    "out_dataset[100,0] = 20\n",
    "out_dataset[100,3] = 20\n",
    "out_dataset[100,4] = 20\n",
    "out_dataset[100,7] = 20\n",
    "out_dataset[100,9] = 20\n",
    "out_dataset[100,14] = 20\n",
    "out_dataset[100,22] = 20\n",
    "\n",
    "out_dataset[400,0] = 20\n",
    "out_dataset[400,3] = 20\n",
    "out_dataset[400,4] = 20\n",
    "out_dataset[400,7] = 20\n",
    "out_dataset[400,9] = 20\n",
    "out_dataset[400,14] = 20\n",
    "out_dataset[400,22] = 20\n",
    "\n",
    "out_dataset[600,0] = 20\n",
    "out_dataset[600,3] = 20\n",
    "out_dataset[600,4] = 20\n",
    "out_dataset[600,7] = 20\n",
    "out_dataset[600,9] = 20\n",
    "out_dataset[600,14] = 20\n",
    "out_dataset[600,22] = 20\n",
    "\n",
    "out_dataset[900,0] = 20\n",
    "out_dataset[900,3] = 20\n",
    "out_dataset[900,4] = 20\n",
    "out_dataset[900,7] = 20\n",
    "out_dataset[900,9] = 20\n",
    "out_dataset[900,14] = 20\n",
    "out_dataset[900,22] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4a8a2c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0)\n",
    "np.random.seed(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7483704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 1s 5ms/step - loss: 0.8828 - reconstruction_loss: 0.7192 - kl_loss: 0.1637\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.7180 - reconstruction_loss: 0.8019 - kl_loss: 0.0652\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.8998 - reconstruction_loss: 0.6515 - kl_loss: 0.0330\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6476 - reconstruction_loss: 0.6166 - kl_loss: 0.0202\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5964 - reconstruction_loss: 0.5784 - kl_loss: 0.0136\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4159 - reconstruction_loss: 0.5364 - kl_loss: 0.0105\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4187 - reconstruction_loss: 0.7262 - kl_loss: 0.0086\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7496 - reconstruction_loss: 0.4642 - kl_loss: 0.0069\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6073 - reconstruction_loss: 0.4462 - kl_loss: 0.0058\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5019 - reconstruction_loss: 0.4380 - kl_loss: 0.0050\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4434 - reconstruction_loss: 0.5480 - kl_loss: 0.0044\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3950 - reconstruction_loss: 0.4316 - kl_loss: 0.0038\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4591 - reconstruction_loss: 0.4299 - kl_loss: 0.0034\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5946 - reconstruction_loss: 0.4296 - kl_loss: 0.0030\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4474 - reconstruction_loss: 0.4292 - kl_loss: 0.0027\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7220 - reconstruction_loss: 0.4303 - kl_loss: 0.0024\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4677 - reconstruction_loss: 0.4299 - kl_loss: 0.0022\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4417 - reconstruction_loss: 0.4291 - kl_loss: 0.0020\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5728 - reconstruction_loss: 0.4287 - kl_loss: 0.0018\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5623 - reconstruction_loss: 0.4292 - kl_loss: 0.0016\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3166 - reconstruction_loss: 0.4281 - kl_loss: 0.0015\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.7205 - reconstruction_loss: 0.5434 - kl_loss: 0.0014\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3639 - reconstruction_loss: 0.4292 - kl_loss: 0.0013\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6331 - reconstruction_loss: 0.4292 - kl_loss: 0.0012\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4607 - reconstruction_loss: 0.5435 - kl_loss: 0.0012\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4702 - reconstruction_loss: 0.4289 - kl_loss: 0.0011\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4565 - reconstruction_loss: 0.4294 - kl_loss: 9.9071e-04\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3515 - reconstruction_loss: 0.5428 - kl_loss: 9.5766e-04\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3625 - reconstruction_loss: 0.4287 - kl_loss: 8.9127e-04\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3685 - reconstruction_loss: 0.4284 - kl_loss: 8.4224e-04\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4162 - reconstruction_loss: 0.4300 - kl_loss: 7.9462e-04\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3267 - reconstruction_loss: 0.4297 - kl_loss: 7.5209e-04\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5378 - reconstruction_loss: 0.4290 - kl_loss: 7.2328e-04\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4465 - reconstruction_loss: 0.4286 - kl_loss: 6.8432e-04\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4188 - reconstruction_loss: 0.4282 - kl_loss: 6.4504e-04\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3790 - reconstruction_loss: 0.4297 - kl_loss: 6.2389e-04\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4184 - reconstruction_loss: 0.4289 - kl_loss: 5.8981e-04\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3144 - reconstruction_loss: 0.4287 - kl_loss: 5.8799e-04\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3511 - reconstruction_loss: 0.4284 - kl_loss: 5.4740e-04\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4961 - reconstruction_loss: 0.6583 - kl_loss: 5.2328e-04\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4618 - reconstruction_loss: 0.5434 - kl_loss: 5.0297e-04\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4284 - reconstruction_loss: 0.4299 - kl_loss: 4.8011e-04\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2561 - reconstruction_loss: 0.4293 - kl_loss: 4.6865e-04\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2396 - reconstruction_loss: 0.5430 - kl_loss: 4.6072e-04\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3223 - reconstruction_loss: 0.4294 - kl_loss: 4.3245e-04\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4625 - reconstruction_loss: 0.4283 - kl_loss: 4.1440e-04\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5702 - reconstruction_loss: 0.4289 - kl_loss: 3.9357e-04\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3773 - reconstruction_loss: 0.5434 - kl_loss: 3.9552e-04\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5708 - reconstruction_loss: 0.4291 - kl_loss: 3.8023e-04\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2436 - reconstruction_loss: 0.4288 - kl_loss: 3.5980e-04\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3456 - reconstruction_loss: 0.4288 - kl_loss: 3.4584e-04\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2439 - reconstruction_loss: 0.5433 - kl_loss: 3.3548e-04\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3970 - reconstruction_loss: 0.4291 - kl_loss: 3.2726e-04\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3903 - reconstruction_loss: 0.4289 - kl_loss: 3.1421e-04\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4406 - reconstruction_loss: 0.4285 - kl_loss: 3.1569e-04\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3820 - reconstruction_loss: 0.5444 - kl_loss: 3.0820e-04\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4879 - reconstruction_loss: 0.4287 - kl_loss: 2.9657e-04\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3028 - reconstruction_loss: 0.4296 - kl_loss: 2.9477e-04\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4481 - reconstruction_loss: 0.4284 - kl_loss: 2.9216e-04\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2703 - reconstruction_loss: 0.4288 - kl_loss: 2.7965e-04\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5956 - reconstruction_loss: 0.4288 - kl_loss: 2.7497e-04\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4121 - reconstruction_loss: 0.4290 - kl_loss: 2.6383e-04\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4303 - reconstruction_loss: 0.4287 - kl_loss: 2.6267e-04\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3074 - reconstruction_loss: 0.5440 - kl_loss: 2.6381e-04\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4503 - reconstruction_loss: 0.4286 - kl_loss: 2.6028e-04\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3802 - reconstruction_loss: 0.4290 - kl_loss: 2.4640e-04\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5216 - reconstruction_loss: 0.4288 - kl_loss: 2.4529e-04\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4574 - reconstruction_loss: 0.4285 - kl_loss: 2.2977e-04\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3472 - reconstruction_loss: 0.4287 - kl_loss: 2.2316e-04\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2943 - reconstruction_loss: 0.4289 - kl_loss: 2.1975e-04\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3240 - reconstruction_loss: 0.4286 - kl_loss: 2.1807e-04\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3363 - reconstruction_loss: 0.5433 - kl_loss: 2.1631e-04\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3961 - reconstruction_loss: 0.4284 - kl_loss: 2.0826e-04\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3764 - reconstruction_loss: 0.4290 - kl_loss: 1.9979e-04\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4232 - reconstruction_loss: 0.4284 - kl_loss: 1.9297e-04\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6157 - reconstruction_loss: 0.4297 - kl_loss: 1.9640e-04\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3529 - reconstruction_loss: 0.4287 - kl_loss: 1.9558e-04\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4081 - reconstruction_loss: 0.4295 - kl_loss: 1.9018e-04\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2695 - reconstruction_loss: 0.5434 - kl_loss: 1.9337e-04\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5773 - reconstruction_loss: 0.4286 - kl_loss: 2.0202e-04\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4117 - reconstruction_loss: 0.4295 - kl_loss: 1.9290e-04\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3870 - reconstruction_loss: 0.4288 - kl_loss: 1.8733e-04\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4149 - reconstruction_loss: 0.5434 - kl_loss: 1.9514e-04\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4221 - reconstruction_loss: 0.4292 - kl_loss: 1.8069e-04\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5550 - reconstruction_loss: 0.4290 - kl_loss: 1.7038e-04\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6619 - reconstruction_loss: 0.4286 - kl_loss: 1.6245e-04\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3205 - reconstruction_loss: 0.4287 - kl_loss: 1.5786e-04\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.8132 - reconstruction_loss: 0.4292 - kl_loss: 1.5270e-04\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.6656 - reconstruction_loss: 0.4291 - kl_loss: 1.4900e-04\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4829 - reconstruction_loss: 0.5437 - kl_loss: 1.5527e-04\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5550 - reconstruction_loss: 0.4294 - kl_loss: 1.5151e-04\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4246 - reconstruction_loss: 0.4293 - kl_loss: 1.4662e-04\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5621 - reconstruction_loss: 0.4287 - kl_loss: 1.4858e-04\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5495 - reconstruction_loss: 0.4284 - kl_loss: 1.4330e-04\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3437 - reconstruction_loss: 0.5434 - kl_loss: 1.4482e-04\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.5103 - reconstruction_loss: 0.4287 - kl_loss: 1.4400e-04\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4809 - reconstruction_loss: 0.5431 - kl_loss: 1.3930e-04\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.4580 - reconstruction_loss: 0.4286 - kl_loss: 1.3826e-04\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.2892 - reconstruction_loss: 0.4290 - kl_loss: 1.3716e-04\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.3249 - reconstruction_loss: 0.5458 - kl_loss: 1.3963e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd6da661ed0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(30,))\n",
    "# x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "# x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Flatten()(x)\n",
    "x = layers.Dense(30, activation=\"tanh\")(encoder_inputs)\n",
    "x = layers.Dense(20, activation=\"tanh\")(x)\n",
    "x = layers.Dense(18, activation=\"tanh\")(x)\n",
    "x = layers.Dense(16, activation=\"tanh\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "# x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "# x = layers.Reshape((7, 7, 64))(x)\n",
    "# x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "x = layers.Dense(16, activation=\"tanh\")(latent_inputs)\n",
    "x = layers.Dense(18, activation=\"tanh\")(x)\n",
    "x = layers.Dense(20, activation=\"tanh\")(x)\n",
    "decoder_outputs = layers.Dense(30, activation=\"tanh\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "#             reconstruction_loss = tf.reduce_mean(\n",
    "#                 tf.reduce_sum(\n",
    "#                     keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "#                 )\n",
    "#             )\n",
    "            reconstruction_loss = tf.keras.losses.MeanSquaredError()(data,reconstruction)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "creditdata = np.concatenate([out_dataset, val_dataset], axis=0)\n",
    "creditdata = np.expand_dims(creditdata, -1).astype(\"float32\")\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=tf.keras.optimizers.legacy.Adam())\n",
    "vae.fit(creditdata, epochs=100, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffb6a39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec Loss =  0.07947672  Case No.  0\n",
      "Rec Loss =  0.10691883  Case No.  1\n",
      "Rec Loss =  0.08086084  Case No.  2\n",
      "Rec Loss =  0.10728254  Case No.  3\n",
      "Rec Loss =  0.08119939  Case No.  4\n",
      "Rec Loss =  0.07774002  Case No.  5\n",
      "Rec Loss =  0.06357469  Case No.  6\n",
      "Rec Loss =  0.10001295  Case No.  7\n",
      "Rec Loss =  0.08098818  Case No.  8\n",
      "Rec Loss =  0.094823666  Case No.  9\n",
      "Rec Loss =  0.07094186  Case No.  10\n",
      "Rec Loss =  0.08174196  Case No.  11\n",
      "Rec Loss =  0.08672131  Case No.  12\n",
      "Rec Loss =  0.0656073  Case No.  13\n",
      "Rec Loss =  0.07251198  Case No.  14\n",
      "Rec Loss =  0.12106066  Case No.  15\n",
      "Rec Loss =  0.10060452  Case No.  16\n",
      "Rec Loss =  0.06803354  Case No.  17\n",
      "Rec Loss =  0.06838033  Case No.  18\n",
      "Rec Loss =  0.07226968  Case No.  19\n",
      "Rec Loss =  0.08470681  Case No.  20\n",
      "Rec Loss =  0.07705819  Case No.  21\n",
      "Rec Loss =  0.07993095  Case No.  22\n",
      "Rec Loss =  0.07472696  Case No.  23\n",
      "Rec Loss =  0.08716389  Case No.  24\n",
      "Rec Loss =  0.07195595  Case No.  25\n",
      "Rec Loss =  0.05473205  Case No.  26\n",
      "Rec Loss =  0.081940435  Case No.  27\n",
      "Rec Loss =  0.09364362  Case No.  28\n",
      "Rec Loss =  0.078984715  Case No.  29\n",
      "Rec Loss =  0.08672697  Case No.  30\n",
      "Rec Loss =  0.0863272  Case No.  31\n",
      "Rec Loss =  0.0777912  Case No.  32\n",
      "Rec Loss =  0.06500311  Case No.  33\n",
      "Rec Loss =  0.052128717  Case No.  34\n",
      "Rec Loss =  0.08402458  Case No.  35\n",
      "Rec Loss =  0.09182357  Case No.  36\n",
      "Rec Loss =  0.10139932  Case No.  37\n",
      "Rec Loss =  0.08597294  Case No.  38\n",
      "Rec Loss =  0.09849324  Case No.  39\n",
      "Rec Loss =  0.08639088  Case No.  40\n",
      "Rec Loss =  0.07568174  Case No.  41\n",
      "Rec Loss =  0.08093817  Case No.  42\n",
      "Rec Loss =  0.0807065  Case No.  43\n",
      "Rec Loss =  0.10033585  Case No.  44\n",
      "Rec Loss =  0.061753966  Case No.  45\n",
      "Rec Loss =  0.075984925  Case No.  46\n",
      "Rec Loss =  0.114897974  Case No.  47\n",
      "Rec Loss =  0.09444143  Case No.  48\n",
      "Rec Loss =  0.077217795  Case No.  49\n",
      "Rec Loss =  0.064141154  Case No.  50\n",
      "Rec Loss =  0.11733235  Case No.  51\n",
      "Rec Loss =  0.09182187  Case No.  52\n",
      "Rec Loss =  0.058421843  Case No.  53\n",
      "Rec Loss =  0.07062968  Case No.  54\n",
      "Rec Loss =  0.103655025  Case No.  55\n",
      "Rec Loss =  0.069178246  Case No.  56\n",
      "Rec Loss =  0.10272126  Case No.  57\n",
      "Rec Loss =  0.102459714  Case No.  58\n",
      "Rec Loss =  0.10086026  Case No.  59\n",
      "Rec Loss =  0.09434493  Case No.  60\n",
      "Rec Loss =  0.07406439  Case No.  61\n",
      "Rec Loss =  0.09349799  Case No.  62\n",
      "Rec Loss =  0.080091946  Case No.  63\n",
      "Rec Loss =  0.07839371  Case No.  64\n",
      "Rec Loss =  0.109842926  Case No.  65\n",
      "Rec Loss =  0.0855224  Case No.  66\n",
      "Rec Loss =  0.08769927  Case No.  67\n",
      "Rec Loss =  0.07599678  Case No.  68\n",
      "Rec Loss =  0.0881118  Case No.  69\n",
      "Rec Loss =  0.08369454  Case No.  70\n",
      "Rec Loss =  0.063463844  Case No.  71\n",
      "Rec Loss =  0.10410543  Case No.  72\n",
      "Rec Loss =  0.086008295  Case No.  73\n",
      "Rec Loss =  0.095706195  Case No.  74\n",
      "Rec Loss =  0.064926915  Case No.  75\n",
      "Rec Loss =  0.08034681  Case No.  76\n",
      "Rec Loss =  0.10467793  Case No.  77\n",
      "Rec Loss =  0.08076556  Case No.  78\n",
      "Rec Loss =  0.084605485  Case No.  79\n",
      "Rec Loss =  0.070901744  Case No.  80\n",
      "Rec Loss =  0.09415943  Case No.  81\n",
      "Rec Loss =  0.086363286  Case No.  82\n",
      "Rec Loss =  0.07639796  Case No.  83\n",
      "Rec Loss =  0.06786825  Case No.  84\n",
      "Rec Loss =  0.07704886  Case No.  85\n",
      "Rec Loss =  0.10021469  Case No.  86\n",
      "Rec Loss =  0.08184991  Case No.  87\n",
      "Rec Loss =  0.076896794  Case No.  88\n",
      "Rec Loss =  0.05765313  Case No.  89\n",
      "Rec Loss =  0.08848227  Case No.  90\n",
      "Rec Loss =  0.07169043  Case No.  91\n",
      "Rec Loss =  0.094506696  Case No.  92\n",
      "Rec Loss =  0.08254995  Case No.  93\n",
      "Rec Loss =  0.08713644  Case No.  94\n",
      "Rec Loss =  0.07651898  Case No.  95\n",
      "Rec Loss =  0.06418015  Case No.  96\n",
      "Rec Loss =  0.054466452  Case No.  97\n",
      "Rec Loss =  0.062779136  Case No.  98\n",
      "Rec Loss =  0.08350051  Case No.  99\n",
      "Rec Loss =  0.08291699  Case No.  100\n",
      "Rec Loss =  0.06846278  Case No.  101\n",
      "Rec Loss =  0.0727828  Case No.  102\n",
      "Rec Loss =  0.1013186  Case No.  103\n",
      "Rec Loss =  0.0870003  Case No.  104\n",
      "Rec Loss =  0.1030823  Case No.  105\n",
      "Rec Loss =  0.065983936  Case No.  106\n",
      "Rec Loss =  0.09650292  Case No.  107\n",
      "Rec Loss =  0.08334323  Case No.  108\n",
      "Rec Loss =  0.07868579  Case No.  109\n",
      "Rec Loss =  0.055614218  Case No.  110\n",
      "Rec Loss =  0.073252596  Case No.  111\n",
      "Rec Loss =  0.095743544  Case No.  112\n",
      "Rec Loss =  0.07496069  Case No.  113\n",
      "Rec Loss =  0.096055515  Case No.  114\n",
      "Rec Loss =  0.07142598  Case No.  115\n",
      "Rec Loss =  0.062779754  Case No.  116\n",
      "Rec Loss =  0.093456514  Case No.  117\n",
      "Rec Loss =  0.09991483  Case No.  118\n",
      "Rec Loss =  0.06753038  Case No.  119\n",
      "Rec Loss =  0.08729796  Case No.  120\n",
      "Rec Loss =  0.0819476  Case No.  121\n",
      "Rec Loss =  0.067068145  Case No.  122\n",
      "Rec Loss =  0.07902018  Case No.  123\n",
      "Rec Loss =  0.08837678  Case No.  124\n",
      "Rec Loss =  0.08063562  Case No.  125\n",
      "Rec Loss =  0.09042182  Case No.  126\n",
      "Rec Loss =  0.08869896  Case No.  127\n",
      "Rec Loss =  0.08676582  Case No.  128\n",
      "Rec Loss =  0.09412872  Case No.  129\n",
      "Rec Loss =  0.083650365  Case No.  130\n",
      "Rec Loss =  0.09079322  Case No.  131\n",
      "Rec Loss =  0.078153916  Case No.  132\n",
      "Rec Loss =  0.10757412  Case No.  133\n",
      "Rec Loss =  0.08597164  Case No.  134\n",
      "Rec Loss =  0.09124317  Case No.  135\n",
      "Rec Loss =  0.07643537  Case No.  136\n",
      "Rec Loss =  0.08558542  Case No.  137\n",
      "Rec Loss =  0.08209349  Case No.  138\n",
      "Rec Loss =  0.086204514  Case No.  139\n",
      "Rec Loss =  0.079834364  Case No.  140\n",
      "Rec Loss =  0.083724596  Case No.  141\n",
      "Rec Loss =  0.1118828  Case No.  142\n",
      "Rec Loss =  0.081420295  Case No.  143\n",
      "Rec Loss =  0.093349904  Case No.  144\n",
      "Rec Loss =  0.09671612  Case No.  145\n",
      "Rec Loss =  0.10567649  Case No.  146\n",
      "Rec Loss =  0.09415167  Case No.  147\n",
      "Rec Loss =  0.082899265  Case No.  148\n",
      "Rec Loss =  0.09524823  Case No.  149\n",
      "Rec Loss =  0.058654908  Case No.  150\n",
      "Rec Loss =  0.06512152  Case No.  151\n",
      "Rec Loss =  0.09073748  Case No.  152\n",
      "Rec Loss =  0.10802203  Case No.  153\n",
      "Rec Loss =  0.1186906  Case No.  154\n",
      "Rec Loss =  0.07644913  Case No.  155\n",
      "Rec Loss =  0.077301756  Case No.  156\n",
      "Rec Loss =  0.08006707  Case No.  157\n",
      "Rec Loss =  0.0849959  Case No.  158\n",
      "Rec Loss =  0.06172102  Case No.  159\n",
      "Rec Loss =  0.09720887  Case No.  160\n",
      "Rec Loss =  0.08485586  Case No.  161\n",
      "Rec Loss =  0.104399316  Case No.  162\n",
      "Rec Loss =  0.10172231  Case No.  163\n",
      "Rec Loss =  0.05810226  Case No.  164\n",
      "Rec Loss =  0.082576364  Case No.  165\n",
      "Rec Loss =  0.08493977  Case No.  166\n",
      "Rec Loss =  0.08459937  Case No.  167\n",
      "Rec Loss =  0.08396475  Case No.  168\n",
      "Rec Loss =  0.10529265  Case No.  169\n",
      "Rec Loss =  0.08427874  Case No.  170\n",
      "Rec Loss =  0.122054145  Case No.  171\n",
      "Rec Loss =  0.10370454  Case No.  172\n",
      "Rec Loss =  0.08640487  Case No.  173\n",
      "Rec Loss =  0.103300944  Case No.  174\n",
      "Rec Loss =  0.07922397  Case No.  175\n",
      "Rec Loss =  0.08965094  Case No.  176\n",
      "Rec Loss =  0.082902975  Case No.  177\n",
      "Rec Loss =  0.074737266  Case No.  178\n",
      "Rec Loss =  0.11238181  Case No.  179\n",
      "Rec Loss =  0.121616624  Case No.  180\n",
      "Rec Loss =  0.090520576  Case No.  181\n",
      "Rec Loss =  0.10636495  Case No.  182\n",
      "Rec Loss =  0.10891516  Case No.  183\n",
      "Rec Loss =  0.09168915  Case No.  184\n",
      "Rec Loss =  0.09968201  Case No.  185\n",
      "Rec Loss =  0.07272803  Case No.  186\n",
      "Rec Loss =  0.07065897  Case No.  187\n",
      "Rec Loss =  0.1014351  Case No.  188\n",
      "Rec Loss =  0.12114253  Case No.  189\n",
      "Rec Loss =  0.07884571  Case No.  190\n",
      "Rec Loss =  0.07021203  Case No.  191\n",
      "Rec Loss =  0.091114126  Case No.  192\n",
      "Rec Loss =  0.08408046  Case No.  193\n",
      "Rec Loss =  0.086504176  Case No.  194\n",
      "Rec Loss =  0.07704522  Case No.  195\n",
      "Rec Loss =  0.099113606  Case No.  196\n",
      "Rec Loss =  0.06440767  Case No.  197\n",
      "Rec Loss =  0.06726388  Case No.  198\n",
      "Rec Loss =  0.07655194  Case No.  199\n",
      "mean =  0.084958136\n",
      "std =  0.0145767005\n"
     ]
    }
   ],
   "source": [
    "rec_loss = []\n",
    "\n",
    "for i in range(val_dataset.shape[0]):\n",
    "        \n",
    "    test = val_dataset[i,:]\n",
    "\n",
    "    test = test.reshape([1,30])\n",
    "\n",
    "    z_mean, z_log_var, z = vae.encoder(test)\n",
    "    reconstruction = vae.decoder(z)\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)\n",
    "\n",
    "    rec_loss.append(reconstruction_loss.numpy())\n",
    "\n",
    "\n",
    "    print(\"Rec Loss = \", reconstruction_loss.numpy(), \" Case No. \", i)\n",
    "\n",
    "rec_loss = np.array(rec_loss)\n",
    "print(\"mean = \", np.mean(rec_loss))\n",
    "print(\"std = \", np.std(rec_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea7d174e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_673686/3892452865.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mz_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_log_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_data' is not defined"
     ]
    }
   ],
   "source": [
    "mean_data = mean_data.reshape([1,30])\n",
    "\n",
    "z_mean, z_log_var, z = vae.encoder(mean_data)\n",
    "reconstruction = vae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(mean_data,reconstruction)\n",
    "\n",
    "print(reconstruction_loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b7b8d61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_targets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_673686/1373858311.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rec Loss = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruction_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" -------- Label = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Case No. \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'val_targets' is not defined"
     ]
    }
   ],
   "source": [
    "test = out_dataset[3,:]\n",
    "\n",
    "test = test.reshape([1,30])\n",
    "\n",
    "z_mean, z_log_var, z = vae.encoder(test)\n",
    "reconstruction = vae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)\n",
    "\n",
    "# rec_loss.append(reconstruction_loss.numpy())\n",
    "\n",
    "\n",
    "print(\"Rec Loss = \", reconstruction_loss.numpy(), \" -------- Label = \", val_targets[i], \" Case No. \", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "5906b2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.077600114\n",
      "0.08050948\n",
      "0.07515724\n",
      "0.081566714\n",
      "0.08005253\n",
      "0.08238045\n",
      "0.06840949\n",
      "0.073207274\n",
      "0.08070919\n",
      "0.07982394\n",
      "MEAN_MEAN =  0.07794164\n"
     ]
    }
   ],
   "source": [
    "out_dataset = random_samples\n",
    "\n",
    "\n",
    "out_dataset[0,0] = 0.2\n",
    "out_dataset[0,3] = 0.2\n",
    "out_dataset[0,4] = 0.2\n",
    "out_dataset[0,7] = 0.2\n",
    "out_dataset[0,9] = 0.2\n",
    "out_dataset[0,14] = 0.2\n",
    "out_dataset[0,22] = 0.2\n",
    "\n",
    "\n",
    "out_dataset[100,0] = 20\n",
    "out_dataset[100,3] = 20\n",
    "out_dataset[100,4] = 20\n",
    "out_dataset[100,7] = 20\n",
    "out_dataset[100,9] = 20\n",
    "out_dataset[100,14] = 20\n",
    "out_dataset[100,22] = 20\n",
    "\n",
    "out_dataset[400,0] = 20\n",
    "out_dataset[400,3] = 20\n",
    "out_dataset[400,4] = 20\n",
    "out_dataset[400,7] = 20\n",
    "out_dataset[400,9] = 20\n",
    "out_dataset[400,14] = 20\n",
    "out_dataset[400,22] = 20\n",
    "\n",
    "out_dataset[600,0] = 20\n",
    "out_dataset[600,3] = 20\n",
    "out_dataset[600,4] = 20\n",
    "out_dataset[600,7] = 20\n",
    "out_dataset[600,9] = 20\n",
    "out_dataset[600,14] = 20\n",
    "out_dataset[600,22] = 20\n",
    "\n",
    "out_dataset[900,0] = 20\n",
    "out_dataset[900,3] = 20\n",
    "out_dataset[900,4] = 20\n",
    "out_dataset[900,7] = 20\n",
    "out_dataset[900,9] = 20\n",
    "out_dataset[900,14] = 20\n",
    "out_dataset[900,22] = 20\n",
    "\n",
    "\n",
    "\n",
    "inliers = out_dataset[5:80,:]\n",
    "mean_data = np.mean(out_dataset, axis=0)\n",
    "outlier1 = val_dataset[0,:]\n",
    "\n",
    "\n",
    "\n",
    "# #------------------ replacing the genes here ---------------------\n",
    "\n",
    "# inliers[:,1] = mean_data[1]\n",
    "\n",
    "# outlier1[1] = mean_data[1]\n",
    "\n",
    "# # ----------------------------------------------------------------\n",
    "\n",
    "mean_mean = []\n",
    "\n",
    "for step in range(10):\n",
    "\n",
    "    outlier1 = outlier1.reshape([1,30])\n",
    "\n",
    "    z_mean, z_log_var, z = vae.encoder(outlier1)\n",
    "    reconstruction = vae.decoder(z)\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(outlier1,reconstruction)\n",
    "\n",
    "    print(reconstruction_loss.numpy())\n",
    "    \n",
    "    mean_mean.append(reconstruction_loss.numpy())\n",
    "\n",
    "print(\"MEAN_MEAN = \", np.mean(np.array(mean_mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f0319f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011079184\n",
      "0.00035862127\n",
      "0.00019719994\n",
      "0.00015978632\n",
      "0.00019239521\n",
      "0.0002868758\n",
      "0.0004647649\n",
      "0.00041914513\n",
      "0.00020846368\n",
      "0.0002250349\n",
      "MEAN_MEAN =  0.00036202057\n"
     ]
    }
   ],
   "source": [
    "mean_mean = []\n",
    "\n",
    "for step in range(10):\n",
    "\n",
    "    mean_data = mean_data.reshape([1,30])\n",
    "\n",
    "    z_mean, z_log_var, z = vae.encoder(mean_data)\n",
    "    reconstruction = vae.decoder(z)\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(mean_data,reconstruction)\n",
    "\n",
    "    print(reconstruction_loss.numpy())\n",
    "    \n",
    "    mean_mean.append(reconstruction_loss.numpy())\n",
    "\n",
    "print(\"MEAN_MEAN = \", np.mean(np.array(mean_mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "673648e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean =  0.08066361\n",
      "mean =  0.08035765\n",
      "mean =  0.080403335\n",
      "mean =  0.08022572\n",
      "mean =  0.080229364\n",
      "mean =  0.080227144\n",
      "mean =  0.08026449\n",
      "mean =  0.0800722\n",
      "mean =  0.07993385\n",
      "mean =  0.08020754\n",
      "MEAN_MEAN =  0.080258496\n"
     ]
    }
   ],
   "source": [
    "mean_mean = []\n",
    "\n",
    "for step in range(10):\n",
    "\n",
    "    mean_ins_error = []\n",
    "\n",
    "    for index in range(inliers.shape[0]):\n",
    "\n",
    "        cand = inliers[index,:]\n",
    "\n",
    "        cand = cand.reshape([1,30])\n",
    "\n",
    "        mean_data = mean_data.reshape([1,30])\n",
    "\n",
    "        z_mean, z_log_var, z = vae.encoder(cand)\n",
    "        reconstruction = vae.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(cand,reconstruction)\n",
    "\n",
    "        mean_ins_error.append(reconstruction_loss.numpy())\n",
    "\n",
    "    #     print(\"MEAN\", index, \" = \", reconstruction_loss.numpy())\n",
    "\n",
    "    mean_ins_error = np.array(mean_ins_error)\n",
    "    print(\"mean = \", np.mean(mean_ins_error))\n",
    "    \n",
    "    mean_mean.append(np.mean(mean_ins_error))\n",
    "    \n",
    "\n",
    "print(\"MEAN_MEAN = \", np.mean(np.array(mean_mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bf748ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "942.0569754394712"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "75.56239/0.08021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "55c4ed35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1813"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "660+260+240+125+128+400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b8bdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d86ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb1636f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "79bedbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygad \n",
    "\n",
    "\n",
    "out_dataset = random_samples\n",
    "\n",
    "\n",
    "out_dataset[0,0] = 20\n",
    "out_dataset[0,1] = 20\n",
    "out_dataset[0,2] = 20\n",
    "out_dataset[0,3] = 20\n",
    "out_dataset[0,4] = 20\n",
    "out_dataset[0,5] = 20\n",
    "out_dataset[0,6] = 20\n",
    "\n",
    "\n",
    "out_dataset[100,0] = 20\n",
    "out_dataset[100,3] = 20\n",
    "out_dataset[100,4] = 20\n",
    "out_dataset[100,7] = 20\n",
    "out_dataset[100,9] = 20\n",
    "out_dataset[100,14] = 20\n",
    "out_dataset[100,22] = 20\n",
    "\n",
    "out_dataset[400,0] = 20\n",
    "out_dataset[400,3] = 20\n",
    "out_dataset[400,4] = 20\n",
    "out_dataset[400,7] = 20\n",
    "out_dataset[400,9] = 20\n",
    "out_dataset[400,14] = 20\n",
    "out_dataset[400,22] = 20\n",
    "\n",
    "out_dataset[600,0] = 20\n",
    "out_dataset[600,3] = 20\n",
    "out_dataset[600,4] = 20\n",
    "out_dataset[600,7] = 20\n",
    "out_dataset[600,9] = 20\n",
    "out_dataset[600,14] = 20\n",
    "out_dataset[600,22] = 20\n",
    "\n",
    "out_dataset[900,0] = 20\n",
    "out_dataset[900,3] = 20\n",
    "out_dataset[900,4] = 20\n",
    "out_dataset[900,7] = 20\n",
    "out_dataset[900,9] = 20\n",
    "out_dataset[900,14] = 20\n",
    "out_dataset[900,22] = 20\n",
    "\n",
    "\n",
    "def fitness_func_avg(ga_instance, solution, solution_idx):\n",
    "    \n",
    "    inliers = out_dataset[10:15,:]\n",
    "    \n",
    "    avg_ins = np.mean(out_dataset[10:15,:], axis=0)\n",
    "    avg_ins = avg_ins.reshape([1,30])\n",
    "    \n",
    "    particle = out_dataset[0,:]\n",
    "    particle = particle.reshape([1,30])\n",
    "    \n",
    "#     abn_subspace = solution * val_features[6728,:]\n",
    "    \n",
    "#     abn_subspace = abn_subspace.reshape([1,30])\n",
    "\n",
    "    avg_in_rec = []\n",
    "    \n",
    "    for index in range(inliers.shape[0]):\n",
    "        \n",
    "        candidate_inlier = inliers[index,:]\n",
    "        candidate_inlier = candidate_inlier.reshape([1,30])\n",
    "        \n",
    "        in_remain = candidate_inlier * solution\n",
    "        \n",
    "        in_normal_subspace = 1 - solution\n",
    "        \n",
    "        in_replace = in_normal_subspace * avg_ins\n",
    "        \n",
    "        in_candidate = in_remain + in_replace\n",
    "        \n",
    "        z_mean, z_log_var, z = vae.encoder(in_candidate)\n",
    "        in_candidate_rec = vae.decoder(z)\n",
    "        \n",
    "        \n",
    "        rec_loss = tf.keras.losses.MeanSquaredError()(in_candidate,in_candidate_rec)\n",
    "        \n",
    "        avg_in_rec.append(rec_loss.numpy())\n",
    "    \n",
    "    avg_in_rec = np.array(avg_in_rec)\n",
    "    avg_in_rec = np.mean(avg_in_rec)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "#     z_mean, z_log_var, z = vae.encoder(particle)\n",
    "#     reconstruction_1 = vae.decoder(z)\n",
    "    \n",
    "    out_remain = particle * solution\n",
    "    \n",
    "    out_normal_subspace = 1 - solution\n",
    "    \n",
    "    out_replace = avg_ins * out_normal_subspace\n",
    "    \n",
    "    out_candidate = out_remain + out_replace\n",
    "    \n",
    "    \n",
    "    z_mean, z_log_var, z = vae.encoder(out_candidate)\n",
    "    out_candidate_rec = vae.decoder(z)\n",
    "    \n",
    "    rec_loss = tf.keras.losses.MeanSquaredError()(out_candidate,out_candidate_rec)\n",
    "    rec_loss = rec_loss.numpy()\n",
    "    \n",
    "    fitness = rec_loss / avg_in_rec\n",
    "    \n",
    "    return fitness\n",
    "\n",
    "def on_generation(ga):\n",
    "    print(\"Generation\", ga.generations_completed)\n",
    "    \n",
    "    solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "    \n",
    "    print(solution_fitness)\n",
    "\n",
    "\n",
    "fitness_function = fitness_func_avg\n",
    "\n",
    "num_generations = 100\n",
    "num_parents_mating = 4\n",
    "\n",
    "sol_per_pop = 100\n",
    "num_genes = out_dataset.shape[1]\n",
    "\n",
    "init_range_low = -2\n",
    "init_range_high = 5\n",
    "\n",
    "parent_selection_type = \"sss\"\n",
    "keep_parents = 1\n",
    "\n",
    "space = [[0,1] for i in range(num_genes)]\n",
    "\n",
    "crossover_type = \"single_point\"\n",
    "\n",
    "mutation_type = \"random\"\n",
    "mutation_percent_genes = 20\n",
    "\n",
    "ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                       num_parents_mating=num_parents_mating,\n",
    "                       fitness_func=fitness_function,\n",
    "                       sol_per_pop=sol_per_pop,\n",
    "                       num_genes=num_genes,\n",
    "                       init_range_low=init_range_low,\n",
    "                       init_range_high=init_range_high,\n",
    "                       parent_selection_type=parent_selection_type,\n",
    "                       keep_parents=keep_parents,\n",
    "                       crossover_type=crossover_type,\n",
    "                       mutation_type=mutation_type,\n",
    "                       mutation_percent_genes=mutation_percent_genes,\n",
    "                       on_generation=on_generation,\n",
    "                       gene_space = space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dc7fafd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1\n",
      "3395.2341\n",
      "Generation 2\n",
      "3246.3584\n",
      "Generation 3\n",
      "3360.5186\n",
      "Generation 4\n",
      "3323.6565\n",
      "Generation 5\n",
      "3622.429\n",
      "Generation 6\n",
      "3622.429\n",
      "Generation 7\n",
      "3622.429\n",
      "Generation 8\n",
      "3622.429\n",
      "Generation 9\n",
      "3622.429\n",
      "Generation 10\n",
      "3622.429\n",
      "Generation 11\n",
      "3622.429\n",
      "Generation 12\n",
      "3622.429\n",
      "Generation 13\n",
      "3622.429\n",
      "Generation 14\n",
      "3622.429\n",
      "Generation 15\n",
      "3622.429\n",
      "Generation 16\n",
      "3622.429\n",
      "Generation 17\n",
      "3622.429\n",
      "Generation 18\n",
      "3622.429\n",
      "Generation 19\n",
      "3622.429\n",
      "Generation 20\n",
      "3622.429\n",
      "Generation 21\n",
      "3622.429\n",
      "Generation 22\n",
      "3622.429\n",
      "Generation 23\n",
      "3622.429\n",
      "Generation 24\n",
      "3622.429\n",
      "Generation 25\n",
      "3622.429\n",
      "Generation 26\n",
      "3622.429\n",
      "Generation 27\n",
      "3622.429\n",
      "Generation 28\n",
      "3622.429\n",
      "Generation 29\n",
      "3622.429\n",
      "Generation 30\n",
      "3622.429\n",
      "Generation 31\n",
      "3705.433\n",
      "Generation 32\n",
      "3702.5957\n",
      "Generation 33\n",
      "3702.5957\n",
      "Generation 34\n",
      "3702.5957\n",
      "Generation 35\n",
      "3977.9626\n",
      "Generation 36\n",
      "3964.604\n",
      "Generation 37\n",
      "3964.604\n",
      "Generation 38\n",
      "3964.604\n",
      "Generation 39\n",
      "3964.604\n",
      "Generation 40\n",
      "3964.604\n",
      "Generation 41\n",
      "3964.604\n",
      "Generation 42\n",
      "3964.604\n",
      "Generation 43\n",
      "3964.604\n",
      "Generation 44\n",
      "3964.604\n",
      "Generation 45\n",
      "3964.604\n",
      "Generation 46\n",
      "3964.604\n",
      "Generation 47\n",
      "3964.604\n",
      "Generation 48\n",
      "3964.604\n",
      "Generation 49\n",
      "3964.604\n",
      "Generation 50\n",
      "3964.604\n",
      "Generation 51\n",
      "3964.604\n",
      "Generation 52\n",
      "3964.604\n",
      "Generation 53\n",
      "3964.604\n",
      "Generation 54\n",
      "3964.604\n",
      "Generation 55\n",
      "3964.604\n",
      "Generation 56\n",
      "3964.604\n",
      "Generation 57\n",
      "3964.604\n",
      "Generation 58\n",
      "3964.604\n",
      "Generation 59\n",
      "3964.604\n",
      "Generation 60\n",
      "3964.604\n",
      "Generation 61\n",
      "3964.604\n",
      "Generation 62\n",
      "3964.604\n",
      "Generation 63\n",
      "3964.604\n",
      "Generation 64\n",
      "3964.604\n",
      "Generation 65\n",
      "3964.604\n",
      "Generation 66\n",
      "3964.604\n",
      "Generation 67\n",
      "3964.604\n",
      "Generation 68\n",
      "3964.604\n",
      "Generation 69\n",
      "3964.604\n",
      "Generation 70\n",
      "3964.604\n",
      "Generation 71\n",
      "3964.604\n",
      "Generation 72\n",
      "3964.604\n",
      "Generation 73\n",
      "3964.604\n",
      "Generation 74\n",
      "3964.604\n",
      "Generation 75\n",
      "3964.604\n",
      "Generation 76\n",
      "3964.604\n",
      "Generation 77\n",
      "3964.604\n",
      "Generation 78\n",
      "3964.604\n",
      "Generation 79\n",
      "3964.604\n",
      "Generation 80\n",
      "3964.604\n",
      "Generation 81\n",
      "3964.604\n",
      "Generation 82\n",
      "3964.604\n",
      "Generation 83\n",
      "3964.604\n",
      "Generation 84\n",
      "3964.604\n",
      "Generation 85\n",
      "3964.604\n",
      "Generation 86\n",
      "3964.604\n",
      "Generation 87\n",
      "3964.604\n",
      "Generation 88\n",
      "3964.604\n",
      "Generation 89\n",
      "3964.604\n",
      "Generation 90\n",
      "3964.604\n",
      "Generation 91\n",
      "3964.604\n",
      "Generation 92\n",
      "3964.604\n",
      "Generation 93\n",
      "3964.604\n",
      "Generation 94\n",
      "3964.604\n",
      "Generation 95\n",
      "3964.604\n",
      "Generation 96\n",
      "3964.604\n",
      "Generation 97\n",
      "3964.604\n",
      "Generation 98\n",
      "3964.604\n",
      "Generation 99\n",
      "3964.604\n",
      "Generation 100\n",
      "3964.604\n"
     ]
    }
   ],
   "source": [
    "ga_instance.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9ddcfdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters of the best solution : [1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 0. 0. 0. 0. 0.]\n",
      "Fitness value of the best solution = 3964.60400390625\n"
     ]
    }
   ],
   "source": [
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "print(\"Parameters of the best solution : {solution}\".format(solution=solution))\n",
    "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb3af35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out_dataset[900,0] = 20\n",
    "out_dataset[900,3] = 20\n",
    "out_dataset[900,4] = 20\n",
    "out_dataset[900,7] = 20\n",
    "out_dataset[900,9] = 20\n",
    "out_dataset[900,14] = 20\n",
    "out_dataset[900,22] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cd346b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04bfbc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1eb969d2",
   "metadata": {},
   "source": [
    "# Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "97e1e2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "num_samples = 10000\n",
    "num_dimensions = 30\n",
    "\n",
    "# Generate random samples\n",
    "raw_data = np.random.rand(num_samples, num_dimensions)\n",
    "\n",
    "outlier_indices_1 = [0,1,2,3,4,5,6]\n",
    "outlyin_amount_1 = 20\n",
    "outlier_indices_2 = [0,2,4,6,8,10,12]\n",
    "outlyin_amount_2 = 10\n",
    "outlier_indices_3 = [0,3,6,9,12,15,18]\n",
    "outlyin_amount_3 = 5\n",
    "outlier_indices_4 = [0,4,8,12,16,20,24]\n",
    "outlyin_amount_4 = 2\n",
    "outlier_indices_5 = [0,5,10,15,20,25,29]\n",
    "outlyin_amount_5 = 1.1\n",
    "\n",
    "corrupted_data = raw_data\n",
    "\n",
    "corrupted_data[100:120, outlier_indices_1] = outlyin_amount_1\n",
    "corrupted_data[200:220, outlier_indices_2] = outlyin_amount_2\n",
    "corrupted_data[300:320, outlier_indices_3] = outlyin_amount_3\n",
    "corrupted_data[400:420, outlier_indices_4] = outlyin_amount_4\n",
    "corrupted_data[500:520, outlier_indices_5] = outlyin_amount_5\n",
    "\n",
    "# print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "470d324d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00000000e+01, 2.00000000e+01, 2.00000000e+01, 2.00000000e+01,\n",
       "       2.00000000e+01, 2.00000000e+01, 2.00000000e+01, 5.34271818e-02,\n",
       "       7.25594364e-01, 1.14274586e-02, 7.70580749e-01, 1.46946645e-01,\n",
       "       7.95220826e-02, 8.96030342e-02, 6.72047807e-01, 2.45367210e-01,\n",
       "       4.20539467e-01, 5.57368791e-01, 8.60551174e-01, 7.27044263e-01,\n",
       "       2.70327905e-01, 1.31482799e-01, 5.53743204e-02, 3.01598634e-01,\n",
       "       2.62118149e-01, 4.56140567e-01, 6.83281336e-01, 6.95625446e-01,\n",
       "       2.83518847e-01, 3.79926956e-01])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a4a3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee4b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
