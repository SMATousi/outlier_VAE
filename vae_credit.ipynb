{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91d009a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 12:06:07.648344: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651470eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3516357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 10:33:57.707973: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 28, 28, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 14, 14, 32)   320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 7, 7, 64)     18496       ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 3136)         0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 16)           50192       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " z_mean (Dense)                 (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " z_log_var (Dense)              (None, 2)            34          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 \n",
      "                                                                  'z_log_var[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 69,076\n",
      "Trainable params: 69,076\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb6afd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 2)]               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3136)              9408      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 7, 7, 64)          0         \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 14, 14, 64)       36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 28, 28, 32)       18464     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 28, 28, 1)        289       \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65,089\n",
      "Trainable params: 65,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37049328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(\n",
    "                    keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "                )\n",
    "            )\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e22a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "Epoch 1/2\n",
      "547/547 [==============================] - 66s 115ms/step - loss: 248.3755 - reconstruction_loss: 205.1570 - kl_loss: 3.4305\n",
      "Epoch 2/2\n",
      "547/547 [==============================] - 69s 126ms/step - loss: 178.6454 - reconstruction_loss: 167.3625 - kl_loss: 5.1407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb91645a450>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, _), (x_test, _) = keras.datasets.mnist.load_data()\n",
    "mnist_digits = np.concatenate([x_train, x_test], axis=0)\n",
    "mnist_digits = np.expand_dims(mnist_digits, -1).astype(\"float32\") / 255\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(mnist_digits, epochs=2, batch_size=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a421872f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADER: \"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
      "EXAMPLE FEATURES: [0.0, -1.3598071336738, -0.0727811733098497, 2.53634673796914, 1.37815522427443, -0.338320769942518, 0.462387777762292, 0.239598554061257, 0.0986979012610507, 0.363786969611213, 0.0907941719789316, -0.551599533260813, -0.617800855762348, -0.991389847235408, -0.311169353699879, 1.46817697209427, -0.470400525259478, 0.207971241929242, 0.0257905801985591, 0.403992960255733, 0.251412098239705, -0.018306777944153, 0.277837575558899, -0.110473910188767, 0.0669280749146731, 0.128539358273528, -0.189114843888824, 0.133558376740387, -0.0210530534538215, 149.62]\n",
      "features.shape: (284807, 30)\n",
      "targets.shape: (284807, 1)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/\n",
    "fname = \"/home/ali/vae/archive/creditcard.csv\"\n",
    "\n",
    "all_features = []\n",
    "all_targets = []\n",
    "with open(fname) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(\"HEADER:\", line.strip())\n",
    "            continue  # Skip header\n",
    "        fields = line.strip().split(\",\")\n",
    "        all_features.append([float(v.replace('\"', \"\")) for v in fields[:-1]])\n",
    "        all_targets.append([int(fields[-1].replace('\"', \"\"))])\n",
    "        if i == 1:\n",
    "            print(\"EXAMPLE FEATURES:\", all_features[-1])\n",
    "\n",
    "features = np.array(all_features, dtype=\"float32\")\n",
    "targets = np.array(all_targets, dtype=\"uint8\")\n",
    "print(\"features.shape:\", features.shape)\n",
    "print(\"targets.shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "676f201a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 227846\n",
      "Number of validation samples: 56961\n"
     ]
    }
   ],
   "source": [
    "num_val_samples = int(len(features) * 0.2)\n",
    "train_features = features[:-num_val_samples]\n",
    "train_targets = targets[:-num_val_samples]\n",
    "val_features = features[-num_val_samples:]\n",
    "val_targets = targets[-num_val_samples:]\n",
    "\n",
    "print(\"Number of training samples:\", len(train_features))\n",
    "print(\"Number of validation samples:\", len(val_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b91d1a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples in training data: 417 (0.18% of total)\n"
     ]
    }
   ],
   "source": [
    "counts = np.bincount(train_targets[:, 0])\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(train_targets)\n",
    "    )\n",
    ")\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "270cc1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(train_features, axis=0)\n",
    "train_features -= mean\n",
    "val_features -= mean\n",
    "std = np.std(train_features, axis=0)\n",
    "train_features /= std\n",
    "val_features /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17e5ed00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 12:07:11.701677: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(30,))\n",
    "# x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "# x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Flatten()(x)\n",
    "x = layers.Dense(30, activation=\"tanh\")(encoder_inputs)\n",
    "x = layers.Dense(20, activation=\"tanh\")(x)\n",
    "x = layers.Dense(18, activation=\"tanh\")(x)\n",
    "x = layers.Dense(16, activation=\"tanh\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "# x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "# x = layers.Reshape((7, 7, 64))(x)\n",
    "# x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "x = layers.Dense(16, activation=\"tanh\")(latent_inputs)\n",
    "x = layers.Dense(18, activation=\"tanh\")(x)\n",
    "x = layers.Dense(20, activation=\"tanh\")(x)\n",
    "decoder_outputs = layers.Dense(30, activation=\"tanh\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8459cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "#             reconstruction_loss = tf.reduce_mean(\n",
    "#                 tf.reduce_sum(\n",
    "#                     keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "#                 )\n",
    "#             )\n",
    "            reconstruction_loss = tf.keras.losses.MeanSquaredError()(data,reconstruction)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d68e9af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2226/2226 [==============================] - 12s 4ms/step - loss: 1.0353 - reconstruction_loss: 1.0053 - kl_loss: 0.0033\n",
      "Epoch 2/10\n",
      "2226/2226 [==============================] - 10s 4ms/step - loss: 0.9919 - reconstruction_loss: 1.0045 - kl_loss: 2.7358e-05\n",
      "Epoch 3/10\n",
      "2226/2226 [==============================] - 9s 4ms/step - loss: 0.9895 - reconstruction_loss: 1.0045 - kl_loss: 7.4854e-06\n",
      "Epoch 4/10\n",
      "2226/2226 [==============================] - 10s 5ms/step - loss: 0.9985 - reconstruction_loss: 1.0046 - kl_loss: 2.5930e-06\n",
      "Epoch 5/10\n",
      "2226/2226 [==============================] - 10s 4ms/step - loss: 1.0088 - reconstruction_loss: 1.0050 - kl_loss: 1.3118e-06\n",
      "Epoch 6/10\n",
      "2226/2226 [==============================] - 10s 4ms/step - loss: 1.0088 - reconstruction_loss: 1.0045 - kl_loss: 1.2981e-06\n",
      "Epoch 7/10\n",
      "2226/2226 [==============================] - 10s 4ms/step - loss: 0.9918 - reconstruction_loss: 1.0045 - kl_loss: 1.0868e-06\n",
      "Epoch 8/10\n",
      "2226/2226 [==============================] - 10s 5ms/step - loss: 1.0031 - reconstruction_loss: 1.0044 - kl_loss: 9.8672e-07\n",
      "Epoch 9/10\n",
      "2226/2226 [==============================] - 10s 5ms/step - loss: 1.0227 - reconstruction_loss: 1.0043 - kl_loss: 5.1649e-07\n",
      "Epoch 10/10\n",
      "2226/2226 [==============================] - 10s 5ms/step - loss: 0.9991 - reconstruction_loss: 1.0043 - kl_loss: 4.7192e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5826782190>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "creditdata = np.concatenate([train_features, val_features], axis=0)\n",
    "creditdata = np.expand_dims(creditdata, -1).astype(\"float32\")\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())\n",
    "vae.fit(creditdata, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "565eba6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56961, 30)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1406bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = train_features[1,:]\n",
    "\n",
    "test = test.reshape([1,30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d3f7b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_mean, z_log_var, z = vae.encoder(test)\n",
    "reconstruction = vae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8cc1db9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41741496"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18959388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41229054,  0.02760492,  0.00990844, -0.13348393, -0.03093056,\n",
       "         0.04025487, -0.02018991,  0.01525421, -0.0075708 ,  0.0047708 ,\n",
       "         0.01919036, -0.07557505,  0.04402744, -0.0281549 , -0.02722586,\n",
       "        -0.04202577,  0.0039289 , -0.01101142,  0.02211242,  0.00563244,\n",
       "        -0.01162461,  0.01399416,  0.0240715 ,  0.01843469, -0.0130402 ,\n",
       "        -0.07930604, -0.01005057, -0.0105467 ,  0.00252189, -0.00742954]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "321b2d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.0008564 ,  0.64784056,  0.16869006, -0.01075485,  0.28652602,\n",
       "         0.09012804, -0.08543706, -0.04230526,  0.0670673 , -0.2293523 ,\n",
       "        -0.15108545,  1.4866744 ,  1.0515386 ,  0.4633228 , -0.18291788,\n",
       "         0.62796277,  0.5300628 , -0.14867269, -0.1904353 , -0.16889915,\n",
       "        -0.10145202, -0.29439253, -0.8689419 ,  0.17637353, -0.56267697,\n",
       "         0.25427756,  0.25284111, -0.0230066 ,  0.03819847, -0.3517778 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c1b318f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rec Loss =  7.518621  -------- Label =  [1]  Case No.  1866\n",
      "Rec Loss =  3.3214269  -------- Label =  [1]  Case No.  1884\n",
      "Rec Loss =  10.046226  -------- Label =  [1]  Case No.  2230\n",
      "Rec Loss =  6.4760523  -------- Label =  [1]  Case No.  2630\n",
      "Rec Loss =  5.1012144  -------- Label =  [1]  Case No.  4132\n",
      "Rec Loss =  5.3102617  -------- Label =  [1]  Case No.  5412\n",
      "Rec Loss =  12.018083  -------- Label =  [1]  Case No.  6728\n",
      "Rec Loss =  5.242398  -------- Label =  [1]  Case No.  6786\n",
      "Rec Loss =  5.2372904  -------- Label =  [1]  Case No.  6787\n",
      "Rec Loss =  6.823598  -------- Label =  [1]  Case No.  6859\n",
      "Rec Loss =  8.673994  -------- Label =  [1]  Case No.  7770\n",
      "Rec Loss =  8.768185  -------- Label =  [1]  Case No.  7788\n",
      "Rec Loss =  8.026488  -------- Label =  [1]  Case No.  7798\n",
      "Rec Loss =  7.537183  -------- Label =  [1]  Case No.  9261\n",
      "Rec Loss =  4.942793  -------- Label =  [1]  Case No.  9580\n",
      "Rec Loss =  15.527534  -------- Label =  [1]  Case No.  10376\n",
      "Rec Loss =  4.081621  -------- Label =  [1]  Case No.  10520\n",
      "Rec Loss =  1.3889285  -------- Label =  [1]  Case No.  10620\n",
      "Rec Loss =  3.4451296  -------- Label =  [1]  Case No.  11653\n",
      "Rec Loss =  3.3472002  -------- Label =  [1]  Case No.  11655\n",
      "Rec Loss =  2.231992  -------- Label =  [1]  Case No.  12376\n",
      "Rec Loss =  2.5654826  -------- Label =  [1]  Case No.  13408\n",
      "Rec Loss =  7.9517384  -------- Label =  [1]  Case No.  13599\n",
      "Rec Loss =  12.217542  -------- Label =  [1]  Case No.  15547\n",
      "Rec Loss =  11.59644  -------- Label =  [1]  Case No.  15701\n",
      "Rec Loss =  17.21053  -------- Label =  [1]  Case No.  15853\n",
      "Rec Loss =  17.084362  -------- Label =  [1]  Case No.  15903\n",
      "Rec Loss =  16.816175  -------- Label =  [1]  Case No.  16002\n",
      "Rec Loss =  16.603888  -------- Label =  [1]  Case No.  16158\n",
      "Rec Loss =  16.44059  -------- Label =  [1]  Case No.  16487\n",
      "Rec Loss =  0.93735534  -------- Label =  [1]  Case No.  17501\n",
      "Rec Loss =  0.8146561  -------- Label =  [1]  Case No.  17710\n",
      "Rec Loss =  6.0953455  -------- Label =  [1]  Case No.  19827\n",
      "Rec Loss =  3.6017768  -------- Label =  [1]  Case No.  20149\n",
      "Rec Loss =  7.453237  -------- Label =  [1]  Case No.  20450\n",
      "Rec Loss =  2.1319098  -------- Label =  [1]  Case No.  21125\n",
      "Rec Loss =  6.265192  -------- Label =  [1]  Case No.  21321\n",
      "Rec Loss =  1.812343  -------- Label =  [1]  Case No.  21393\n",
      "Rec Loss =  7.9023542  -------- Label =  [1]  Case No.  21761\n",
      "Rec Loss =  9.864085  -------- Label =  [1]  Case No.  21982\n",
      "Rec Loss =  12.25763  -------- Label =  [1]  Case No.  22117\n",
      "Rec Loss =  12.021208  -------- Label =  [1]  Case No.  22915\n",
      "Rec Loss =  11.888178  -------- Label =  [1]  Case No.  23631\n",
      "Rec Loss =  8.593738  -------- Label =  [1]  Case No.  24020\n",
      "Rec Loss =  1.6101412  -------- Label =  [1]  Case No.  24035\n",
      "Rec Loss =  1.5795468  -------- Label =  [1]  Case No.  24045\n",
      "Rec Loss =  9.0892515  -------- Label =  [1]  Case No.  24058\n",
      "Rec Loss =  11.898467  -------- Label =  [1]  Case No.  24278\n",
      "Rec Loss =  11.73923  -------- Label =  [1]  Case No.  24928\n",
      "Rec Loss =  0.548644  -------- Label =  [1]  Case No.  26498\n",
      "Rec Loss =  0.9737219  -------- Label =  [1]  Case No.  26549\n",
      "Rec Loss =  5.2584386  -------- Label =  [1]  Case No.  27557\n",
      "Rec Loss =  8.055904  -------- Label =  [1]  Case No.  27710\n",
      "Rec Loss =  6.716641  -------- Label =  [1]  Case No.  30557\n",
      "Rec Loss =  3.422318  -------- Label =  [1]  Case No.  33210\n",
      "Rec Loss =  3.931028  -------- Label =  [1]  Case No.  33627\n",
      "Rec Loss =  3.7291398  -------- Label =  [1]  Case No.  34079\n",
      "Rec Loss =  11.874956  -------- Label =  [1]  Case No.  34714\n",
      "Rec Loss =  14.075636  -------- Label =  [1]  Case No.  34980\n",
      "Rec Loss =  0.92487085  -------- Label =  [1]  Case No.  35234\n",
      "Rec Loss =  13.837084  -------- Label =  [1]  Case No.  35428\n",
      "Rec Loss =  10.566635  -------- Label =  [1]  Case No.  35478\n",
      "Rec Loss =  10.218656  -------- Label =  [1]  Case No.  36031\n",
      "Rec Loss =  9.115311  -------- Label =  [1]  Case No.  40529\n",
      "Rec Loss =  12.157335  -------- Label =  [1]  Case No.  44675\n",
      "Rec Loss =  7.045984  -------- Label =  [1]  Case No.  46536\n",
      "Rec Loss =  2.0389097  -------- Label =  [1]  Case No.  46629\n",
      "Rec Loss =  1.1885357  -------- Label =  [1]  Case No.  48146\n",
      "Rec Loss =  0.7222474  -------- Label =  [1]  Case No.  48225\n",
      "Rec Loss =  3.5176  -------- Label =  [1]  Case No.  49018\n",
      "Rec Loss =  6.0483475  -------- Label =  [1]  Case No.  52017\n",
      "Rec Loss =  3.4067676  -------- Label =  [1]  Case No.  52297\n",
      "Rec Loss =  3.138141  -------- Label =  [1]  Case No.  52303\n",
      "Rec Loss =  5.859358  -------- Label =  [1]  Case No.  53298\n",
      "Rec Loss =  0.69356674  -------- Label =  [1]  Case No.  53828\n",
      "mean =  7.0689917\n",
      "std =  4.660659\n"
     ]
    }
   ],
   "source": [
    "rec_loss = []\n",
    "\n",
    "for i in range(val_features.shape[0]):\n",
    "    \n",
    "    if val_targets[i] == 1:\n",
    "        \n",
    "        test = val_features[i,:]\n",
    "\n",
    "        test = test.reshape([1,30])\n",
    "\n",
    "        z_mean, z_log_var, z = vae.encoder(test)\n",
    "        reconstruction = vae.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)\n",
    "        \n",
    "        rec_loss.append(reconstruction_loss.numpy())\n",
    "\n",
    "\n",
    "        print(\"Rec Loss = \", reconstruction_loss.numpy(), \" -------- Label = \", val_targets[i], \" Case No. \", i)\n",
    "\n",
    "rec_loss = np.array(rec_loss)\n",
    "print(\"mean = \", np.mean(rec_loss))\n",
    "print(\"std = \", np.std(rec_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8bfe2279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.7462541 ,  -0.50769436,   2.6397295 ,  -4.164056  ,\n",
       "         3.7277195 ,   1.124849  ,  -1.683369  ,  -1.2245529 ,\n",
       "         0.51499057,  -3.5956087 ,  -5.744471  ,   3.3477724 ,\n",
       "        -7.4962306 ,  -1.657718  , -12.934678  ,  -0.2816221 ,\n",
       "        -1.6018841 ,  -3.7281365 ,   0.09827992,  -2.1891088 ,\n",
       "         0.669623  ,   0.54346186,  -1.3017883 ,  -0.5808547 ,\n",
       "        -0.36568624,   0.4449785 ,   0.09471209,   2.115266  ,\n",
       "         1.5862252 ,  -0.36251542], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_features[6728,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3b2326",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
