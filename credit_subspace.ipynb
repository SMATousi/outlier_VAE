{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba529fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 13:26:50.800287: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-17 13:26:50.863117: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-17 13:26:52.911202: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/macula/SMATousi/.conda/envs/mac-deep/lib/:/home/macula/SMATousi/.conda/envs/mac-deep/lib/python3.7/site-packages/nvidia/cudnn/lib\n",
      "2023-07-17 13:26:52.911423: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/macula/SMATousi/.conda/envs/mac-deep/lib/:/home/macula/SMATousi/.conda/envs/mac-deep/lib/python3.7/site-packages/nvidia/cudnn/lib\n",
      "2023-07-17 13:26:52.911439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5720365588546116564\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 22572105728\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 57147650234564996\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-17 13:26:58.890827: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-17 13:26:58.953787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 13:26:58.986714: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 13:26:58.986791: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 13:26:59.255991: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 13:26:59.256091: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 13:26:59.256138: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-17 13:26:59.256186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 21526 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pygad\n",
    "import wandb\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2eeb41",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097b80bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADER: \"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
      "EXAMPLE FEATURES: [0.0, -1.3598071336738, -0.0727811733098497, 2.53634673796914, 1.37815522427443, -0.338320769942518, 0.462387777762292, 0.239598554061257, 0.0986979012610507, 0.363786969611213, 0.0907941719789316, -0.551599533260813, -0.617800855762348, -0.991389847235408, -0.311169353699879, 1.46817697209427, -0.470400525259478, 0.207971241929242, 0.0257905801985591, 0.403992960255733, 0.251412098239705, -0.018306777944153, 0.277837575558899, -0.110473910188767, 0.0669280749146731, 0.128539358273528, -0.189114843888824, 0.133558376740387, -0.0210530534538215, 149.62]\n",
      "features.shape: (284807, 30)\n",
      "targets.shape: (284807, 1)\n",
      "Number of training samples: 227846\n",
      "Number of validation samples: 56961\n",
      "Number of positive samples in training data: 417 (0.18% of total)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/\n",
    "fname = \"/home/macula/SMATousi/VAE_outlier/archive/creditcard.csv\"\n",
    "\n",
    "all_features = []\n",
    "all_targets = []\n",
    "with open(fname) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(\"HEADER:\", line.strip())\n",
    "            continue  # Skip header\n",
    "        fields = line.strip().split(\",\")\n",
    "        all_features.append([float(v.replace('\"', \"\")) for v in fields[:-1]])\n",
    "        all_targets.append([int(fields[-1].replace('\"', \"\"))])\n",
    "        if i == 1:\n",
    "            print(\"EXAMPLE FEATURES:\", all_features[-1])\n",
    "\n",
    "features = np.array(all_features, dtype=\"float32\")\n",
    "targets = np.array(all_targets, dtype=\"uint8\")\n",
    "print(\"features.shape:\", features.shape)\n",
    "print(\"targets.shape:\", targets.shape)\n",
    "\n",
    "\n",
    "num_val_samples = int(len(features) * 0.2)\n",
    "train_features = features[:-num_val_samples]\n",
    "train_targets = targets[:-num_val_samples]\n",
    "val_features = features[-num_val_samples:]\n",
    "val_targets = targets[-num_val_samples:]\n",
    "\n",
    "print(\"Number of training samples:\", len(train_features))\n",
    "print(\"Number of validation samples:\", len(val_features))\n",
    "\n",
    "\n",
    "counts = np.bincount(train_targets[:, 0])\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(train_targets)\n",
    "    )\n",
    ")\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]\n",
    "\n",
    "mean = np.mean(train_features, axis=0)\n",
    "train_features -= mean\n",
    "val_features -= mean\n",
    "std = np.std(train_features, axis=0)\n",
    "train_features /= std\n",
    "val_features /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f667b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e52cb",
   "metadata": {},
   "source": [
    "# Training the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b856f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mstmmc\u001b[0m (\u001b[33mtousi-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/macula/SMATousi/VAE_outlier/outlier_VAE/wandb/run-20230717_133512-jawxuahq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tousi-team/VAE_Outlier_Creditcard_Training_2/runs/jawxuahq' target=\"_blank\">tough-gorge-1</a></strong> to <a href='https://wandb.ai/tousi-team/VAE_Outlier_Creditcard_Training_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tousi-team/VAE_Outlier_Creditcard_Training_2' target=\"_blank\">https://wandb.ai/tousi-team/VAE_Outlier_Creditcard_Training_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tousi-team/VAE_Outlier_Creditcard_Training_2/runs/jawxuahq' target=\"_blank\">https://wandb.ai/tousi-team/VAE_Outlier_Creditcard_Training_2/runs/jawxuahq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "223/223 [==============================] - 3s 7ms/step - loss: 1.0472 - reconstruction_loss: 1.0101 - kl_loss: 0.0023\n",
      "Epoch 2/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 1.0067 - reconstruction_loss: 1.0036 - kl_loss: 3.1679e-06\n",
      "Epoch 3/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.9985 - reconstruction_loss: 0.9749 - kl_loss: 0.0054\n",
      "Epoch 4/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.9271 - reconstruction_loss: 0.9055 - kl_loss: 0.0166\n",
      "Epoch 5/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.8905 - reconstruction_loss: 0.8537 - kl_loss: 0.0212\n",
      "Epoch 6/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.8438 - reconstruction_loss: 0.7971 - kl_loss: 0.0261\n",
      "Epoch 7/100\n",
      "223/223 [==============================] - 1s 6ms/step - loss: 0.7941 - reconstruction_loss: 0.7380 - kl_loss: 0.0321\n",
      "Epoch 8/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.7328 - reconstruction_loss: 0.6943 - kl_loss: 0.0342\n",
      "Epoch 9/100\n",
      "223/223 [==============================] - 1s 6ms/step - loss: 0.7030 - reconstruction_loss: 0.6555 - kl_loss: 0.0384\n",
      "Epoch 10/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.6628 - reconstruction_loss: 0.6091 - kl_loss: 0.0420\n",
      "Epoch 11/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.6216 - reconstruction_loss: 0.5743 - kl_loss: 0.0454\n",
      "Epoch 12/100\n",
      "223/223 [==============================] - 2s 8ms/step - loss: 0.6010 - reconstruction_loss: 0.5518 - kl_loss: 0.0479\n",
      "Epoch 13/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.5825 - reconstruction_loss: 0.5395 - kl_loss: 0.0490\n",
      "Epoch 14/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.5810 - reconstruction_loss: 0.5272 - kl_loss: 0.0500\n",
      "Epoch 15/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.5713 - reconstruction_loss: 0.5154 - kl_loss: 0.0514\n",
      "Epoch 16/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.5627 - reconstruction_loss: 0.5014 - kl_loss: 0.0533\n",
      "Epoch 17/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.5326 - reconstruction_loss: 0.4897 - kl_loss: 0.0538\n",
      "Epoch 18/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.5351 - reconstruction_loss: 0.4784 - kl_loss: 0.0538\n",
      "Epoch 19/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.5255 - reconstruction_loss: 0.4686 - kl_loss: 0.0545\n",
      "Epoch 20/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.5190 - reconstruction_loss: 0.4619 - kl_loss: 0.0551\n",
      "Epoch 21/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.5089 - reconstruction_loss: 0.4560 - kl_loss: 0.0556\n",
      "Epoch 22/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.5123 - reconstruction_loss: 0.4523 - kl_loss: 0.0560\n",
      "Epoch 23/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4964 - reconstruction_loss: 0.4485 - kl_loss: 0.0562\n",
      "Epoch 24/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.5149 - reconstruction_loss: 0.4454 - kl_loss: 0.0565\n",
      "Epoch 25/100\n",
      "223/223 [==============================] - 2s 8ms/step - loss: 0.4951 - reconstruction_loss: 0.4421 - kl_loss: 0.0567\n",
      "Epoch 26/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4963 - reconstruction_loss: 0.4391 - kl_loss: 0.0570\n",
      "Epoch 27/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4944 - reconstruction_loss: 0.4368 - kl_loss: 0.0572\n",
      "Epoch 28/100\n",
      "223/223 [==============================] - 1s 7ms/step - loss: 0.4911 - reconstruction_loss: 0.4320 - kl_loss: 0.0574\n",
      "Epoch 29/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4768 - reconstruction_loss: 0.4294 - kl_loss: 0.0575\n",
      "Epoch 30/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4819 - reconstruction_loss: 0.4270 - kl_loss: 0.0575\n",
      "Epoch 31/100\n",
      "223/223 [==============================] - 2s 8ms/step - loss: 0.4738 - reconstruction_loss: 0.4247 - kl_loss: 0.0577\n",
      "Epoch 32/100\n",
      "223/223 [==============================] - 2s 8ms/step - loss: 0.4834 - reconstruction_loss: 0.4232 - kl_loss: 0.0577\n",
      "Epoch 33/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4787 - reconstruction_loss: 0.4221 - kl_loss: 0.0578\n",
      "Epoch 34/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4728 - reconstruction_loss: 0.4207 - kl_loss: 0.0580\n",
      "Epoch 35/100\n",
      "223/223 [==============================] - 2s 8ms/step - loss: 0.4732 - reconstruction_loss: 0.4185 - kl_loss: 0.0582\n",
      "Epoch 36/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4721 - reconstruction_loss: 0.4166 - kl_loss: 0.0584\n",
      "Epoch 37/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4628 - reconstruction_loss: 0.4143 - kl_loss: 0.0588\n",
      "Epoch 38/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4731 - reconstruction_loss: 0.4124 - kl_loss: 0.0589\n",
      "Epoch 39/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4637 - reconstruction_loss: 0.4114 - kl_loss: 0.0590\n",
      "Epoch 40/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4629 - reconstruction_loss: 0.4097 - kl_loss: 0.0591\n",
      "Epoch 41/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4728 - reconstruction_loss: 0.4076 - kl_loss: 0.0592\n",
      "Epoch 42/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4536 - reconstruction_loss: 0.4079 - kl_loss: 0.0593\n",
      "Epoch 43/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4640 - reconstruction_loss: 0.4049 - kl_loss: 0.0596\n",
      "Epoch 44/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4559 - reconstruction_loss: 0.4032 - kl_loss: 0.0598\n",
      "Epoch 45/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4654 - reconstruction_loss: 0.4020 - kl_loss: 0.0599\n",
      "Epoch 46/100\n",
      "223/223 [==============================] - 1s 6ms/step - loss: 0.4670 - reconstruction_loss: 0.4010 - kl_loss: 0.0600\n",
      "Epoch 47/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4618 - reconstruction_loss: 0.3998 - kl_loss: 0.0601\n",
      "Epoch 48/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4766 - reconstruction_loss: 0.3994 - kl_loss: 0.0601\n",
      "Epoch 49/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4554 - reconstruction_loss: 0.3993 - kl_loss: 0.0602\n",
      "Epoch 50/100\n",
      "223/223 [==============================] - 1s 7ms/step - loss: 0.4581 - reconstruction_loss: 0.3976 - kl_loss: 0.0603\n",
      "Epoch 51/100\n",
      "223/223 [==============================] - 1s 6ms/step - loss: 0.4493 - reconstruction_loss: 0.3969 - kl_loss: 0.0603\n",
      "Epoch 52/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4529 - reconstruction_loss: 0.3964 - kl_loss: 0.0604\n",
      "Epoch 53/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4525 - reconstruction_loss: 0.3958 - kl_loss: 0.0603\n",
      "Epoch 54/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4522 - reconstruction_loss: 0.3956 - kl_loss: 0.0603\n",
      "Epoch 55/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4547 - reconstruction_loss: 0.3950 - kl_loss: 0.0604\n",
      "Epoch 56/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4590 - reconstruction_loss: 0.3946 - kl_loss: 0.0604\n",
      "Epoch 57/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4444 - reconstruction_loss: 0.3940 - kl_loss: 0.0603\n",
      "Epoch 58/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4571 - reconstruction_loss: 0.3934 - kl_loss: 0.0604\n",
      "Epoch 59/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4475 - reconstruction_loss: 0.3930 - kl_loss: 0.0604\n",
      "Epoch 60/100\n",
      "223/223 [==============================] - 2s 8ms/step - loss: 0.4475 - reconstruction_loss: 0.3925 - kl_loss: 0.0604\n",
      "Epoch 61/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4370 - reconstruction_loss: 0.3919 - kl_loss: 0.0604\n",
      "Epoch 62/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4549 - reconstruction_loss: 0.3916 - kl_loss: 0.0604\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4606 - reconstruction_loss: 0.3912 - kl_loss: 0.0604\n",
      "Epoch 64/100\n",
      "223/223 [==============================] - 1s 7ms/step - loss: 0.4479 - reconstruction_loss: 0.3913 - kl_loss: 0.0604\n",
      "Epoch 65/100\n",
      "223/223 [==============================] - 1s 6ms/step - loss: 0.4543 - reconstruction_loss: 0.3904 - kl_loss: 0.0605\n",
      "Epoch 66/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4485 - reconstruction_loss: 0.3902 - kl_loss: 0.0604\n",
      "Epoch 67/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4502 - reconstruction_loss: 0.3895 - kl_loss: 0.0604\n",
      "Epoch 68/100\n",
      "223/223 [==============================] - 2s 8ms/step - loss: 0.4391 - reconstruction_loss: 0.3894 - kl_loss: 0.0605\n",
      "Epoch 69/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4440 - reconstruction_loss: 0.3889 - kl_loss: 0.0604\n",
      "Epoch 70/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4457 - reconstruction_loss: 0.3890 - kl_loss: 0.0604\n",
      "Epoch 71/100\n",
      "223/223 [==============================] - 2s 8ms/step - loss: 0.4597 - reconstruction_loss: 0.3883 - kl_loss: 0.0604\n",
      "Epoch 72/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4336 - reconstruction_loss: 0.3882 - kl_loss: 0.0604\n",
      "Epoch 73/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4474 - reconstruction_loss: 0.3877 - kl_loss: 0.0604\n",
      "Epoch 74/100\n",
      "223/223 [==============================] - 2s 8ms/step - loss: 0.4484 - reconstruction_loss: 0.3873 - kl_loss: 0.0604\n",
      "Epoch 75/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4405 - reconstruction_loss: 0.3870 - kl_loss: 0.0604\n",
      "Epoch 76/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4490 - reconstruction_loss: 0.3867 - kl_loss: 0.0604\n",
      "Epoch 77/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4441 - reconstruction_loss: 0.3865 - kl_loss: 0.0604\n",
      "Epoch 78/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4580 - reconstruction_loss: 0.3864 - kl_loss: 0.0604\n",
      "Epoch 79/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4572 - reconstruction_loss: 0.3859 - kl_loss: 0.0604\n",
      "Epoch 80/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4475 - reconstruction_loss: 0.3859 - kl_loss: 0.0605\n",
      "Epoch 81/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4431 - reconstruction_loss: 0.3853 - kl_loss: 0.0604\n",
      "Epoch 82/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4447 - reconstruction_loss: 0.3851 - kl_loss: 0.0604\n",
      "Epoch 83/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4423 - reconstruction_loss: 0.3851 - kl_loss: 0.0603\n",
      "Epoch 84/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4570 - reconstruction_loss: 0.3848 - kl_loss: 0.0604\n",
      "Epoch 85/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4438 - reconstruction_loss: 0.3846 - kl_loss: 0.0603\n",
      "Epoch 86/100\n",
      "223/223 [==============================] - 1s 7ms/step - loss: 0.4372 - reconstruction_loss: 0.3847 - kl_loss: 0.0604\n",
      "Epoch 87/100\n",
      "223/223 [==============================] - 1s 7ms/step - loss: 0.4424 - reconstruction_loss: 0.3849 - kl_loss: 0.0604\n",
      "Epoch 88/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4439 - reconstruction_loss: 0.3839 - kl_loss: 0.0604\n",
      "Epoch 89/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4444 - reconstruction_loss: 0.3838 - kl_loss: 0.0604\n",
      "Epoch 90/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4406 - reconstruction_loss: 0.3834 - kl_loss: 0.0603\n",
      "Epoch 91/100\n",
      "223/223 [==============================] - 2s 8ms/step - loss: 0.4413 - reconstruction_loss: 0.3831 - kl_loss: 0.0603\n",
      "Epoch 92/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4524 - reconstruction_loss: 0.3831 - kl_loss: 0.0604\n",
      "Epoch 93/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4389 - reconstruction_loss: 0.3828 - kl_loss: 0.0603\n",
      "Epoch 94/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4396 - reconstruction_loss: 0.3826 - kl_loss: 0.0603\n",
      "Epoch 95/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4393 - reconstruction_loss: 0.3827 - kl_loss: 0.0603\n",
      "Epoch 96/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4277 - reconstruction_loss: 0.3822 - kl_loss: 0.0603\n",
      "Epoch 97/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4522 - reconstruction_loss: 0.3819 - kl_loss: 0.0604\n",
      "Epoch 98/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4469 - reconstruction_loss: 0.3821 - kl_loss: 0.0604\n",
      "Epoch 99/100\n",
      "223/223 [==============================] - 2s 8ms/step - loss: 0.4409 - reconstruction_loss: 0.3817 - kl_loss: 0.0603\n",
      "Epoch 100/100\n",
      "223/223 [==============================] - 2s 7ms/step - loss: 0.4512 - reconstruction_loss: 0.3813 - kl_loss: 0.0604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5b04674750>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"VAE_Outlier_Creditcard_Training_2\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"CIFAR-100\",\n",
    "#     \"epochs\": 20,\n",
    "#     }\n",
    ")\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(30,))\n",
    "# x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "# x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(128, activation=\"tanh\")(encoder_inputs)\n",
    "x = layers.Dense(1024, activation=\"sigmoid\")(encoder_inputs)\n",
    "x = layers.Dense(512, activation=\"sigmoid\")(x)\n",
    "x = layers.Dense(256, activation=\"sigmoid\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "# x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "# x = layers.Reshape((7, 7, 64))(x)\n",
    "# x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "x = layers.Dense(256, activation=\"sigmoid\")(latent_inputs)\n",
    "x = layers.Dense(512, activation=\"sigmoid\")(x)\n",
    "x = layers.Dense(1024, activation=\"sigmoid\")(x)\n",
    "# x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "decoder_outputs = layers.Dense(30, activation=\"tanh\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "# class CustomCallback(keras.callbacks.Callback):\n",
    "#     def on_epoch_end(self, batch, logs=None):\n",
    "#         # Custom logic to execute at the end of each training batch\n",
    "#         # You can access the training metrics and perform any desired actions\n",
    "#         # Here is an example of printing the loss at the end of each batch\n",
    "# #         print(f\"Batch {batch}: loss = {logs['loss']}\")\n",
    "# #             wandb.log({\"Training/loss\": logs[\"loss\"], \n",
    "# #            \"Training/reconstruction_loss\": logs[\"reconstruction_loss\"],\n",
    "# #            \"Training/kl_loss\": logs[\"kl_loss\"]})\n",
    "#         print(\"#####\")\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, epochs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "        self.epochs = epochs\n",
    "        self.current_epoch = tf.Variable(0, trainable=False, dtype=tf.float32)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            reconstruction_loss = tf.keras.losses.MeanSquaredError()(data, reconstruction)\n",
    "            \n",
    "            # Calculate weight using linear decay\n",
    "            kl_weight = 1.0 - (self.current_epoch / self.epochs)\n",
    "            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "#             kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            kl_loss_weighted = kl_weight * kl_loss\n",
    "            \n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "# Define custom callback to track the current epoch\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.model.current_epoch.assign(epoch)\n",
    "\n",
    "    def on_epoch_end(self, batch, logs=None):\n",
    "        # Custom logic to execute at the end of each training batch\n",
    "        # You can access the training metrics and perform any desired actions\n",
    "        # Here is an example of printing the loss at the end of each batch\n",
    "#         print(f\"Batch {batch}: loss = {logs['loss']}\")\n",
    "        wandb.log({\"Training/loss\": logs[\"loss\"], \n",
    "       \"Training/reconstruction_loss\": logs[\"reconstruction_loss\"],\n",
    "       \"Training/kl_loss\": logs[\"kl_loss\"]})\n",
    "#         print(\"#####\")\n",
    "\n",
    "# Create an instance of VAE model\n",
    "vae = VAE(encoder, decoder, epochs=100)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Define the creditdata and custom callback\n",
    "creditdata = np.concatenate([train_features], axis=0)\n",
    "creditdata = np.expand_dims(creditdata, -1).astype(\"float32\")\n",
    "custom_callback = CustomCallback()\n",
    "\n",
    "# Train the VAE model\n",
    "vae.fit(creditdata, epochs=100, batch_size=1024, callbacks=[custom_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b57d0",
   "metadata": {},
   "source": [
    "# Finding the subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_loss = []\n",
    "detected_outliers = []\n",
    "inlier_features = []\n",
    "best_solutions = []\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"VAE_Outlier_GA_Creditcard_subspace_detection_1\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"CIFAR-100\",\n",
    "#     \"epochs\": 20,\n",
    "#     }\n",
    ")\n",
    "\n",
    "print(\" ********** Finding Inliers ************\")\n",
    "\n",
    "inlier_count = 3\n",
    "c = 0\n",
    "\n",
    "for j in range(val_features.shape[0]):\n",
    "    \n",
    "    test = val_features[j,:]\n",
    "\n",
    "    test = test.reshape([1,30])\n",
    "\n",
    "    z_mean, z_log_var, z = vae.encoder(test)\n",
    "    reconstruction = vae.decoder(z)\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)\n",
    "    reconstruction_loss = reconstruction_loss.numpy()\n",
    "    \n",
    "    if reconstruction_loss < 0.4:\n",
    "        c = c + 1\n",
    "        inlier_features.append(val_features[j,:])\n",
    "    \n",
    "    if c == inlier_count:\n",
    "        \n",
    "        break\n",
    "    \n",
    "inlier_features = np.array(inlier_features)\n",
    "\n",
    "print(\" ********* Inliers found: The shape: \", inlier_features.shape[0], \" **************\")\n",
    "        \n",
    "\n",
    "\n",
    "for i in range(val_features.shape[0]):\n",
    "    \n",
    "    test = val_features[i,:]\n",
    "\n",
    "    test = test.reshape([1,30])\n",
    "\n",
    "    z_mean, z_log_var, z = vae.encoder(test)\n",
    "    reconstruction = vae.decoder(z)\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)\n",
    "    reconstruction_loss = reconstruction_loss.numpy()\n",
    "\n",
    "    rec_loss.append(reconstruction_loss)\n",
    "    \n",
    "    if reconstruction_loss > 3:\n",
    "        \n",
    "        print(\"Rec Loss = \", reconstruction_loss, \" -------- Label = \", val_targets[i], \" Case No. \", i)\n",
    "        \n",
    "        detected_outliers.append([i,val_targets[i]])\n",
    "        \n",
    "        def fitness_func_avg_with_penalty(ga_instance, solution, solution_idx):\n",
    "    \n",
    "            inliers = inlier_features\n",
    "\n",
    "            avg_ins = np.mean(inliers, axis=0)\n",
    "            avg_ins = avg_ins.reshape([1,30])\n",
    "\n",
    "            particle = val_features[i,:]\n",
    "            particle = particle.reshape([1,30])\n",
    "\n",
    "        #     abn_subspace = solution * val_features[6728,:]\n",
    "\n",
    "        #     abn_subspace = abn_subspace.reshape([1,30])\n",
    "\n",
    "            avg_in_rec = []\n",
    "\n",
    "            for index in range(inliers.shape[0]):\n",
    "\n",
    "                candidate_inlier = inliers[index,:]\n",
    "                candidate_inlier = candidate_inlier.reshape([1,30])\n",
    "\n",
    "                in_remain = candidate_inlier * solution\n",
    "\n",
    "                in_normal_subspace = 1 - solution\n",
    "\n",
    "                in_replace = in_normal_subspace * avg_ins\n",
    "\n",
    "                in_candidate = in_remain + in_replace\n",
    "\n",
    "                z_mean, z_log_var, z = vae.encoder(in_candidate)\n",
    "                in_candidate_rec = vae.decoder(z)\n",
    "\n",
    "\n",
    "                rec_loss = tf.keras.losses.MeanSquaredError()(in_candidate,in_candidate_rec)\n",
    "\n",
    "                avg_in_rec.append(rec_loss.numpy())\n",
    "\n",
    "            avg_in_rec = np.array(avg_in_rec)\n",
    "            avg_in_rec = np.mean(avg_in_rec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #     z_mean, z_log_var, z = vae.encoder(particle)\n",
    "        #     reconstruction_1 = vae.decoder(z)\n",
    "\n",
    "            out_remain = particle * solution\n",
    "\n",
    "            out_normal_subspace = 1 - solution\n",
    "\n",
    "            out_replace = avg_ins * out_normal_subspace\n",
    "\n",
    "            out_candidate = out_remain + out_replace\n",
    "\n",
    "\n",
    "            z_mean, z_log_var, z = vae.encoder(out_candidate)\n",
    "            out_candidate_rec = vae.decoder(z)\n",
    "\n",
    "            rec_loss = tf.keras.losses.MeanSquaredError()(out_candidate,out_candidate_rec)\n",
    "            rec_loss = rec_loss.numpy()\n",
    "\n",
    "            # adding the penalty here!\n",
    "\n",
    "            num_ones = np.count_nonzero(solution == 1)\n",
    "\n",
    "            penalty_rate = 1.0 / val_features.shape[1]\n",
    "\n",
    "            regul_param = 1\n",
    "\n",
    "            penalty_rate = penalty_rate / regul_param\n",
    "\n",
    "            fitness = rec_loss / (avg_in_rec + num_ones * penalty_rate * avg_in_rec)\n",
    "\n",
    "    #         fitness = fitness - num_ones * penalty_rate * fitness\n",
    "\n",
    "            return fitness\n",
    "        \n",
    "        \n",
    "        def on_generation(ga):\n",
    "            print(\"Generation\", ga.generations_completed)\n",
    "\n",
    "            solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "\n",
    "            wandb.log({\"GA - \" + str(i) + \"/solution_fitness\": solution_fitness})\n",
    "\n",
    "            print(solution_fitness)\n",
    "            \n",
    "        \n",
    "        fitness_function = fitness_func_avg_with_penalty\n",
    "\n",
    "        num_generations = 40\n",
    "        num_parents_mating = 4\n",
    "\n",
    "        sol_per_pop = 100\n",
    "        num_genes = val_features.shape[1]\n",
    "\n",
    "        init_range_low = -2\n",
    "        init_range_high = 5\n",
    "\n",
    "        parent_selection_type = \"sss\"\n",
    "        keep_parents = 1\n",
    "\n",
    "        space = [[0,1] for i in range(num_genes)]\n",
    "\n",
    "        crossover_type = \"single_point\"\n",
    "\n",
    "        mutation_type = \"random\"\n",
    "        mutation_percent_genes = 15\n",
    "\n",
    "        ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                               num_parents_mating=num_parents_mating,\n",
    "                               fitness_func=fitness_function,\n",
    "                               sol_per_pop=sol_per_pop,\n",
    "                               num_genes=num_genes,\n",
    "                               init_range_low=init_range_low,\n",
    "                               init_range_high=init_range_high,\n",
    "                               parent_selection_type=parent_selection_type,\n",
    "    #                            keep_parents=keep_parents,\n",
    "                               keep_elitism=2,\n",
    "                               crossover_type=crossover_type,\n",
    "                               mutation_type=mutation_type,\n",
    "                               mutation_percent_genes=mutation_percent_genes,\n",
    "                               on_generation=on_generation,\n",
    "                               gene_space = space)\n",
    "\n",
    "        ga_instance.run()\n",
    "\n",
    "        solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "        print(\"##########  End of the outlier numver \", i, \" epoch ##########\")\n",
    "        print(\"Parameters of the best solution : {solution}\".format(solution=solution))\n",
    "\n",
    "        best_solutions.append(solution)\n",
    "\n",
    "        wandb.log({\"Solutions/solution\": solution})\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "detected_outliers = np.array(detected_outliers)\n",
    "np.save(\"creditcard_detected_outliers_1.npy\", detected_outliers)\n",
    "\n",
    "best_solutions = np.array(best_solutions)\n",
    "np.save(\"creditcard_outlier_subspaces_1.npy\", best_solutions)\n",
    "\n",
    "\n",
    "rec_loss = np.array(rec_loss)\n",
    "print(\"mean = \", np.mean(rec_loss))\n",
    "print(\"std = \", np.std(rec_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49140c00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rec_loss = []\n",
    "\n",
    "for i in range(train_features.shape[0]):\n",
    "        \n",
    "    if train_targets[i] == 1:\n",
    "        test = train_features[i,:]\n",
    "\n",
    "        test = test.reshape([1,30])\n",
    "\n",
    "        z_mean, z_log_var, z = vae.encoder(test)\n",
    "        reconstruction = vae.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)\n",
    "\n",
    "\n",
    "\n",
    "    #     if reconstruction_loss.numpy() > 40:\n",
    "\n",
    "        rec_loss.append(reconstruction_loss.numpy())\n",
    "        print(\"Rec Loss = \", reconstruction_loss.numpy(), \" -------- Label = \", train_targets[i], \" Case No. \", i)\n",
    "\n",
    "rec_loss = np.array(rec_loss)\n",
    "print(\"mean = \", np.mean(rec_loss))\n",
    "print(\"std = \", np.std(rec_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81a83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03089c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
