{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cba529fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 12:28:12.970489: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-27 12:28:19.853657: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-27 12:28:28.636070: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/macula/SMATousi/.conda/envs/mac-deep/lib/:/home/macula/SMATousi/.conda/envs/mac-deep/lib/python3.7/site-packages/nvidia/cudnn/lib\n",
      "2023-06-27 12:28:28.638119: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/macula/SMATousi/.conda/envs/mac-deep/lib/:/home/macula/SMATousi/.conda/envs/mac-deep/lib/python3.7/site-packages/nvidia/cudnn/lib\n",
      "2023-06-27 12:28:28.638134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 2074667495335683686\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 22542680064\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9253401144424842193\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-27 12:28:53.513418: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-27 12:28:53.596510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-27 12:28:53.706798: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-27 12:28:53.707149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-27 12:28:57.923577: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-27 12:28:57.934471: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-27 12:28:57.934836: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-27 12:28:57.946611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 21498 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pygad\n",
    "import wandb\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "tf.random.set_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2eeb41",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097b80bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEADER: \"Time\",\"V1\",\"V2\",\"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\"V17\",\"V18\",\"V19\",\"V20\",\"V21\",\"V22\",\"V23\",\"V24\",\"V25\",\"V26\",\"V27\",\"V28\",\"Amount\",\"Class\"\n",
      "EXAMPLE FEATURES: [0.0, -1.3598071336738, -0.0727811733098497, 2.53634673796914, 1.37815522427443, -0.338320769942518, 0.462387777762292, 0.239598554061257, 0.0986979012610507, 0.363786969611213, 0.0907941719789316, -0.551599533260813, -0.617800855762348, -0.991389847235408, -0.311169353699879, 1.46817697209427, -0.470400525259478, 0.207971241929242, 0.0257905801985591, 0.403992960255733, 0.251412098239705, -0.018306777944153, 0.277837575558899, -0.110473910188767, 0.0669280749146731, 0.128539358273528, -0.189114843888824, 0.133558376740387, -0.0210530534538215, 149.62]\n",
      "features.shape: (284807, 30)\n",
      "targets.shape: (284807, 1)\n",
      "Number of training samples: 227846\n",
      "Number of validation samples: 56961\n",
      "Number of positive samples in training data: 417 (0.18% of total)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Get the real data from https://www.kaggle.com/mlg-ulb/creditcardfraud/\n",
    "fname = \"/home/macula/SMATousi/VAE_outlier/archive/creditcard.csv\"\n",
    "\n",
    "all_features = []\n",
    "all_targets = []\n",
    "with open(fname) as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(\"HEADER:\", line.strip())\n",
    "            continue  # Skip header\n",
    "        fields = line.strip().split(\",\")\n",
    "        all_features.append([float(v.replace('\"', \"\")) for v in fields[:-1]])\n",
    "        all_targets.append([int(fields[-1].replace('\"', \"\"))])\n",
    "        if i == 1:\n",
    "            print(\"EXAMPLE FEATURES:\", all_features[-1])\n",
    "\n",
    "features = np.array(all_features, dtype=\"float32\")\n",
    "targets = np.array(all_targets, dtype=\"uint8\")\n",
    "print(\"features.shape:\", features.shape)\n",
    "print(\"targets.shape:\", targets.shape)\n",
    "\n",
    "\n",
    "num_val_samples = int(len(features) * 0.2)\n",
    "train_features = features[:-num_val_samples]\n",
    "train_targets = targets[:-num_val_samples]\n",
    "val_features = features[-num_val_samples:]\n",
    "val_targets = targets[-num_val_samples:]\n",
    "\n",
    "print(\"Number of training samples:\", len(train_features))\n",
    "print(\"Number of validation samples:\", len(val_features))\n",
    "\n",
    "\n",
    "counts = np.bincount(train_targets[:, 0])\n",
    "print(\n",
    "    \"Number of positive samples in training data: {} ({:.2f}% of total)\".format(\n",
    "        counts[1], 100 * float(counts[1]) / len(train_targets)\n",
    "    )\n",
    ")\n",
    "\n",
    "weight_for_0 = 1.0 / counts[0]\n",
    "weight_for_1 = 1.0 / counts[1]\n",
    "\n",
    "mean = np.mean(train_features, axis=0)\n",
    "train_features -= mean\n",
    "val_features -= mean\n",
    "std = np.std(train_features, axis=0)\n",
    "train_features /= std\n",
    "val_features /= std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e52cb",
   "metadata": {},
   "source": [
    "# Training the VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b856f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ncyynq9i) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"VAE_Outlier_Creditcard_Training_1\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"CIFAR-100\",\n",
    "#     \"epochs\": 20,\n",
    "#     }\n",
    ")\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "latent_dim = 2\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(30,))\n",
    "# x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "# x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Flatten()(x)\n",
    "# x = layers.Dense(128, activation=\"tanh\")(encoder_inputs)\n",
    "x = layers.Dense(1024, activation=\"tanh\")(encoder_inputs)\n",
    "x = layers.Dense(512, activation=\"tanh\")(x)\n",
    "x = layers.Dense(256, activation=\"tanh\")(x)\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "# x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "# x = layers.Reshape((7, 7, 64))(x)\n",
    "# x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "# decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "x = layers.Dense(256, activation=\"tanh\")(latent_inputs)\n",
    "x = layers.Dense(512, activation=\"tanh\")(x)\n",
    "x = layers.Dense(1024, activation=\"tanh\")(x)\n",
    "# x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "decoder_outputs = layers.Dense(30, activation=\"tanh\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        # Custom logic to execute at the end of each training batch\n",
    "        # You can access the training metrics and perform any desired actions\n",
    "        # Here is an example of printing the loss at the end of each batch\n",
    "#         print(f\"Batch {batch}: loss = {logs['loss']}\")\n",
    "            wandb.log({\"Training/loss\": logs[\"loss\"], \n",
    "           \"Training/reconstruction_loss\": logs[\"reconstruction_loss\"],\n",
    "           \"Training/kl_loss\": logs[\"kl_loss\"]})\n",
    "        \n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "            name=\"reconstruction_loss\"\n",
    "        )\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "#             reconstruction_loss = tf.reduce_mean(\n",
    "#                 tf.reduce_sum(\n",
    "#                     keras.losses.binary_crossentropy(data, reconstruction), axis=(1, 2)\n",
    "#                 )\n",
    "#             )\n",
    "            reconstruction_loss = tf.keras.losses.MeanSquaredError()(data,reconstruction)\n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = 0.8 * reconstruction_loss + 0.2 * kl_loss\n",
    "\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\n",
    "            \"loss\": self.total_loss_tracker.result(),\n",
    "            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "            \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "creditdata = np.concatenate([train_features], axis=0)\n",
    "creditdata = np.expand_dims(creditdata, -1).astype(\"float32\")\n",
    "\n",
    "custom_callback = CustomCallback()\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=tf.keras.optimizers.legacy.SGD())\n",
    "vae.fit(creditdata, epochs=200, batch_size=1024, callbacks=[custom_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b57d0",
   "metadata": {},
   "source": [
    "# Finding the subspaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_loss = []\n",
    "detected_outliers = []\n",
    "inlier_features = []\n",
    "best_solutions = []\n",
    "\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"VAE_Outlier_GA_Creditcard_subspace_detection_1\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"CIFAR-100\",\n",
    "#     \"epochs\": 20,\n",
    "#     }\n",
    ")\n",
    "\n",
    "print(\" ********** Finding Inliers ************\")\n",
    "\n",
    "inlier_count = 3\n",
    "c = 0\n",
    "\n",
    "for j in range(val_features.shape[0]):\n",
    "    \n",
    "    test = val_features[j,:]\n",
    "\n",
    "    test = test.reshape([1,30])\n",
    "\n",
    "    z_mean, z_log_var, z = vae.encoder(test)\n",
    "    reconstruction = vae.decoder(z)\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)\n",
    "    reconstruction_loss = reconstruction_loss.numpy()\n",
    "    \n",
    "    if reconstruction_loss < 0.4:\n",
    "        c = c + 1\n",
    "        inlier_features.append(val_features[j,:])\n",
    "    \n",
    "    if c == inlier_count:\n",
    "        \n",
    "        break\n",
    "    \n",
    "inlier_features = np.array(inlier_features)\n",
    "\n",
    "print(\" ********* Inliers found: The shape: \", inlier_features.shape[0], \" **************\")\n",
    "        \n",
    "\n",
    "\n",
    "for i in range(val_features.shape[0]):\n",
    "    \n",
    "    test = val_features[i,:]\n",
    "\n",
    "    test = test.reshape([1,30])\n",
    "\n",
    "    z_mean, z_log_var, z = vae.encoder(test)\n",
    "    reconstruction = vae.decoder(z)\n",
    "\n",
    "    reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)\n",
    "    reconstruction_loss = reconstruction_loss.numpy()\n",
    "\n",
    "    rec_loss.append(reconstruction_loss)\n",
    "    \n",
    "    if reconstruction_loss > 3:\n",
    "        \n",
    "        print(\"Rec Loss = \", reconstruction_loss, \" -------- Label = \", val_targets[i], \" Case No. \", i)\n",
    "        \n",
    "        detected_outliers.append([i,val_targets[i]])\n",
    "        \n",
    "        def fitness_func_avg_with_penalty(ga_instance, solution, solution_idx):\n",
    "    \n",
    "            inliers = inlier_features\n",
    "\n",
    "            avg_ins = np.mean(inliers, axis=0)\n",
    "            avg_ins = avg_ins.reshape([1,30])\n",
    "\n",
    "            particle = val_features[i,:]\n",
    "            particle = particle.reshape([1,30])\n",
    "\n",
    "        #     abn_subspace = solution * val_features[6728,:]\n",
    "\n",
    "        #     abn_subspace = abn_subspace.reshape([1,30])\n",
    "\n",
    "            avg_in_rec = []\n",
    "\n",
    "            for index in range(inliers.shape[0]):\n",
    "\n",
    "                candidate_inlier = inliers[index,:]\n",
    "                candidate_inlier = candidate_inlier.reshape([1,30])\n",
    "\n",
    "                in_remain = candidate_inlier * solution\n",
    "\n",
    "                in_normal_subspace = 1 - solution\n",
    "\n",
    "                in_replace = in_normal_subspace * avg_ins\n",
    "\n",
    "                in_candidate = in_remain + in_replace\n",
    "\n",
    "                z_mean, z_log_var, z = vae.encoder(in_candidate)\n",
    "                in_candidate_rec = vae.decoder(z)\n",
    "\n",
    "\n",
    "                rec_loss = tf.keras.losses.MeanSquaredError()(in_candidate,in_candidate_rec)\n",
    "\n",
    "                avg_in_rec.append(rec_loss.numpy())\n",
    "\n",
    "            avg_in_rec = np.array(avg_in_rec)\n",
    "            avg_in_rec = np.mean(avg_in_rec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #     z_mean, z_log_var, z = vae.encoder(particle)\n",
    "        #     reconstruction_1 = vae.decoder(z)\n",
    "\n",
    "            out_remain = particle * solution\n",
    "\n",
    "            out_normal_subspace = 1 - solution\n",
    "\n",
    "            out_replace = avg_ins * out_normal_subspace\n",
    "\n",
    "            out_candidate = out_remain + out_replace\n",
    "\n",
    "\n",
    "            z_mean, z_log_var, z = vae.encoder(out_candidate)\n",
    "            out_candidate_rec = vae.decoder(z)\n",
    "\n",
    "            rec_loss = tf.keras.losses.MeanSquaredError()(out_candidate,out_candidate_rec)\n",
    "            rec_loss = rec_loss.numpy()\n",
    "\n",
    "            # adding the penalty here!\n",
    "\n",
    "            num_ones = np.count_nonzero(solution == 1)\n",
    "\n",
    "            penalty_rate = 1.0 / val_features.shape[1]\n",
    "\n",
    "            regul_param = 1\n",
    "\n",
    "            penalty_rate = penalty_rate / regul_param\n",
    "\n",
    "            fitness = rec_loss / (avg_in_rec + num_ones * penalty_rate * avg_in_rec)\n",
    "\n",
    "    #         fitness = fitness - num_ones * penalty_rate * fitness\n",
    "\n",
    "            return fitness\n",
    "        \n",
    "        \n",
    "        def on_generation(ga):\n",
    "            print(\"Generation\", ga.generations_completed)\n",
    "\n",
    "            solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "\n",
    "            wandb.log({\"GA - \" + str(i) + \"/solution_fitness\": solution_fitness})\n",
    "\n",
    "            print(solution_fitness)\n",
    "            \n",
    "        \n",
    "        fitness_function = fitness_func_avg_with_penalty\n",
    "\n",
    "        num_generations = 40\n",
    "        num_parents_mating = 4\n",
    "\n",
    "        sol_per_pop = 100\n",
    "        num_genes = val_features.shape[1]\n",
    "\n",
    "        init_range_low = -2\n",
    "        init_range_high = 5\n",
    "\n",
    "        parent_selection_type = \"sss\"\n",
    "        keep_parents = 1\n",
    "\n",
    "        space = [[0,1] for i in range(num_genes)]\n",
    "\n",
    "        crossover_type = \"single_point\"\n",
    "\n",
    "        mutation_type = \"random\"\n",
    "        mutation_percent_genes = 15\n",
    "\n",
    "        ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                               num_parents_mating=num_parents_mating,\n",
    "                               fitness_func=fitness_function,\n",
    "                               sol_per_pop=sol_per_pop,\n",
    "                               num_genes=num_genes,\n",
    "                               init_range_low=init_range_low,\n",
    "                               init_range_high=init_range_high,\n",
    "                               parent_selection_type=parent_selection_type,\n",
    "    #                            keep_parents=keep_parents,\n",
    "                               keep_elitism=2,\n",
    "                               crossover_type=crossover_type,\n",
    "                               mutation_type=mutation_type,\n",
    "                               mutation_percent_genes=mutation_percent_genes,\n",
    "                               on_generation=on_generation,\n",
    "                               gene_space = space)\n",
    "\n",
    "        ga_instance.run()\n",
    "\n",
    "        solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "        print(\"##########  End of the outlier numver \", i, \" epoch ##########\")\n",
    "        print(\"Parameters of the best solution : {solution}\".format(solution=solution))\n",
    "\n",
    "        best_solutions.append(solution)\n",
    "\n",
    "        wandb.log({\"Solutions/solution\": solution})\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "detected_outliers = np.array(detected_outliers)\n",
    "np.save(\"creditcard_detected_outliers_1.npy\", detected_outliers)\n",
    "\n",
    "best_solutions = np.array(best_solutions)\n",
    "np.save(\"creditcard_outlier_subspaces_1.npy\", best_solutions)\n",
    "\n",
    "\n",
    "rec_loss = np.array(rec_loss)\n",
    "print(\"mean = \", np.mean(rec_loss))\n",
    "print(\"std = \", np.std(rec_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49140c00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rec_loss = []\n",
    "\n",
    "for i in range(train_features.shape[0]):\n",
    "        \n",
    "    if train_targets[i] == 1:\n",
    "        test = train_features[i,:]\n",
    "\n",
    "        test = test.reshape([1,30])\n",
    "\n",
    "        z_mean, z_log_var, z = vae.encoder(test)\n",
    "        reconstruction = vae.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(test,reconstruction)\n",
    "\n",
    "\n",
    "\n",
    "    #     if reconstruction_loss.numpy() > 40:\n",
    "\n",
    "        rec_loss.append(reconstruction_loss.numpy())\n",
    "        print(\"Rec Loss = \", reconstruction_loss.numpy(), \" -------- Label = \", train_targets[i], \" Case No. \", i)\n",
    "\n",
    "rec_loss = np.array(rec_loss)\n",
    "print(\"mean = \", np.mean(rec_loss))\n",
    "print(\"std = \", np.std(rec_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c81a83c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03089c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
