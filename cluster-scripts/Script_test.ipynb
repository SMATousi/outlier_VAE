{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genetic_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lables = csv_data_loader(\"mnist-06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_VAE(data,\n",
    "              latent_dim = 2,\n",
    "              hidden_layer_n = [20,18,16],\n",
    "              num_dims = 10,\n",
    "              kl_loss_factor = 0.01,\n",
    "              epochs = 100,\n",
    "              batch_size = 128\n",
    "              ):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Training the VAE on the data\n",
    "    \"\"\"\n",
    "\n",
    "    class Sampling(layers.Layer):\n",
    "        \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    latent_dim = latent_dim\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(num_dims,))\n",
    "    x = layers.Dense(num_dims, activation=\"tanh\")(encoder_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"tanh\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"tanh\")(latent_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"tanh\")(x)\n",
    "    decoder_outputs = layers.Dense(num_dims, activation=\"linear\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    class VAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "                name=\"reconstruction_loss\"\n",
    "            )\n",
    "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                reconstruction_loss = tf.keras.losses.MeanSquaredError()(data,reconstruction)\n",
    "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                total_loss = reconstruction_loss + kl_loss_factor * kl_loss\n",
    "        \n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            }\n",
    "    \n",
    "    creditdata = np.concatenate([data], axis=0)\n",
    "    creditdata = np.expand_dims(creditdata, -1).astype(\"float32\")\n",
    "\n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    history = vae.fit(creditdata,epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "\n",
    "    return vae, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, history = train_VAE(data, num_dims=784, hidden_layer_n=[512, 256, 128] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[3, :].reshape([1, 784])\n",
    "\n",
    "z_mean, z_log_var, z = vae.encoder(sample.astype('float32'))\n",
    "reconstruction = vae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample.astype('float32'),reconstruction)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_detect_outliers(data,\n",
    "                        vae_model,\n",
    "                        num_dims\n",
    "                        ):\n",
    "\n",
    "    data_mean = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1, num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z_mean, z_log_var, z = vae_model.encoder(sample)\n",
    "        reconstruction = vae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample,reconstruction)\n",
    "        \n",
    "        data_mean.append(reconstruction_loss)\n",
    "    \n",
    "    data_mean = np.array(data_mean)\n",
    "    i_mean = np.mean(data_mean)\n",
    "    i_std = np.std(data_mean)\n",
    "    \n",
    "\n",
    "    threshold = i_mean + 2*i_std\n",
    "\n",
    "    classes = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z_mean, z_log_var, z = vae_model.encoder(sample)\n",
    "        reconstruction = vae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample,reconstruction)\n",
    "        \n",
    "        if reconstruction_loss > threshold:\n",
    "            \n",
    "            classes.append(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            classes.append(0)\n",
    "\n",
    "    classes = np.array(classes)\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = vae_detect_outliers(data, vae, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# sklearn.metrics.confusion_matrix(lables, 1-classes, labels=None, sample_weight=None, normalize=None)\n",
    "true_labels = 1- lables\n",
    "predicted_labels = classes\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Normalize the confusion matrix to get percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"Confusion Matrix (Percentage):\")\n",
    "print(cm_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "true_labels = 1- lables\n",
    "predicted_labels = classes\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[:, 0] = mat[:, 0]/(mat[0, 0]+ mat[1, 0])\n",
    "mat[:, 1] = mat[:, 1]/(mat[0, 1]+ mat[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RAE(data,\n",
    "              latent_dim = 2,\n",
    "              hidden_layer_n = [20,18,16],\n",
    "              num_dims = 10,\n",
    "              z_loss_w = 0.01,\n",
    "              REG_loss_w = 0.01,\n",
    "              epochs = 100,\n",
    "              batch_size = 128\n",
    "              ):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Training the RAE on the data\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(num_dims,))\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"sigmoid\")(encoder_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"sigmoid\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"sigmoid\")(x)\n",
    "    encoder_output = layers.Dense(latent_dim, activation=\"sigmoid\")(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_output, name=\"encoder\")\n",
    "\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"sigmoid\")(latent_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"sigmoid\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"sigmoid\")(x)\n",
    "    decoder_outputs = layers.Dense(num_dims, activation=\"linear\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "\n",
    "    class RAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "                name=\"reconstruction_loss\"\n",
    "            )\n",
    "            self.z_tracker = keras.metrics.Mean(name=\"z_loss\")\n",
    "            self.REG_tracker = keras.metrics.Mean(name=\"REG_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.z_tracker,\n",
    "                self.REG_tracker,\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "\n",
    "                reconstruction_loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(data,reconstruction)\n",
    "\n",
    "                z_loss = K.mean(K.square(z), axis=[1])\n",
    "        \n",
    "                REG_loss = K.mean(K.square(K.gradients(K.square(reconstruction), z)))\n",
    "\n",
    "#                 z_loss_w = z_loss_w\n",
    "#                 REG_loss_w = REG_loss_w\n",
    "\n",
    "                total_loss = reconstruction_loss +  z_loss_w * z_loss + REG_loss_w * REG_loss\n",
    "            \n",
    "                grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "                self.total_loss_tracker.update_state(total_loss)\n",
    "                self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "                self.z_tracker.update_state(z_loss)\n",
    "                self.REG_tracker.update_state(REG_loss)\n",
    "                del tape\n",
    "                return {\n",
    "                    \"loss\": self.total_loss_tracker.result(),\n",
    "                    \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                    \"z_loss\": self.z_tracker.result(),\n",
    "                    \"REG_loss\": self.REG_tracker.result(),\n",
    "                }\n",
    "\n",
    "    tdata = np.concatenate([data], axis=0)\n",
    "    tdata = np.expand_dims(tdata, -1).astype(\"float32\")\n",
    "\n",
    "    rae = RAE(encoder, decoder)\n",
    "    rae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    history = rae.fit(tdata, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "    return rae, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rae, history = train_RAE(data, num_dims=784, hidden_layer_n=[512, 256, 128], z_loss_w = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[1, :].reshape([1, 784])\n",
    "\n",
    "z = rae.encoder(sample.astype('float32'))\n",
    "reconstruction = rae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample.astype('float32'), reconstruction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rae_detect_outliers(data,\n",
    "                        rae_model,\n",
    "                        num_dims\n",
    "                        ):\n",
    "\n",
    "    data_mean = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z = rae_model.encoder(sample)\n",
    "        reconstruction = rae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample, reconstruction)\n",
    "        \n",
    "        data_mean.append(reconstruction_loss)\n",
    "    \n",
    "    data_mean = np.array(data_mean)\n",
    "    data_std = np.std(data_mean)\n",
    "\n",
    "    threshold = data_mean + 3*data_std\n",
    "\n",
    "    classes = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "        \n",
    "        z = rae_model.encoder(sample)\n",
    "        reconstruction = rae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample, reconstruction)\n",
    "        \n",
    "        if any(reconstruction_loss > threshold):\n",
    "            \n",
    "            classes.append(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            classes.append(0)\n",
    "\n",
    "    classes = np.array(classes)\n",
    "\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = rae_detect_outliers(data, rae, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# sklearn.metrics.confusion_matrix(lables, 1-classes, labels=None, sample_weight=None, normalize=None)\n",
    "true_labels = 1 - lables\n",
    "predicted_labels = classes\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Normalize the confusion matrix to get percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"Confusion Matrix (Percentage):\")\n",
    "print(cm_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "true_labels = 1- lables\n",
    "predicted_labels = classes\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mnist-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = csv_data_loader(\"mnist-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_VAE(data,\n",
    "              latent_dim = 2,\n",
    "              hidden_layer_n = [20,18,16],\n",
    "              num_dims = 10,\n",
    "              kl_loss_factor = 0.1,\n",
    "              epochs = 200,\n",
    "              batch_size = 64\n",
    "              ):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Training the VAE on the data\n",
    "    \"\"\"\n",
    "\n",
    "    class Sampling(layers.Layer):\n",
    "        \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    latent_dim = latent_dim\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(num_dims,))\n",
    "    x = layers.Dense(num_dims, activation=\"tanh\")(encoder_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"tanh\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"tanh\")(latent_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"tanh\")(x)\n",
    "    decoder_outputs = layers.Dense(num_dims, activation=\"linear\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    class VAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "                name=\"reconstruction_loss\"\n",
    "            )\n",
    "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                reconstruction_loss = tf.keras.losses.MeanSquaredError()(data,reconstruction)\n",
    "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                total_loss = reconstruction_loss + kl_loss_factor * kl_loss\n",
    "        \n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            }\n",
    "    \n",
    "    creditdata = np.concatenate([data], axis=0)\n",
    "    creditdata = np.expand_dims(creditdata, -1).astype(\"float32\")\n",
    "\n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    history = vae.fit(creditdata,epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "\n",
    "    return vae, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, history = train_VAE(data, num_dims=784, hidden_layer_n=[512, 256, 128] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[1, :].reshape([1, 784])\n",
    "\n",
    "z_mean, z_log_var, z = vae.encoder(sample.astype('float32'))\n",
    "reconstruction = vae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample.astype('float32'),reconstruction)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_detect_outliers(data,\n",
    "                        vae_model,\n",
    "                        num_dims\n",
    "                        ):\n",
    "\n",
    "    data_mean = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1, num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z_mean, z_log_var, z = vae_model.encoder(sample)\n",
    "        reconstruction = vae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample,reconstruction)\n",
    "        \n",
    "        data_mean.append(reconstruction_loss)\n",
    "    \n",
    "    data_mean = np.array(data_mean)\n",
    "    i_mean = np.mean(data_mean)\n",
    "    i_std = np.std(data_mean)\n",
    "    \n",
    "\n",
    "    threshold = i_mean + 2*i_std\n",
    "\n",
    "    classes = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z_mean, z_log_var, z = vae_model.encoder(sample)\n",
    "        reconstruction = vae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample,reconstruction)\n",
    "        \n",
    "        if reconstruction_loss > threshold:\n",
    "            \n",
    "            classes.append(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            classes.append(0)\n",
    "\n",
    "    classes = np.array(classes)\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = vae_detect_outliers(data, vae, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# sklearn.metrics.confusion_matrix(lables, 1-classes, labels=None, sample_weight=None, normalize=None)\n",
    "true_labels = 1- labels\n",
    "predicted_labels = classes\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Normalize the confusion matrix to get percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"Confusion Matrix (Percentage):\")\n",
    "print(cm_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RAE(data,\n",
    "              latent_dim = 2,\n",
    "              hidden_layer_n = [20,18,16],\n",
    "              num_dims = 10,\n",
    "              z_loss_w = 0.01,\n",
    "              REG_loss_w = 0.01,\n",
    "              epochs = 100,\n",
    "              batch_size = 128\n",
    "              ):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Training the RAE on the data\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(num_dims,))\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"sigmoid\")(encoder_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"sigmoid\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"sigmoid\")(x)\n",
    "    encoder_output = layers.Dense(latent_dim, activation=\"sigmoid\")(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_output, name=\"encoder\")\n",
    "\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"sigmoid\")(latent_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"sigmoid\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"sigmoid\")(x)\n",
    "    decoder_outputs = layers.Dense(num_dims, activation=\"linear\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "\n",
    "    class RAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "                name=\"reconstruction_loss\"\n",
    "            )\n",
    "            self.z_tracker = keras.metrics.Mean(name=\"z_loss\")\n",
    "            self.REG_tracker = keras.metrics.Mean(name=\"REG_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.z_tracker,\n",
    "                self.REG_tracker,\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "\n",
    "                reconstruction_loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(data,reconstruction)\n",
    "\n",
    "                z_loss = K.mean(K.square(z), axis=[1])\n",
    "        \n",
    "                REG_loss = K.mean(K.square(K.gradients(K.square(reconstruction), z)))\n",
    "\n",
    "#                 z_loss_w = z_loss_w\n",
    "#                 REG_loss_w = REG_loss_w\n",
    "\n",
    "                total_loss = reconstruction_loss +  z_loss_w * z_loss + REG_loss_w * REG_loss\n",
    "            \n",
    "                grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "                self.total_loss_tracker.update_state(total_loss)\n",
    "                self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "                self.z_tracker.update_state(z_loss)\n",
    "                self.REG_tracker.update_state(REG_loss)\n",
    "                del tape\n",
    "                return {\n",
    "                    \"loss\": self.total_loss_tracker.result(),\n",
    "                    \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                    \"z_loss\": self.z_tracker.result(),\n",
    "                    \"REG_loss\": self.REG_tracker.result(),\n",
    "                }\n",
    "\n",
    "    tdata = np.concatenate([data], axis=0)\n",
    "    tdata = np.expand_dims(tdata, -1).astype(\"float32\")\n",
    "\n",
    "    rae = RAE(encoder, decoder)\n",
    "    rae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    history = rae.fit(tdata, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "    return rae, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rae, history = train_RAE(data, num_dims=784, hidden_layer_n=[512, 256, 128], z_loss_w = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[-1, :].reshape([1, 784])\n",
    "\n",
    "z = rae.encoder(sample.astype('float32'))\n",
    "reconstruction = rae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample.astype('float32'), reconstruction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rae_detect_outliers(data,\n",
    "                        rae_model,\n",
    "                        num_dims\n",
    "                        ):\n",
    "\n",
    "    data_mean = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z = rae_model.encoder(sample)\n",
    "        reconstruction = rae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample, reconstruction)\n",
    "        \n",
    "        data_mean.append(reconstruction_loss)\n",
    "    \n",
    "    data_mean = np.array(data_mean)\n",
    "    data_std = np.std(data_mean)\n",
    "\n",
    "    threshold = data_mean + 3*data_std\n",
    "\n",
    "    classes = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "        \n",
    "        z = rae_model.encoder(sample)\n",
    "        reconstruction = rae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample, reconstruction)\n",
    "        \n",
    "        if any(reconstruction_loss > threshold):\n",
    "            \n",
    "            classes.append(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            classes.append(0)\n",
    "\n",
    "    classes = np.array(classes)\n",
    "\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = rae_detect_outliers(data, rae, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# sklearn.metrics.confusion_matrix(lables, 1-classes, labels=None, sample_weight=None, normalize=None)\n",
    "true_labels = 1 - labels\n",
    "predicted_labels = classes\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Normalize the confusion matrix to get percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"Confusion Matrix (Percentage):\")\n",
    "print(cm_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fashion TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lables = csv_data_loader(\"fashion-TB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7100, 784)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "56/56 [==============================] - 2s 28ms/step - loss: 13953.7340 - reconstruction_loss: 12852.5107 - kl_loss: 53.8569\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - 1s 26ms/step - loss: 9894.8077 - reconstruction_loss: 9220.1543 - kl_loss: 50.7969\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 7323.2210 - reconstruction_loss: 6882.2422 - kl_loss: 29.7589\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 5652.4044 - reconstruction_loss: 5402.7446 - kl_loss: 15.9262\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 4728.5422 - reconstruction_loss: 4511.9873 - kl_loss: 10.0228\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 4086.6173 - reconstruction_loss: 3995.3069 - kl_loss: 6.8983\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 3760.3821 - reconstruction_loss: 3718.8577 - kl_loss: 5.5764\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3595.5738 - reconstruction_loss: 3568.2703 - kl_loss: 4.6030\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3492.6390 - reconstruction_loss: 3501.0610 - kl_loss: 3.7848\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3472.9141 - reconstruction_loss: 3474.6975 - kl_loss: 3.3975\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 3471.4704 - reconstruction_loss: 3462.9309 - kl_loss: 2.7321\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3450.6988 - reconstruction_loss: 3457.4546 - kl_loss: 15.1971\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3455.7892 - reconstruction_loss: 3457.2888 - kl_loss: 11.1428\n",
      "Epoch 14/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3493.0611 - reconstruction_loss: 3451.4805 - kl_loss: 8.0181\n",
      "Epoch 15/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3433.6665 - reconstruction_loss: 3454.5974 - kl_loss: 6.7856\n",
      "Epoch 16/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3446.5347 - reconstruction_loss: 3455.3164 - kl_loss: 4.7979\n",
      "Epoch 17/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3436.1346 - reconstruction_loss: 3457.2485 - kl_loss: 3.2859\n",
      "Epoch 18/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3437.8249 - reconstruction_loss: 3457.7581 - kl_loss: 2.7878\n",
      "Epoch 19/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3458.6486 - reconstruction_loss: 3459.6907 - kl_loss: 2.1317\n",
      "Epoch 20/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3474.6419 - reconstruction_loss: 3453.2532 - kl_loss: 2.0337\n",
      "Epoch 21/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3475.1099 - reconstruction_loss: 3459.7292 - kl_loss: 2.4558\n",
      "Epoch 22/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3460.4938 - reconstruction_loss: 3454.5898 - kl_loss: 2.2730\n",
      "Epoch 23/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3456.5922 - reconstruction_loss: 3455.1763 - kl_loss: 2.1297\n",
      "Epoch 24/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3450.8755 - reconstruction_loss: 3452.8296 - kl_loss: 1.9962\n",
      "Epoch 25/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3457.1318 - reconstruction_loss: 3453.4414 - kl_loss: 1.8929\n",
      "Epoch 26/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3470.4445 - reconstruction_loss: 3454.8438 - kl_loss: 1.7325\n",
      "Epoch 27/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3459.8824 - reconstruction_loss: 3458.1760 - kl_loss: 1.7759\n",
      "Epoch 28/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3464.6855 - reconstruction_loss: 3452.4043 - kl_loss: 1.6749\n",
      "Epoch 29/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3439.2503 - reconstruction_loss: 3456.8210 - kl_loss: 1.5659\n",
      "Epoch 30/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3440.8037 - reconstruction_loss: 3452.5220 - kl_loss: 1.6051\n",
      "Epoch 31/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3423.3443 - reconstruction_loss: 3452.1165 - kl_loss: 1.4490\n",
      "Epoch 32/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3496.3452 - reconstruction_loss: 3454.4478 - kl_loss: 1.6132\n",
      "Epoch 33/100\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 3426.2376 - reconstruction_loss: 3455.9453 - kl_loss: 3.4506\n",
      "Epoch 34/100\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 3437.0442 - reconstruction_loss: 3456.2776 - kl_loss: 2.9662\n",
      "Epoch 35/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3445.2917 - reconstruction_loss: 3458.0776 - kl_loss: 2.6586\n",
      "Epoch 36/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3422.8191 - reconstruction_loss: 3453.4011 - kl_loss: 2.3999\n",
      "Epoch 37/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3439.2107 - reconstruction_loss: 3453.4197 - kl_loss: 2.1611\n",
      "Epoch 38/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3464.4775 - reconstruction_loss: 3457.0891 - kl_loss: 1.9448\n",
      "Epoch 39/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3453.8126 - reconstruction_loss: 3459.8513 - kl_loss: 1.6540\n",
      "Epoch 40/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3413.6429 - reconstruction_loss: 3452.6082 - kl_loss: 1.4306\n",
      "Epoch 41/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3446.1372 - reconstruction_loss: 3457.9849 - kl_loss: 1.3762\n",
      "Epoch 42/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3485.0539 - reconstruction_loss: 3454.5684 - kl_loss: 1.4048\n",
      "Epoch 43/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3468.6265 - reconstruction_loss: 3454.0854 - kl_loss: 1.2145\n",
      "Epoch 44/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3440.9268 - reconstruction_loss: 3452.3943 - kl_loss: 1.4555\n",
      "Epoch 45/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3484.9998 - reconstruction_loss: 3457.1499 - kl_loss: 1.3341\n",
      "Epoch 46/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3472.2064 - reconstruction_loss: 3455.7690 - kl_loss: 1.2031\n",
      "Epoch 47/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3450.7004 - reconstruction_loss: 3452.1011 - kl_loss: 2.1234\n",
      "Epoch 48/100\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 3434.5788 - reconstruction_loss: 3451.9495 - kl_loss: 2.0518\n",
      "Epoch 49/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3447.7161 - reconstruction_loss: 3453.3538 - kl_loss: 1.8231\n",
      "Epoch 50/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3438.9490 - reconstruction_loss: 3453.4998 - kl_loss: 1.6420\n",
      "Epoch 51/100\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 3458.6786 - reconstruction_loss: 3450.8699 - kl_loss: 1.4750\n",
      "Epoch 52/100\n",
      "56/56 [==============================] - 2s 34ms/step - loss: 3486.3464 - reconstruction_loss: 3455.3171 - kl_loss: 1.3308\n",
      "Epoch 53/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3483.0023 - reconstruction_loss: 3453.1575 - kl_loss: 1.2511\n",
      "Epoch 54/100\n",
      "56/56 [==============================] - 2s 35ms/step - loss: 3466.7438 - reconstruction_loss: 3452.2915 - kl_loss: 1.2194\n",
      "Epoch 55/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3409.1279 - reconstruction_loss: 3453.7302 - kl_loss: 1.1904\n",
      "Epoch 56/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3442.5845 - reconstruction_loss: 3456.5667 - kl_loss: 1.2024\n",
      "Epoch 57/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3423.0977 - reconstruction_loss: 3452.4246 - kl_loss: 1.1736\n",
      "Epoch 58/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3457.0175 - reconstruction_loss: 3457.8528 - kl_loss: 1.2187\n",
      "Epoch 59/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3447.4595 - reconstruction_loss: 3456.3245 - kl_loss: 1.1526\n",
      "Epoch 60/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3463.3585 - reconstruction_loss: 3452.9058 - kl_loss: 1.0907\n",
      "Epoch 61/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3412.9612 - reconstruction_loss: 3454.4224 - kl_loss: 1.0337\n",
      "Epoch 62/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3469.8738 - reconstruction_loss: 3455.0747 - kl_loss: 1.3376\n",
      "Epoch 63/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3420.0334 - reconstruction_loss: 3457.7173 - kl_loss: 1.1443\n",
      "Epoch 64/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3472.4274 - reconstruction_loss: 3454.6350 - kl_loss: 1.0998\n",
      "Epoch 65/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3440.4036 - reconstruction_loss: 3453.4194 - kl_loss: 1.3482\n",
      "Epoch 66/100\n",
      "56/56 [==============================] - 2s 33ms/step - loss: 3455.1725 - reconstruction_loss: 3458.1482 - kl_loss: 1.1572\n",
      "Epoch 67/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3469.6834 - reconstruction_loss: 3478.4116 - kl_loss: 8.9252\n",
      "Epoch 68/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3462.9533 - reconstruction_loss: 3458.9468 - kl_loss: 37.9809\n",
      "Epoch 69/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3430.0504 - reconstruction_loss: 3454.6956 - kl_loss: 36.3066\n",
      "Epoch 70/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3478.3678 - reconstruction_loss: 3455.2712 - kl_loss: 35.7212\n",
      "Epoch 71/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3481.3218 - reconstruction_loss: 3451.3706 - kl_loss: 32.2793\n",
      "Epoch 72/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3457.7819 - reconstruction_loss: 3459.3892 - kl_loss: 27.5553\n",
      "Epoch 73/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3469.2273 - reconstruction_loss: 3454.1890 - kl_loss: 25.8196\n",
      "Epoch 74/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3475.5064 - reconstruction_loss: 3456.4587 - kl_loss: 24.0962\n",
      "Epoch 75/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3445.6609 - reconstruction_loss: 3454.8704 - kl_loss: 22.2448\n",
      "Epoch 76/100\n",
      "56/56 [==============================] - 2s 29ms/step - loss: 3446.4257 - reconstruction_loss: 3452.6934 - kl_loss: 19.0801\n",
      "Epoch 77/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3421.7993 - reconstruction_loss: 3450.4612 - kl_loss: 17.1774\n",
      "Epoch 78/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3450.6720 - reconstruction_loss: 3456.2483 - kl_loss: 16.1031\n",
      "Epoch 79/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3468.3447 - reconstruction_loss: 3456.0193 - kl_loss: 14.9243\n",
      "Epoch 80/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3442.3983 - reconstruction_loss: 3451.8367 - kl_loss: 13.2785\n",
      "Epoch 81/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3439.8252 - reconstruction_loss: 3460.1726 - kl_loss: 12.9674\n",
      "Epoch 82/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3454.4352 - reconstruction_loss: 3458.2656 - kl_loss: 12.8448\n",
      "Epoch 83/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3461.2636 - reconstruction_loss: 3453.7249 - kl_loss: 12.7429\n",
      "Epoch 84/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3474.4898 - reconstruction_loss: 3455.0369 - kl_loss: 12.2884\n",
      "Epoch 85/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3475.4690 - reconstruction_loss: 3453.0603 - kl_loss: 11.4348\n",
      "Epoch 86/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3439.0916 - reconstruction_loss: 3457.6743 - kl_loss: 11.3555\n",
      "Epoch 87/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3513.7349 - reconstruction_loss: 3453.1111 - kl_loss: 11.1906\n",
      "Epoch 88/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3434.2870 - reconstruction_loss: 3457.8823 - kl_loss: 9.6673\n",
      "Epoch 89/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3452.8721 - reconstruction_loss: 3458.2456 - kl_loss: 8.8727\n",
      "Epoch 90/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3453.2440 - reconstruction_loss: 3455.3059 - kl_loss: 8.9084\n",
      "Epoch 91/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3455.6462 - reconstruction_loss: 3456.8784 - kl_loss: 8.3668\n",
      "Epoch 92/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3440.6998 - reconstruction_loss: 3454.7280 - kl_loss: 7.6722\n",
      "Epoch 93/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3433.8472 - reconstruction_loss: 3457.0208 - kl_loss: 7.6277\n",
      "Epoch 94/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3458.9169 - reconstruction_loss: 3455.1738 - kl_loss: 7.5597\n",
      "Epoch 95/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3454.5127 - reconstruction_loss: 3451.6575 - kl_loss: 7.1655\n",
      "Epoch 96/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3433.0380 - reconstruction_loss: 3455.5269 - kl_loss: 6.7458\n",
      "Epoch 97/100\n",
      "56/56 [==============================] - 2s 32ms/step - loss: 3420.3192 - reconstruction_loss: 3450.6323 - kl_loss: 6.6785\n",
      "Epoch 98/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3450.1779 - reconstruction_loss: 3450.7566 - kl_loss: 6.6283\n",
      "Epoch 99/100\n",
      "56/56 [==============================] - 2s 31ms/step - loss: 3443.7237 - reconstruction_loss: 3454.7917 - kl_loss: 6.6083\n",
      "Epoch 100/100\n",
      "56/56 [==============================] - 2s 30ms/step - loss: 3436.4915 - reconstruction_loss: 3453.7661 - kl_loss: 6.5404\n"
     ]
    }
   ],
   "source": [
    "vae, history = train_VAE(data, num_dims=784, hidden_layer_n=[512, 256, 128] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6847.4556"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = data[-1, :].reshape([1, 784])\n",
    "\n",
    "z_mean, z_log_var, z = vae.encoder(sample.astype('float32'))\n",
    "reconstruction = vae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample.astype('float32'),reconstruction)\n",
    "reconstruction_loss.numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = vae_detect_outliers(data, vae, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[6725  275]\n",
      " [  69   31]]\n",
      "Confusion Matrix (Percentage):\n",
      "[[0.96071429 0.03928571]\n",
      " [0.69       0.31      ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# sklearn.metrics.confusion_matrix(lables, 1-classes, labels=None, sample_weight=None, normalize=None)\n",
    "true_labels = 1 - lables\n",
    "predicted_labels = classes\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Normalize the confusion matrix to get percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"Confusion Matrix (Percentage):\")\n",
    "print(cm_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.10130718954248366\n",
      "Recall: 0.31\n",
      "F1 Score: 0.15270935960591134\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "true_labels = 1- lables\n",
    "predicted_labels = classes\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 2s 17ms/step - loss: 13913.2075 - reconstruction_loss: 12779.5615 - z_loss: 0.0167 - REG_loss: 10.2060\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 9696.4660 - reconstruction_loss: 9030.4717 - z_loss: 1.6441e-05 - REG_loss: 0.0711\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 7132.9180 - reconstruction_loss: 6709.0601 - z_loss: 1.6159e-05 - REG_loss: 0.0040\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 5505.6176 - reconstruction_loss: 5271.9648 - z_loss: 1.6082e-05 - REG_loss: 0.0010\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4624.1415 - reconstruction_loss: 4417.0283 - z_loss: 1.6017e-05 - REG_loss: 3.2104e-04\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4021.5470 - reconstruction_loss: 3935.4287 - z_loss: 1.5956e-05 - REG_loss: 1.2038e-04\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3720.6035 - reconstruction_loss: 3680.3230 - z_loss: 1.5893e-05 - REG_loss: 4.0605e-05\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3573.6898 - reconstruction_loss: 3554.2803 - z_loss: 1.5828e-05 - REG_loss: 1.6651e-05\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3481.5293 - reconstruction_loss: 3495.8496 - z_loss: 1.5760e-05 - REG_loss: 5.9749e-06\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3467.2138 - reconstruction_loss: 3470.7798 - z_loss: 1.5690e-05 - REG_loss: 2.3514e-06\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3469.2427 - reconstruction_loss: 3460.6245 - z_loss: 1.5617e-05 - REG_loss: 7.6577e-07\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3449.4088 - reconstruction_loss: 3456.7791 - z_loss: 1.5541e-05 - REG_loss: 4.8193e-07\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3455.4049 - reconstruction_loss: 3455.4260 - z_loss: 1.5463e-05 - REG_loss: 1.9200e-07\n",
      "Epoch 14/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3492.7692 - reconstruction_loss: 3455.0479 - z_loss: 1.5382e-05 - REG_loss: 2.7575e-07\n",
      "Epoch 15/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3433.5241 - reconstruction_loss: 3454.8699 - z_loss: 1.5298e-05 - REG_loss: 4.3999e-07\n",
      "Epoch 16/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 3446.4304 - reconstruction_loss: 3455.0820 - z_loss: 1.5212e-05 - REG_loss: 3.0603e-07\n",
      "Epoch 17/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 3436.0928 - reconstruction_loss: 3454.8896 - z_loss: 1.5123e-05 - REG_loss: 3.7546e-07\n",
      "Epoch 18/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3437.7994 - reconstruction_loss: 3454.7366 - z_loss: 1.5032e-05 - REG_loss: 4.2744e-07\n",
      "Epoch 19/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3458.6413 - reconstruction_loss: 3454.9009 - z_loss: 1.4939e-05 - REG_loss: 1.9318e-07\n",
      "Epoch 20/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3474.6319 - reconstruction_loss: 3454.8884 - z_loss: 1.4844e-05 - REG_loss: 6.3688e-07\n",
      "Epoch 21/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3475.1004 - reconstruction_loss: 3454.8127 - z_loss: 1.4746e-05 - REG_loss: 2.8866e-07\n",
      "Epoch 22/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3460.4834 - reconstruction_loss: 3454.8777 - z_loss: 1.4645e-05 - REG_loss: 7.0482e-08\n",
      "Epoch 23/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3456.5800 - reconstruction_loss: 3454.7693 - z_loss: 1.4543e-05 - REG_loss: 2.0530e-07\n",
      "Epoch 24/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3450.8688 - reconstruction_loss: 3454.8787 - z_loss: 1.4440e-05 - REG_loss: 1.7775e-07\n",
      "Epoch 25/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3457.1251 - reconstruction_loss: 3455.2119 - z_loss: 1.4336e-05 - REG_loss: 6.1870e-07\n",
      "Epoch 26/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3470.4525 - reconstruction_loss: 3454.9626 - z_loss: 1.4232e-05 - REG_loss: 4.7115e-07\n",
      "Epoch 27/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3459.8685 - reconstruction_loss: 3455.0513 - z_loss: 1.4126e-05 - REG_loss: 5.9895e-07\n",
      "Epoch 28/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3464.6816 - reconstruction_loss: 3455.0439 - z_loss: 1.4018e-05 - REG_loss: 5.4432e-07\n",
      "Epoch 29/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3439.2555 - reconstruction_loss: 3455.0415 - z_loss: 1.3910e-05 - REG_loss: 5.0423e-07\n",
      "Epoch 30/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 3440.7971 - reconstruction_loss: 3455.0059 - z_loss: 1.3801e-05 - REG_loss: 4.2342e-07\n",
      "Epoch 31/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3423.3399 - reconstruction_loss: 3455.1733 - z_loss: 1.3691e-05 - REG_loss: 2.8803e-07\n",
      "Epoch 32/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3496.3358 - reconstruction_loss: 3454.8730 - z_loss: 1.3580e-05 - REG_loss: 4.0776e-07\n",
      "Epoch 33/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3426.2388 - reconstruction_loss: 3455.0850 - z_loss: 1.3468e-05 - REG_loss: 4.5135e-07\n",
      "Epoch 34/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3437.0513 - reconstruction_loss: 3455.1377 - z_loss: 1.3356e-05 - REG_loss: 5.2480e-07\n",
      "Epoch 35/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3445.2869 - reconstruction_loss: 3455.0005 - z_loss: 1.3243e-05 - REG_loss: 3.8061e-07\n",
      "Epoch 36/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3422.8090 - reconstruction_loss: 3455.2737 - z_loss: 1.3129e-05 - REG_loss: 7.7785e-07\n",
      "Epoch 37/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3439.2020 - reconstruction_loss: 3454.9199 - z_loss: 1.3015e-05 - REG_loss: 2.7774e-07\n",
      "Epoch 38/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3464.4722 - reconstruction_loss: 3455.1973 - z_loss: 1.2900e-05 - REG_loss: 4.5845e-07\n",
      "Epoch 39/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3453.7921 - reconstruction_loss: 3455.0117 - z_loss: 1.2784e-05 - REG_loss: 4.0415e-07\n",
      "Epoch 40/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 3413.6362 - reconstruction_loss: 3455.1802 - z_loss: 1.2669e-05 - REG_loss: 1.4681e-07\n",
      "Epoch 41/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3446.1395 - reconstruction_loss: 3455.1729 - z_loss: 1.2553e-05 - REG_loss: 3.5562e-07\n",
      "Epoch 42/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3485.0545 - reconstruction_loss: 3455.0813 - z_loss: 1.2436e-05 - REG_loss: 8.9609e-07\n",
      "Epoch 43/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 3468.6250 - reconstruction_loss: 3455.1890 - z_loss: 1.2319e-05 - REG_loss: 4.2352e-07\n",
      "Epoch 44/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 3440.9229 - reconstruction_loss: 3455.0647 - z_loss: 1.2201e-05 - REG_loss: 3.0184e-07\n",
      "Epoch 45/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3484.9933 - reconstruction_loss: 3455.2200 - z_loss: 1.2083e-05 - REG_loss: 8.9558e-07\n",
      "Epoch 46/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3472.2041 - reconstruction_loss: 3455.2576 - z_loss: 1.1964e-05 - REG_loss: 2.7750e-07\n",
      "Epoch 47/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3450.6892 - reconstruction_loss: 3455.1353 - z_loss: 1.1844e-05 - REG_loss: 1.0081e-06\n",
      "Epoch 48/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3434.5668 - reconstruction_loss: 3455.0935 - z_loss: 1.1724e-05 - REG_loss: 2.1784e-07\n",
      "Epoch 49/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3447.7135 - reconstruction_loss: 3455.4509 - z_loss: 1.1600e-05 - REG_loss: 1.9827e-06\n",
      "Epoch 50/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3438.9418 - reconstruction_loss: 3455.2007 - z_loss: 1.1476e-05 - REG_loss: 1.4420e-06\n",
      "Epoch 51/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3458.6856 - reconstruction_loss: 3455.3809 - z_loss: 1.1354e-05 - REG_loss: 1.3955e-06\n",
      "Epoch 52/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3486.3429 - reconstruction_loss: 3455.2126 - z_loss: 1.1230e-05 - REG_loss: 6.2260e-07\n",
      "Epoch 53/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3483.0073 - reconstruction_loss: 3455.1665 - z_loss: 1.1096e-05 - REG_loss: 1.5107e-06\n",
      "Epoch 54/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3466.7426 - reconstruction_loss: 3455.1626 - z_loss: 1.0970e-05 - REG_loss: 1.1391e-06\n",
      "Epoch 55/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3409.1295 - reconstruction_loss: 3455.4146 - z_loss: 1.0844e-05 - REG_loss: 8.3500e-07\n",
      "Epoch 56/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3442.6108 - reconstruction_loss: 3455.5566 - z_loss: 1.0718e-05 - REG_loss: 1.7227e-06\n",
      "Epoch 57/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3423.1069 - reconstruction_loss: 3455.5911 - z_loss: 1.0591e-05 - REG_loss: 8.5812e-07\n",
      "Epoch 58/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3457.0166 - reconstruction_loss: 3455.3396 - z_loss: 1.0464e-05 - REG_loss: 7.1915e-07\n",
      "Epoch 59/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3447.4567 - reconstruction_loss: 3455.2395 - z_loss: 1.0337e-05 - REG_loss: 7.6280e-07\n",
      "Epoch 60/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3463.3562 - reconstruction_loss: 3455.0754 - z_loss: 1.0209e-05 - REG_loss: 1.1471e-06\n",
      "Epoch 61/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 3412.9586 - reconstruction_loss: 3455.3523 - z_loss: 1.0081e-05 - REG_loss: 1.2912e-06\n",
      "Epoch 62/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3469.8810 - reconstruction_loss: 3455.3960 - z_loss: 9.9516e-06 - REG_loss: 1.2090e-06\n",
      "Epoch 63/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3420.0318 - reconstruction_loss: 3455.8315 - z_loss: 9.8238e-06 - REG_loss: 1.4190e-06\n",
      "Epoch 64/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3472.4624 - reconstruction_loss: 3455.8081 - z_loss: 9.6944e-06 - REG_loss: 1.0757e-06\n",
      "Epoch 65/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3440.4070 - reconstruction_loss: 3455.4729 - z_loss: 9.5651e-06 - REG_loss: 6.6823e-07\n",
      "Epoch 66/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3455.1679 - reconstruction_loss: 3455.1226 - z_loss: 9.4362e-06 - REG_loss: 7.2006e-07\n",
      "Epoch 67/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3459.6623 - reconstruction_loss: 3455.6394 - z_loss: 9.3075e-06 - REG_loss: 1.3982e-06\n",
      "Epoch 68/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3462.5606 - reconstruction_loss: 3455.5293 - z_loss: 9.1779e-06 - REG_loss: 1.0714e-06\n",
      "Epoch 69/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3429.7186 - reconstruction_loss: 3455.2671 - z_loss: 9.0504e-06 - REG_loss: 1.1383e-06\n",
      "Epoch 70/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3478.0200 - reconstruction_loss: 3455.2568 - z_loss: 8.9207e-06 - REG_loss: 1.4252e-06\n",
      "Epoch 71/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3481.0029 - reconstruction_loss: 3455.3250 - z_loss: 8.7928e-06 - REG_loss: 2.2171e-06\n",
      "Epoch 72/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3457.5072 - reconstruction_loss: 3455.2349 - z_loss: 8.6645e-06 - REG_loss: 1.0535e-06\n",
      "Epoch 73/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3468.9757 - reconstruction_loss: 3455.3315 - z_loss: 8.5372e-06 - REG_loss: 1.3891e-06\n",
      "Epoch 74/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3475.2709 - reconstruction_loss: 3455.5557 - z_loss: 8.4084e-06 - REG_loss: 9.7833e-07\n",
      "Epoch 75/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 3445.4415 - reconstruction_loss: 3455.3857 - z_loss: 8.2820e-06 - REG_loss: 1.8850e-06\n",
      "Epoch 76/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3446.2516 - reconstruction_loss: 3455.4272 - z_loss: 8.1544e-06 - REG_loss: 8.7272e-07\n",
      "Epoch 77/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3421.6320 - reconstruction_loss: 3455.2539 - z_loss: 8.0269e-06 - REG_loss: 1.2457e-06\n",
      "Epoch 78/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3450.5291 - reconstruction_loss: 3455.2810 - z_loss: 7.9017e-06 - REG_loss: 2.8804e-06\n",
      "Epoch 79/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3468.2058 - reconstruction_loss: 3455.5076 - z_loss: 7.7760e-06 - REG_loss: 2.1237e-06\n",
      "Epoch 80/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3442.2436 - reconstruction_loss: 3455.7058 - z_loss: 7.6489e-06 - REG_loss: 1.2190e-06\n",
      "Epoch 81/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 3439.6989 - reconstruction_loss: 3455.5115 - z_loss: 7.5246e-06 - REG_loss: 9.8491e-07\n",
      "Epoch 82/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3454.3091 - reconstruction_loss: 3455.3960 - z_loss: 7.3997e-06 - REG_loss: 1.4576e-06\n",
      "Epoch 83/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3461.1406 - reconstruction_loss: 3455.7197 - z_loss: 7.2760e-06 - REG_loss: 1.3850e-06\n",
      "Epoch 84/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3474.3689 - reconstruction_loss: 3455.7932 - z_loss: 7.1525e-06 - REG_loss: 2.2993e-06\n",
      "Epoch 85/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3475.3630 - reconstruction_loss: 3455.3862 - z_loss: 7.0287e-06 - REG_loss: 2.2545e-06\n",
      "Epoch 86/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3438.9791 - reconstruction_loss: 3455.5410 - z_loss: 6.9074e-06 - REG_loss: 2.8145e-06\n",
      "Epoch 87/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3513.6296 - reconstruction_loss: 3455.2227 - z_loss: 6.7852e-06 - REG_loss: 2.2207e-06\n",
      "Epoch 88/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3434.2001 - reconstruction_loss: 3455.4563 - z_loss: 6.6648e-06 - REG_loss: 3.7521e-06\n",
      "Epoch 89/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3452.8299 - reconstruction_loss: 3455.5515 - z_loss: 6.5445e-06 - REG_loss: 1.2708e-06\n",
      "Epoch 90/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3453.1693 - reconstruction_loss: 3455.9146 - z_loss: 6.4244e-06 - REG_loss: 1.5125e-06\n",
      "Epoch 91/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3455.5535 - reconstruction_loss: 3455.8193 - z_loss: 6.3067e-06 - REG_loss: 3.6242e-06\n",
      "Epoch 92/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3440.6329 - reconstruction_loss: 3455.7576 - z_loss: 6.1883e-06 - REG_loss: 2.4375e-06\n",
      "Epoch 93/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3433.7655 - reconstruction_loss: 3455.4512 - z_loss: 6.0731e-06 - REG_loss: 2.2570e-06\n",
      "Epoch 94/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3458.8590 - reconstruction_loss: 3455.6167 - z_loss: 5.9559e-06 - REG_loss: 2.4266e-06\n",
      "Epoch 95/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3454.4549 - reconstruction_loss: 3455.7607 - z_loss: 5.8409e-06 - REG_loss: 5.0560e-06\n",
      "Epoch 96/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 3432.9862 - reconstruction_loss: 3455.6902 - z_loss: 5.7279e-06 - REG_loss: 7.3632e-06\n",
      "Epoch 97/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3420.2671 - reconstruction_loss: 3455.5112 - z_loss: 5.6155e-06 - REG_loss: 3.3021e-06\n",
      "Epoch 98/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3450.1260 - reconstruction_loss: 3455.5420 - z_loss: 5.5041e-06 - REG_loss: 4.8150e-06\n",
      "Epoch 99/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3443.6598 - reconstruction_loss: 3455.7871 - z_loss: 5.3932e-06 - REG_loss: 3.3732e-06\n",
      "Epoch 100/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 3436.4383 - reconstruction_loss: 3455.9978 - z_loss: 5.2840e-06 - REG_loss: 3.6995e-06\n"
     ]
    }
   ],
   "source": [
    "rae, history = train_RAE(data, num_dims=784, hidden_layer_n=[512, 256, 128], z_loss_w = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tg/5frnjq714w93j2s_zkk0t39m0000gn/T/ipykernel_3453/684466154.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrae_detect_outliers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/tg/5frnjq714w93j2s_zkk0t39m0000gn/T/ipykernel_3453/58358239.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data, rae_model, num_dims)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mreconstruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrae_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mreconstruction_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMeanSquaredError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreconstruction_loss\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tensor, slice_spec, var)\u001b[0m\n\u001b[1;32m   1152\u001b[0m           \u001b[0mpacked_strides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked_strides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mvar_empty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m       \u001b[0mpacked_begin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpacked_strides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m     return strided_slice(\n\u001b[0m\u001b[1;32m   1157\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m         \u001b[0mpacked_begin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0mpacked_end\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input_, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, var, name)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mstrides\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[0mstrides\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m   op = gen_array_ops.strided_slice(\n\u001b[0m\u001b[1;32m   1330\u001b[0m       \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m       \u001b[0mbegin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m       \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, begin, end, strides, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m  10934\u001b[0m         \"new_axis_mask\", new_axis_mask, \"shrink_axis_mask\", shrink_axis_mask)\n\u001b[1;32m  10935\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10936\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10937\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10938\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m  10939\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10940\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10941\u001b[0m       return strided_slice_eager_fallback(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classes = rae_detect_outliers(data, rae, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
