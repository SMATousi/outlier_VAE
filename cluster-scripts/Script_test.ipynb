{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b741f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genetic_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782a034d",
   "metadata": {},
   "source": [
    "# mnist 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, lables = csv_data_loader(\"mnist-06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a384b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79e2d8",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7007c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_VAE(data,\n",
    "              latent_dim = 2,\n",
    "              hidden_layer_n = [20,18,16],\n",
    "              num_dims = 10,\n",
    "              kl_loss_factor = 0.01,\n",
    "              epochs = 100,\n",
    "              batch_size = 128\n",
    "              ):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Training the VAE on the data\n",
    "    \"\"\"\n",
    "\n",
    "    class Sampling(layers.Layer):\n",
    "        \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    latent_dim = latent_dim\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(num_dims,))\n",
    "    x = layers.Dense(num_dims, activation=\"tanh\")(encoder_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"tanh\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"tanh\")(latent_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"tanh\")(x)\n",
    "    decoder_outputs = layers.Dense(num_dims, activation=\"linear\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    class VAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "                name=\"reconstruction_loss\"\n",
    "            )\n",
    "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                reconstruction_loss = tf.keras.losses.MeanSquaredError()(data,reconstruction)\n",
    "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                total_loss = reconstruction_loss + kl_loss_factor * kl_loss\n",
    "        \n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            }\n",
    "    \n",
    "    creditdata = np.concatenate([data], axis=0)\n",
    "    creditdata = np.expand_dims(creditdata, -1).astype(\"float32\")\n",
    "\n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    history = vae.fit(creditdata,epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "\n",
    "    return vae, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4afc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae, history = train_VAE(data, num_dims=784, hidden_layer_n=[512, 256, 128] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca318b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[3, :].reshape([1, 784])\n",
    "\n",
    "z_mean, z_log_var, z = vae.encoder(sample.astype('float32'))\n",
    "reconstruction = vae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample.astype('float32'),reconstruction)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44934fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a4f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_detect_outliers(data,\n",
    "                        vae_model,\n",
    "                        num_dims\n",
    "                        ):\n",
    "\n",
    "    data_mean = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1, num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z_mean, z_log_var, z = vae_model.encoder(sample)\n",
    "        reconstruction = vae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample,reconstruction)\n",
    "        \n",
    "        data_mean.append(reconstruction_loss)\n",
    "    \n",
    "    data_mean = np.array(data_mean)\n",
    "    i_mean = np.mean(data_mean)\n",
    "    i_std = np.std(data_mean)\n",
    "    \n",
    "\n",
    "    threshold = i_mean + 3*i_std\n",
    "\n",
    "    classes = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z_mean, z_log_var, z = vae_model.encoder(sample)\n",
    "        reconstruction = vae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample,reconstruction)\n",
    "        \n",
    "        if reconstruction_loss > threshold:\n",
    "            \n",
    "            classes.append(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            classes.append(0)\n",
    "\n",
    "    classes = np.array(classes)\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ea131",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = vae_detect_outliers(data, vae, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878392f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# sklearn.metrics.confusion_matrix(lables, 1-classes, labels=None, sample_weight=None, normalize=None)\n",
    "true_labels = 1- lables\n",
    "predicted_labels = classes\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Normalize the confusion matrix to get percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"Confusion Matrix (Percentage):\")\n",
    "print(cm_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88f3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "true_labels = 1- lables\n",
    "predicted_labels = classes\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c80f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat[:, 0] = mat[:, 0]/(mat[0, 0]+ mat[1, 0])\n",
    "mat[:, 1] = mat[:, 1]/(mat[0, 1]+ mat[1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b350c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550728f",
   "metadata": {},
   "source": [
    "## RAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22afaf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RAE(data,\n",
    "              latent_dim = 2,\n",
    "              hidden_layer_n = [20,18,16],\n",
    "              num_dims = 10,\n",
    "              z_loss_w = 0.01,\n",
    "              REG_loss_w = 0.01,\n",
    "              epochs = 100,\n",
    "              batch_size = 128\n",
    "              ):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Training the RAE on the data\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(num_dims,))\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"sigmoid\")(encoder_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"sigmoid\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"sigmoid\")(x)\n",
    "    encoder_output = layers.Dense(latent_dim, activation=\"sigmoid\")(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_output, name=\"encoder\")\n",
    "\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"sigmoid\")(latent_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"sigmoid\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"sigmoid\")(x)\n",
    "    decoder_outputs = layers.Dense(num_dims, activation=\"linear\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "\n",
    "    class RAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "                name=\"reconstruction_loss\"\n",
    "            )\n",
    "            self.z_tracker = keras.metrics.Mean(name=\"z_loss\")\n",
    "            self.REG_tracker = keras.metrics.Mean(name=\"REG_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.z_tracker,\n",
    "                self.REG_tracker,\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "\n",
    "                reconstruction_loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(data,reconstruction)\n",
    "\n",
    "                z_loss = K.mean(K.square(z), axis=[1])\n",
    "        \n",
    "                REG_loss = K.mean(K.square(K.gradients(K.square(reconstruction), z)))\n",
    "\n",
    "#                 z_loss_w = z_loss_w\n",
    "#                 REG_loss_w = REG_loss_w\n",
    "\n",
    "                total_loss = reconstruction_loss +  z_loss_w * z_loss + REG_loss_w * REG_loss\n",
    "            \n",
    "                grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "                self.total_loss_tracker.update_state(total_loss)\n",
    "                self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "                self.z_tracker.update_state(z_loss)\n",
    "                self.REG_tracker.update_state(REG_loss)\n",
    "                del tape\n",
    "                return {\n",
    "                    \"loss\": self.total_loss_tracker.result(),\n",
    "                    \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                    \"z_loss\": self.z_tracker.result(),\n",
    "                    \"REG_loss\": self.REG_tracker.result(),\n",
    "                }\n",
    "\n",
    "    tdata = np.concatenate([data], axis=0)\n",
    "    tdata = np.expand_dims(tdata, -1).astype(\"float32\")\n",
    "\n",
    "    rae = RAE(encoder, decoder)\n",
    "    rae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    history = rae.fit(tdata, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "    return rae, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd14c62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rae, history = train_RAE(data, num_dims=784, hidden_layer_n=[512, 256, 128], z_loss_w = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a2bd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[1, :].reshape([1, 784])\n",
    "\n",
    "z = rae.encoder(sample.astype('float32'))\n",
    "reconstruction = rae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample.astype('float32'), reconstruction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a7991e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb1c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rae_detect_outliers(data,\n",
    "                        rae_model,\n",
    "                        num_dims\n",
    "                        ):\n",
    "\n",
    "    data_mean = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z = rae_model.encoder(sample)\n",
    "        reconstruction = rae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample, reconstruction)\n",
    "        \n",
    "        data_mean.append(reconstruction_loss)\n",
    "    \n",
    "    data_mean = np.array(data_mean)\n",
    "    data_std = np.std(data_mean)\n",
    "\n",
    "    threshold = data_mean + 3*data_std\n",
    "\n",
    "    classes = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "        \n",
    "        z = rae_model.encoder(sample)\n",
    "        reconstruction = rae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample, reconstruction)\n",
    "        \n",
    "        if any(reconstruction_loss > threshold):\n",
    "            \n",
    "            classes.append(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            classes.append(0)\n",
    "\n",
    "    classes = np.array(classes)\n",
    "\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95affb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = rae_detect_outliers(data, rae, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a2b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936ac782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# sklearn.metrics.confusion_matrix(lables, 1-classes, labels=None, sample_weight=None, normalize=None)\n",
    "true_labels = 1 - lables\n",
    "predicted_labels = classes\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Normalize the confusion matrix to get percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"Confusion Matrix (Percentage):\")\n",
    "print(cm_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe551cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "true_labels = 1- lables\n",
    "predicted_labels = classes\n",
    "\n",
    "precision = precision_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate recall\n",
    "recall = recall_score(true_labels, predicted_labels)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(true_labels, predicted_labels)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd90108",
   "metadata": {},
   "source": [
    "# mnist-25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fb6f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = csv_data_loader(\"mnist-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c1fec8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7090, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8398e8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7090,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b68d1",
   "metadata": {},
   "source": [
    "## VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5a820756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_VAE(data,\n",
    "              latent_dim = 2,\n",
    "              hidden_layer_n = [20,18,16],\n",
    "              num_dims = 10,\n",
    "              kl_loss_factor = 0.1,\n",
    "              epochs = 200,\n",
    "              batch_size = 64\n",
    "              ):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Training the VAE on the data\n",
    "    \"\"\"\n",
    "\n",
    "    class Sampling(layers.Layer):\n",
    "        \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "        def call(self, inputs):\n",
    "            z_mean, z_log_var = inputs\n",
    "            batch = tf.shape(z_mean)[0]\n",
    "            dim = tf.shape(z_mean)[1]\n",
    "            epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "            return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "    latent_dim = latent_dim\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(num_dims,))\n",
    "    x = layers.Dense(num_dims, activation=\"tanh\")(encoder_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"tanh\")(x)\n",
    "    z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "    encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"tanh\")(latent_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"tanh\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"tanh\")(x)\n",
    "    decoder_outputs = layers.Dense(num_dims, activation=\"linear\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "    class VAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "                name=\"reconstruction_loss\"\n",
    "            )\n",
    "            self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker,\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape() as tape:\n",
    "                z_mean, z_log_var, z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "                reconstruction_loss = tf.keras.losses.MeanSquaredError()(data,reconstruction)\n",
    "                kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "                total_loss = reconstruction_loss + kl_loss_factor * kl_loss\n",
    "        \n",
    "            grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "            self.total_loss_tracker.update_state(total_loss)\n",
    "            self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "            self.kl_loss_tracker.update_state(kl_loss)\n",
    "            return {\n",
    "                \"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result(),\n",
    "            }\n",
    "    \n",
    "    creditdata = np.concatenate([data], axis=0)\n",
    "    creditdata = np.expand_dims(creditdata, -1).astype(\"float32\")\n",
    "\n",
    "    vae = VAE(encoder, decoder)\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    history = vae.fit(creditdata,epochs=epochs,batch_size=batch_size,verbose=1)\n",
    "\n",
    "    return vae, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8158df8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 7483.5681 - reconstruction_loss: 6718.9336 - kl_loss: 9.9495\n",
      "Epoch 2/200\n",
      "111/111 [==============================] - 2s 22ms/step - loss: 5235.6359 - reconstruction_loss: 4986.9756 - kl_loss: 2.2114\n",
      "Epoch 3/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4511.4428 - reconstruction_loss: 4432.0234 - kl_loss: 1.5608\n",
      "Epoch 4/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4287.9073 - reconstruction_loss: 4271.2686 - kl_loss: 2.6339\n",
      "Epoch 5/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4227.0034 - reconstruction_loss: 4231.8589 - kl_loss: 2.0421\n",
      "Epoch 6/200\n",
      "111/111 [==============================] - 2s 19ms/step - loss: 4230.7820 - reconstruction_loss: 4224.0093 - kl_loss: 1.0265\n",
      "Epoch 7/200\n",
      "111/111 [==============================] - 2s 19ms/step - loss: 4218.2061 - reconstruction_loss: 4223.2607 - kl_loss: 5.4165\n",
      "Epoch 8/200\n",
      "111/111 [==============================] - 2s 21ms/step - loss: 4213.5659 - reconstruction_loss: 4222.8647 - kl_loss: 4.7539\n",
      "Epoch 9/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4215.9406 - reconstruction_loss: 4222.6304 - kl_loss: 4.4545\n",
      "Epoch 10/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4215.4975 - reconstruction_loss: 4222.6992 - kl_loss: 4.1802\n",
      "Epoch 11/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4224.8206 - reconstruction_loss: 4222.8613 - kl_loss: 3.9868\n",
      "Epoch 12/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4228.9143 - reconstruction_loss: 4222.8208 - kl_loss: 3.6846\n",
      "Epoch 13/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4208.0019 - reconstruction_loss: 4222.7451 - kl_loss: 2.8369\n",
      "Epoch 14/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4222.3639 - reconstruction_loss: 4222.9976 - kl_loss: 2.6135\n",
      "Epoch 15/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4224.6606 - reconstruction_loss: 4222.8521 - kl_loss: 2.4886\n",
      "Epoch 16/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4235.3521 - reconstruction_loss: 4223.2739 - kl_loss: 2.3709\n",
      "Epoch 17/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4226.5836 - reconstruction_loss: 4223.2407 - kl_loss: 2.2202\n",
      "Epoch 18/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4208.2868 - reconstruction_loss: 4222.9194 - kl_loss: 2.0449\n",
      "Epoch 19/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4216.5718 - reconstruction_loss: 4222.9585 - kl_loss: 1.8638\n",
      "Epoch 20/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4216.9293 - reconstruction_loss: 4223.1611 - kl_loss: 1.6439\n",
      "Epoch 21/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4215.8384 - reconstruction_loss: 4223.6260 - kl_loss: 1.4455\n",
      "Epoch 22/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4208.0854 - reconstruction_loss: 4222.6406 - kl_loss: 1.3713\n",
      "Epoch 23/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4217.3575 - reconstruction_loss: 4222.6558 - kl_loss: 1.3105\n",
      "Epoch 24/200\n",
      "111/111 [==============================] - 1s 12ms/step - loss: 4215.4334 - reconstruction_loss: 4223.5020 - kl_loss: 1.2717\n",
      "Epoch 25/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4235.2244 - reconstruction_loss: 4223.1973 - kl_loss: 1.2387\n",
      "Epoch 26/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4227.2980 - reconstruction_loss: 4222.4834 - kl_loss: 1.1790\n",
      "Epoch 27/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4234.9771 - reconstruction_loss: 4222.9829 - kl_loss: 1.0962\n",
      "Epoch 28/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4227.6175 - reconstruction_loss: 4223.0840 - kl_loss: 0.9234\n",
      "Epoch 29/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4220.1809 - reconstruction_loss: 4222.9531 - kl_loss: 0.8008\n",
      "Epoch 30/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4209.5663 - reconstruction_loss: 4223.5488 - kl_loss: 0.7557\n",
      "Epoch 31/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4221.3787 - reconstruction_loss: 4223.3745 - kl_loss: 0.8248\n",
      "Epoch 32/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4209.5803 - reconstruction_loss: 4223.4746 - kl_loss: 0.7701\n",
      "Epoch 33/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4210.1823 - reconstruction_loss: 4223.0176 - kl_loss: 0.7042\n",
      "Epoch 34/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4224.9999 - reconstruction_loss: 4223.2256 - kl_loss: 0.6601\n",
      "Epoch 35/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4212.8852 - reconstruction_loss: 4223.0493 - kl_loss: 0.6171\n",
      "Epoch 36/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4187.6788 - reconstruction_loss: 4223.0571 - kl_loss: 0.5537\n",
      "Epoch 37/200\n",
      "111/111 [==============================] - 2s 20ms/step - loss: 4222.8585 - reconstruction_loss: 4223.1499 - kl_loss: 0.5748\n",
      "Epoch 38/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4218.4437 - reconstruction_loss: 4223.6655 - kl_loss: 0.5565\n",
      "Epoch 39/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4224.1732 - reconstruction_loss: 4223.3159 - kl_loss: 0.4733\n",
      "Epoch 40/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4232.7945 - reconstruction_loss: 4223.4111 - kl_loss: 0.4374\n",
      "Epoch 41/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4220.6788 - reconstruction_loss: 4223.2344 - kl_loss: 0.4851\n",
      "Epoch 42/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4224.9677 - reconstruction_loss: 4223.1982 - kl_loss: 0.6989\n",
      "Epoch 43/200\n",
      "111/111 [==============================] - 2s 20ms/step - loss: 4212.4203 - reconstruction_loss: 4223.8276 - kl_loss: 1.6916\n",
      "Epoch 44/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4229.2306 - reconstruction_loss: 4223.5874 - kl_loss: 1.3763\n",
      "Epoch 45/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4225.2310 - reconstruction_loss: 4223.6626 - kl_loss: 1.1520\n",
      "Epoch 46/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4215.7418 - reconstruction_loss: 4222.8818 - kl_loss: 1.0004\n",
      "Epoch 47/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4218.9725 - reconstruction_loss: 4223.8755 - kl_loss: 0.9090\n",
      "Epoch 48/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4210.3823 - reconstruction_loss: 4222.6719 - kl_loss: 0.8108\n",
      "Epoch 49/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4234.6826 - reconstruction_loss: 4223.4561 - kl_loss: 0.7160\n",
      "Epoch 50/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4237.9874 - reconstruction_loss: 4223.3740 - kl_loss: 0.6384\n",
      "Epoch 51/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4235.3978 - reconstruction_loss: 4223.4414 - kl_loss: 0.5744\n",
      "Epoch 52/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4207.8779 - reconstruction_loss: 4223.1885 - kl_loss: 0.5022\n",
      "Epoch 53/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4226.0269 - reconstruction_loss: 4223.5283 - kl_loss: 0.4336\n",
      "Epoch 54/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4218.2338 - reconstruction_loss: 4223.0298 - kl_loss: 0.3498\n",
      "Epoch 55/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4216.3852 - reconstruction_loss: 4223.3647 - kl_loss: 0.2879\n",
      "Epoch 56/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4210.6252 - reconstruction_loss: 4223.1064 - kl_loss: 0.2430\n",
      "Epoch 57/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4215.9280 - reconstruction_loss: 4223.8413 - kl_loss: 0.6700\n",
      "Epoch 58/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4214.7749 - reconstruction_loss: 4223.3667 - kl_loss: 4.5483\n",
      "Epoch 59/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4217.6735 - reconstruction_loss: 4223.2876 - kl_loss: 3.5966\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 2s 16ms/step - loss: 4234.1325 - reconstruction_loss: 4223.4985 - kl_loss: 3.3094\n",
      "Epoch 61/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4221.1360 - reconstruction_loss: 4223.1338 - kl_loss: 3.1506\n",
      "Epoch 62/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4216.4707 - reconstruction_loss: 4223.3237 - kl_loss: 2.9255\n",
      "Epoch 63/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4212.5886 - reconstruction_loss: 4223.4004 - kl_loss: 2.6802\n",
      "Epoch 64/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4235.5768 - reconstruction_loss: 4223.3296 - kl_loss: 2.2814\n",
      "Epoch 65/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4229.8571 - reconstruction_loss: 4222.5605 - kl_loss: 2.9550\n",
      "Epoch 66/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4230.0693 - reconstruction_loss: 4223.2363 - kl_loss: 3.1133\n",
      "Epoch 67/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4231.8524 - reconstruction_loss: 4223.3643 - kl_loss: 2.7260\n",
      "Epoch 68/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4224.1875 - reconstruction_loss: 4223.2051 - kl_loss: 2.5184\n",
      "Epoch 69/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4232.5825 - reconstruction_loss: 4223.5005 - kl_loss: 2.3586\n",
      "Epoch 70/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4216.4378 - reconstruction_loss: 4223.1934 - kl_loss: 2.2266\n",
      "Epoch 71/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4222.5521 - reconstruction_loss: 4223.4038 - kl_loss: 2.1123\n",
      "Epoch 72/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4219.1795 - reconstruction_loss: 4223.3799 - kl_loss: 2.0087\n",
      "Epoch 73/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4203.5466 - reconstruction_loss: 4224.1675 - kl_loss: 1.9114\n",
      "Epoch 74/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4215.9579 - reconstruction_loss: 4223.3589 - kl_loss: 1.8175\n",
      "Epoch 75/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4213.6463 - reconstruction_loss: 4223.3945 - kl_loss: 1.7258\n",
      "Epoch 76/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4229.9239 - reconstruction_loss: 4223.0435 - kl_loss: 1.6271\n",
      "Epoch 77/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4222.3109 - reconstruction_loss: 4222.6230 - kl_loss: 1.3420\n",
      "Epoch 78/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4223.7339 - reconstruction_loss: 4223.3267 - kl_loss: 1.2720\n",
      "Epoch 79/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4216.8630 - reconstruction_loss: 4223.2583 - kl_loss: 1.2083\n",
      "Epoch 80/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4236.7078 - reconstruction_loss: 4222.9644 - kl_loss: 1.0651\n",
      "Epoch 81/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4211.4409 - reconstruction_loss: 4222.9941 - kl_loss: 1.0177\n",
      "Epoch 82/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4230.8125 - reconstruction_loss: 4222.8403 - kl_loss: 0.9868\n",
      "Epoch 83/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4220.3986 - reconstruction_loss: 4223.4995 - kl_loss: 0.9208\n",
      "Epoch 84/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4225.8289 - reconstruction_loss: 4223.1763 - kl_loss: 0.8154\n",
      "Epoch 85/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4217.5506 - reconstruction_loss: 4223.2656 - kl_loss: 0.7480\n",
      "Epoch 86/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4222.6740 - reconstruction_loss: 4222.9424 - kl_loss: 0.6921\n",
      "Epoch 87/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4228.7025 - reconstruction_loss: 4223.0444 - kl_loss: 0.6550\n",
      "Epoch 88/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4223.5668 - reconstruction_loss: 4223.4414 - kl_loss: 0.5302\n",
      "Epoch 89/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4247.3119 - reconstruction_loss: 4223.1309 - kl_loss: 0.4416\n",
      "Epoch 90/200\n",
      "111/111 [==============================] - 2s 18ms/step - loss: 4233.0556 - reconstruction_loss: 4223.1826 - kl_loss: 0.3947\n",
      "Epoch 91/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4219.9786 - reconstruction_loss: 4223.1489 - kl_loss: 0.3622\n",
      "Epoch 92/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4221.2091 - reconstruction_loss: 4223.2754 - kl_loss: 0.3343\n",
      "Epoch 93/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4233.2600 - reconstruction_loss: 4223.2915 - kl_loss: 0.3001\n",
      "Epoch 94/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4208.5354 - reconstruction_loss: 4223.2749 - kl_loss: 0.2477\n",
      "Epoch 95/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4221.6170 - reconstruction_loss: 4222.8037 - kl_loss: 0.2082\n",
      "Epoch 96/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4249.8415 - reconstruction_loss: 4222.7979 - kl_loss: 0.1755\n",
      "Epoch 97/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4225.2701 - reconstruction_loss: 4223.1128 - kl_loss: 0.1474\n",
      "Epoch 98/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4222.8790 - reconstruction_loss: 4223.5176 - kl_loss: 0.0895\n",
      "Epoch 99/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4227.7298 - reconstruction_loss: 4223.1045 - kl_loss: 0.0632\n",
      "Epoch 100/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4243.5993 - reconstruction_loss: 4223.4868 - kl_loss: 0.0238\n",
      "Epoch 101/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4227.6261 - reconstruction_loss: 4223.2842 - kl_loss: 0.0033\n",
      "Epoch 102/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4224.2630 - reconstruction_loss: 4223.3496 - kl_loss: 9.7546e-04\n",
      "Epoch 103/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4203.1403 - reconstruction_loss: 4223.4380 - kl_loss: 2.4803e-04\n",
      "Epoch 104/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4207.1408 - reconstruction_loss: 4223.1055 - kl_loss: 5.3237e-05\n",
      "Epoch 105/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4222.8146 - reconstruction_loss: 4223.7842 - kl_loss: 6.0858e-05\n",
      "Epoch 106/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4212.6096 - reconstruction_loss: 4223.7070 - kl_loss: 8.3813e-06\n",
      "Epoch 107/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4221.6480 - reconstruction_loss: 4223.2090 - kl_loss: 6.8804e-04\n",
      "Epoch 108/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4235.1830 - reconstruction_loss: 4223.2603 - kl_loss: 0.0019\n",
      "Epoch 109/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4226.6059 - reconstruction_loss: 4222.8794 - kl_loss: 0.0046\n",
      "Epoch 110/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4208.5847 - reconstruction_loss: 4223.6968 - kl_loss: 0.0019\n",
      "Epoch 111/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4223.1202 - reconstruction_loss: 4223.6191 - kl_loss: 4.6769e-04\n",
      "Epoch 112/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4218.5061 - reconstruction_loss: 4223.9189 - kl_loss: 2.1802e-04\n",
      "Epoch 113/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4219.9415 - reconstruction_loss: 4223.1909 - kl_loss: 9.8253e-05\n",
      "Epoch 114/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4223.6329 - reconstruction_loss: 4222.8643 - kl_loss: 4.2009e-05\n",
      "Epoch 115/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4215.8201 - reconstruction_loss: 4223.5215 - kl_loss: 1.6965e-05\n",
      "Epoch 116/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4244.8930 - reconstruction_loss: 4223.7388 - kl_loss: 6.4515e-06\n",
      "Epoch 117/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4223.5379 - reconstruction_loss: 4222.9722 - kl_loss: 2.3401e-06\n",
      "Epoch 118/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 2s 14ms/step - loss: 4238.2654 - reconstruction_loss: 4223.0391 - kl_loss: 7.6988e-07\n",
      "Epoch 119/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4230.1164 - reconstruction_loss: 4223.1675 - kl_loss: 2.5345e-07\n",
      "Epoch 120/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4197.6064 - reconstruction_loss: 4223.2778 - kl_loss: 7.3249e-08\n",
      "Epoch 121/200\n",
      "111/111 [==============================] - 2s 18ms/step - loss: 4217.5277 - reconstruction_loss: 4223.6519 - kl_loss: 1.8293e-08\n",
      "Epoch 122/200\n",
      "111/111 [==============================] - 2s 20ms/step - loss: 4232.5534 - reconstruction_loss: 4223.8164 - kl_loss: 0.0000e+00\n",
      "Epoch 123/200\n",
      "111/111 [==============================] - 2s 21ms/step - loss: 4239.2033 - reconstruction_loss: 4223.3271 - kl_loss: 0.0000e+00\n",
      "Epoch 124/200\n",
      "111/111 [==============================] - 2s 18ms/step - loss: 4205.7422 - reconstruction_loss: 4223.3750 - kl_loss: 0.0000e+00\n",
      "Epoch 125/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4228.6739 - reconstruction_loss: 4223.0996 - kl_loss: 2.1522e-09\n",
      "Epoch 126/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4246.2662 - reconstruction_loss: 4223.0220 - kl_loss: 0.0000e+00\n",
      "Epoch 127/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4213.6938 - reconstruction_loss: 4223.0454 - kl_loss: 0.0000e+00\n",
      "Epoch 128/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4223.1718 - reconstruction_loss: 4223.6631 - kl_loss: 0.0000e+00\n",
      "Epoch 129/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4202.3157 - reconstruction_loss: 4223.3828 - kl_loss: 0.0000e+00\n",
      "Epoch 130/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4226.4687 - reconstruction_loss: 4223.3794 - kl_loss: 0.0000e+00\n",
      "Epoch 131/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4223.8077 - reconstruction_loss: 4223.2026 - kl_loss: 0.0000e+00\n",
      "Epoch 132/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4209.2437 - reconstruction_loss: 4223.1206 - kl_loss: 0.0000e+00\n",
      "Epoch 133/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4216.3597 - reconstruction_loss: 4223.1230 - kl_loss: 0.0000e+00\n",
      "Epoch 134/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4220.6869 - reconstruction_loss: 4223.0767 - kl_loss: 0.0000e+00\n",
      "Epoch 135/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4233.3847 - reconstruction_loss: 4222.9170 - kl_loss: 0.0000e+00\n",
      "Epoch 136/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4225.3708 - reconstruction_loss: 4223.7559 - kl_loss: 0.0000e+00\n",
      "Epoch 137/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4222.9062 - reconstruction_loss: 4223.2954 - kl_loss: 0.0000e+00\n",
      "Epoch 138/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4231.2364 - reconstruction_loss: 4223.3569 - kl_loss: 0.0000e+00\n",
      "Epoch 139/200\n",
      "111/111 [==============================] - 2s 18ms/step - loss: 4219.7233 - reconstruction_loss: 4223.0820 - kl_loss: 0.0000e+00\n",
      "Epoch 140/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4206.2014 - reconstruction_loss: 4223.3433 - kl_loss: 0.0000e+00\n",
      "Epoch 141/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4208.9176 - reconstruction_loss: 4223.6533 - kl_loss: 0.0000e+00\n",
      "Epoch 142/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4209.7617 - reconstruction_loss: 4223.4307 - kl_loss: 0.0000e+00\n",
      "Epoch 143/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4226.5971 - reconstruction_loss: 4222.5728 - kl_loss: 0.0000e+00\n",
      "Epoch 144/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4220.5306 - reconstruction_loss: 4223.4316 - kl_loss: 0.0000e+00\n",
      "Epoch 145/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4229.2196 - reconstruction_loss: 4222.8989 - kl_loss: 0.0000e+00\n",
      "Epoch 146/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4212.1827 - reconstruction_loss: 4223.7979 - kl_loss: 0.0000e+00\n",
      "Epoch 147/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4223.5511 - reconstruction_loss: 4222.9316 - kl_loss: 9.1214e-10\n",
      "Epoch 148/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4232.8301 - reconstruction_loss: 4223.4243 - kl_loss: 2.2867e-09\n",
      "Epoch 149/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4247.3738 - reconstruction_loss: 4223.1431 - kl_loss: 0.0000e+00\n",
      "Epoch 150/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4221.8404 - reconstruction_loss: 4223.3599 - kl_loss: 0.0000e+00\n",
      "Epoch 151/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4225.1988 - reconstruction_loss: 4223.3408 - kl_loss: 2.5494e-09\n",
      "Epoch 152/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4212.8419 - reconstruction_loss: 4223.0298 - kl_loss: 1.7486e-09\n",
      "Epoch 153/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4231.0745 - reconstruction_loss: 4223.0229 - kl_loss: 1.8831e-09\n",
      "Epoch 154/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4224.6327 - reconstruction_loss: 4223.1870 - kl_loss: 1.7486e-09\n",
      "Epoch 155/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4231.2061 - reconstruction_loss: 4223.2515 - kl_loss: 3.0937e-09\n",
      "Epoch 156/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4225.1220 - reconstruction_loss: 4224.0947 - kl_loss: 3.0937e-09\n",
      "Epoch 157/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4234.7501 - reconstruction_loss: 4223.5776 - kl_loss: 2.5557e-09\n",
      "Epoch 158/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4230.5589 - reconstruction_loss: 4223.1333 - kl_loss: 2.4212e-09\n",
      "Epoch 159/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4206.3927 - reconstruction_loss: 4222.9253 - kl_loss: 3.6023e-09\n",
      "Epoch 160/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4216.8842 - reconstruction_loss: 4223.2173 - kl_loss: 1.1190e-08\n",
      "Epoch 161/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4212.3702 - reconstruction_loss: 4223.1768 - kl_loss: 8.5309e-09\n",
      "Epoch 162/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4215.8437 - reconstruction_loss: 4223.4438 - kl_loss: 7.3980e-09\n",
      "Epoch 163/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4217.1232 - reconstruction_loss: 4223.0591 - kl_loss: 3.6318e-09\n",
      "Epoch 164/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4193.0712 - reconstruction_loss: 4223.8120 - kl_loss: 4.0353e-09\n",
      "Epoch 165/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4223.7720 - reconstruction_loss: 4222.8501 - kl_loss: 1.6246e-08\n",
      "Epoch 166/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4214.6124 - reconstruction_loss: 4222.8853 - kl_loss: 1.6545e-08\n",
      "Epoch 167/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4211.8393 - reconstruction_loss: 4223.6216 - kl_loss: 6.0109e-09\n",
      "Epoch 168/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4243.3323 - reconstruction_loss: 4223.3975 - kl_loss: 1.7860e-08\n",
      "Epoch 169/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4224.8508 - reconstruction_loss: 4223.0381 - kl_loss: 1.3451e-08\n",
      "Epoch 170/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4218.0530 - reconstruction_loss: 4223.2910 - kl_loss: 1.4258e-08\n",
      "Epoch 171/200\n",
      "111/111 [==============================] - 1s 13ms/step - loss: 4229.5671 - reconstruction_loss: 4223.0151 - kl_loss: 1.7825e-08\n",
      "Epoch 172/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4222.7588 - reconstruction_loss: 4223.3125 - kl_loss: 6.0487e-09\n",
      "Epoch 173/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4208.9930 - reconstruction_loss: 4223.3369 - kl_loss: 2.0170e-08\n",
      "Epoch 174/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4231.2559 - reconstruction_loss: 4223.4858 - kl_loss: 6.3283e-09\n",
      "Epoch 175/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111/111 [==============================] - 2s 15ms/step - loss: 4232.2221 - reconstruction_loss: 4222.5933 - kl_loss: 1.3123e-08\n",
      "Epoch 176/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4247.8536 - reconstruction_loss: 4223.1841 - kl_loss: 1.1749e-08\n",
      "Epoch 177/200\n",
      "111/111 [==============================] - 2s 16ms/step - loss: 4203.8900 - reconstruction_loss: 4223.1250 - kl_loss: 1.0492e-08\n",
      "Epoch 178/200\n",
      "111/111 [==============================] - 2s 17ms/step - loss: 4218.2171 - reconstruction_loss: 4223.5034 - kl_loss: 1.7873e-08\n",
      "Epoch 179/200\n",
      "111/111 [==============================] - 2s 18ms/step - loss: 4229.4735 - reconstruction_loss: 4223.4570 - kl_loss: 5.5990e-09\n",
      "Epoch 180/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4235.7914 - reconstruction_loss: 4222.7100 - kl_loss: 1.6246e-08\n",
      "Epoch 181/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4210.2179 - reconstruction_loss: 4223.1904 - kl_loss: 1.6141e-08\n",
      "Epoch 182/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4224.9955 - reconstruction_loss: 4223.3257 - kl_loss: 1.1164e-08\n",
      "Epoch 183/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4214.1490 - reconstruction_loss: 4223.3359 - kl_loss: 2.0866e-08\n",
      "Epoch 184/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4218.9590 - reconstruction_loss: 4223.3716 - kl_loss: 2.8529e-08\n",
      "Epoch 185/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4237.9101 - reconstruction_loss: 4223.1870 - kl_loss: 2.7352e-08\n",
      "Epoch 186/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4222.5300 - reconstruction_loss: 4224.1182 - kl_loss: 2.4018e-08\n",
      "Epoch 187/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4241.1421 - reconstruction_loss: 4222.9004 - kl_loss: 4.2640e-08\n",
      "Epoch 188/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4211.0970 - reconstruction_loss: 4223.3013 - kl_loss: 3.3989e-08\n",
      "Epoch 189/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4219.7691 - reconstruction_loss: 4223.4351 - kl_loss: 3.0609e-08\n",
      "Epoch 190/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4212.9540 - reconstruction_loss: 4223.1406 - kl_loss: 3.5263e-08\n",
      "Epoch 191/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4224.9218 - reconstruction_loss: 4223.0151 - kl_loss: 3.2955e-08\n",
      "Epoch 192/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4239.7778 - reconstruction_loss: 4223.3579 - kl_loss: 4.4838e-08\n",
      "Epoch 193/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4218.8339 - reconstruction_loss: 4223.6396 - kl_loss: 8.8112e-08\n",
      "Epoch 194/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4197.0166 - reconstruction_loss: 4223.1841 - kl_loss: 7.2368e-08\n",
      "Epoch 195/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4221.0155 - reconstruction_loss: 4223.2192 - kl_loss: 1.2398e-07\n",
      "Epoch 196/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4238.4552 - reconstruction_loss: 4223.0762 - kl_loss: 1.0883e-07\n",
      "Epoch 197/200\n",
      "111/111 [==============================] - 2s 14ms/step - loss: 4219.6815 - reconstruction_loss: 4223.5532 - kl_loss: 5.7301e-08\n",
      "Epoch 198/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4221.6019 - reconstruction_loss: 4223.4702 - kl_loss: 1.5563e-07\n",
      "Epoch 199/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4223.9545 - reconstruction_loss: 4223.3110 - kl_loss: 1.1724e-07\n",
      "Epoch 200/200\n",
      "111/111 [==============================] - 2s 15ms/step - loss: 4229.7671 - reconstruction_loss: 4222.8901 - kl_loss: 5.6032e-08\n"
     ]
    }
   ],
   "source": [
    "vae, history = train_VAE(data, num_dims=784, hidden_layer_n=[512, 256, 128] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39575c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[1, :].reshape([1, 784])\n",
    "\n",
    "z_mean, z_log_var, z = vae.encoder(sample.astype('float32'))\n",
    "reconstruction = vae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample.astype('float32'),reconstruction)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "357c9728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3371.2625"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ac4ae72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef7e13d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_detect_outliers(data,\n",
    "                        vae_model,\n",
    "                        num_dims\n",
    "                        ):\n",
    "\n",
    "    data_mean = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1, num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z_mean, z_log_var, z = vae_model.encoder(sample)\n",
    "        reconstruction = vae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample,reconstruction)\n",
    "        \n",
    "        data_mean.append(reconstruction_loss)\n",
    "    \n",
    "    data_mean = np.array(data_mean)\n",
    "    i_mean = np.mean(data_mean)\n",
    "    i_std = np.std(data_mean)\n",
    "    \n",
    "\n",
    "    threshold = i_mean + 2*i_std\n",
    "\n",
    "    classes = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z_mean, z_log_var, z = vae_model.encoder(sample)\n",
    "        reconstruction = vae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample,reconstruction)\n",
    "        \n",
    "        if reconstruction_loss > threshold:\n",
    "            \n",
    "            classes.append(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            classes.append(0)\n",
    "\n",
    "    classes = np.array(classes)\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e1940f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = vae_detect_outliers(data, vae, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b88a5478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8143d7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[6749  241]\n",
      " [  69   31]]\n",
      "Confusion Matrix (Percentage):\n",
      "[[0.96552217 0.03447783]\n",
      " [0.69       0.31      ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# sklearn.metrics.confusion_matrix(lables, 1-classes, labels=None, sample_weight=None, normalize=None)\n",
    "true_labels = 1- labels\n",
    "predicted_labels = classes\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Normalize the confusion matrix to get percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"Confusion Matrix (Percentage):\")\n",
    "print(cm_percentage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59429041",
   "metadata": {},
   "source": [
    "## RAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fcbdd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_RAE(data,\n",
    "              latent_dim = 2,\n",
    "              hidden_layer_n = [20,18,16],\n",
    "              num_dims = 10,\n",
    "              z_loss_w = 0.01,\n",
    "              REG_loss_w = 0.01,\n",
    "              epochs = 100,\n",
    "              batch_size = 128\n",
    "              ):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Training the RAE on the data\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    encoder_inputs = keras.Input(shape=(num_dims,))\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"sigmoid\")(encoder_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"sigmoid\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"sigmoid\")(x)\n",
    "    encoder_output = layers.Dense(latent_dim, activation=\"sigmoid\")(x)\n",
    "    encoder = keras.Model(encoder_inputs, encoder_output, name=\"encoder\")\n",
    "\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(hidden_layer_n[2], activation=\"sigmoid\")(latent_inputs)\n",
    "    x = layers.Dense(hidden_layer_n[1], activation=\"sigmoid\")(x)\n",
    "    x = layers.Dense(hidden_layer_n[0], activation=\"sigmoid\")(x)\n",
    "    decoder_outputs = layers.Dense(num_dims, activation=\"linear\")(x)\n",
    "    decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "\n",
    "\n",
    "\n",
    "    class RAE(keras.Model):\n",
    "        def __init__(self, encoder, decoder, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "            self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "            self.reconstruction_loss_tracker = keras.metrics.Mean(\n",
    "                name=\"reconstruction_loss\"\n",
    "            )\n",
    "            self.z_tracker = keras.metrics.Mean(name=\"z_loss\")\n",
    "            self.REG_tracker = keras.metrics.Mean(name=\"REG_loss\")\n",
    "\n",
    "        @property\n",
    "        def metrics(self):\n",
    "            return [\n",
    "                self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.z_tracker,\n",
    "                self.REG_tracker,\n",
    "            ]\n",
    "\n",
    "        def train_step(self, data):\n",
    "            with tf.GradientTape(persistent=True) as tape:\n",
    "                z = self.encoder(data)\n",
    "                reconstruction = self.decoder(z)\n",
    "\n",
    "                reconstruction_loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)(data,reconstruction)\n",
    "\n",
    "                z_loss = K.mean(K.square(z), axis=[1])\n",
    "        \n",
    "                REG_loss = K.mean(K.square(K.gradients(K.square(reconstruction), z)))\n",
    "\n",
    "#                 z_loss_w = z_loss_w\n",
    "#                 REG_loss_w = REG_loss_w\n",
    "\n",
    "                total_loss = reconstruction_loss +  z_loss_w * z_loss + REG_loss_w * REG_loss\n",
    "            \n",
    "                grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "                self.total_loss_tracker.update_state(total_loss)\n",
    "                self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "                self.z_tracker.update_state(z_loss)\n",
    "                self.REG_tracker.update_state(REG_loss)\n",
    "                del tape\n",
    "                return {\n",
    "                    \"loss\": self.total_loss_tracker.result(),\n",
    "                    \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                    \"z_loss\": self.z_tracker.result(),\n",
    "                    \"REG_loss\": self.REG_tracker.result(),\n",
    "                }\n",
    "\n",
    "    tdata = np.concatenate([data], axis=0)\n",
    "    tdata = np.expand_dims(tdata, -1).astype(\"float32\")\n",
    "\n",
    "    rae = RAE(encoder, decoder)\n",
    "    rae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "    history = rae.fit(tdata, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "    return rae, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71fe87f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Calling GradientTape.gradient on a persistent tape inside its context is significantly less efficient than calling it outside the context (it causes the gradient ops to be recorded on the tape, leading to increased CPU and memory usage). Only call GradientTape.gradient inside the context if you actually want to trace the gradient in order to compute higher order derivatives.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 15ms/step - loss: 7912.1681 - reconstruction_loss: 7419.7925 - z_loss: 0.4465 - REG_loss: 0.6412\n",
      "Epoch 2/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 6213.7749 - reconstruction_loss: 5947.7300 - z_loss: 0.3995 - REG_loss: 0.0272\n",
      "Epoch 3/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 5298.3408 - reconstruction_loss: 5155.2500 - z_loss: 0.0089 - REG_loss: 7.8722e-04\n",
      "Epoch 4/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4785.3583 - reconstruction_loss: 4713.6729 - z_loss: 0.0012 - REG_loss: 5.4963e-06\n",
      "Epoch 5/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4501.2166 - reconstruction_loss: 4472.1777 - z_loss: 7.5252e-04 - REG_loss: 7.8590e-07\n",
      "Epoch 6/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4364.1955 - reconstruction_loss: 4344.2734 - z_loss: 5.2294e-04 - REG_loss: 2.2202e-07\n",
      "Epoch 7/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4282.6439 - reconstruction_loss: 4279.2915 - z_loss: 3.8610e-04 - REG_loss: 6.5873e-08\n",
      "Epoch 8/100\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 4245.0990 - reconstruction_loss: 4247.8467 - z_loss: 2.9494e-04 - REG_loss: 2.1884e-08\n",
      "Epoch 9/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4228.1067 - reconstruction_loss: 4233.0317 - z_loss: 2.3268e-04 - REG_loss: 7.6966e-09\n",
      "Epoch 10/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 4221.6448 - reconstruction_loss: 4226.4858 - z_loss: 1.8943e-04 - REG_loss: 2.2552e-09\n",
      "Epoch 11/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4225.0851 - reconstruction_loss: 4223.7080 - z_loss: 1.5823e-04 - REG_loss: 1.0859e-09\n",
      "Epoch 12/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4227.9332 - reconstruction_loss: 4222.7407 - z_loss: 1.3506e-04 - REG_loss: 3.9273e-10\n",
      "Epoch 13/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4207.9011 - reconstruction_loss: 4222.2812 - z_loss: 1.1744e-04 - REG_loss: 3.7157e-10\n",
      "Epoch 14/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4221.8726 - reconstruction_loss: 4222.1045 - z_loss: 1.0371e-04 - REG_loss: 3.4124e-10\n",
      "Epoch 15/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4222.8069 - reconstruction_loss: 4222.1421 - z_loss: 9.2803e-05 - REG_loss: 4.0912e-10\n",
      "Epoch 16/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4234.4811 - reconstruction_loss: 4222.1348 - z_loss: 8.3993e-05 - REG_loss: 9.9587e-11\n",
      "Epoch 17/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4225.5179 - reconstruction_loss: 4222.1606 - z_loss: 7.6771e-05 - REG_loss: 4.2446e-11\n",
      "Epoch 18/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 4207.2670 - reconstruction_loss: 4222.2007 - z_loss: 7.0751e-05 - REG_loss: 7.9835e-11\n",
      "Epoch 19/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4215.9826 - reconstruction_loss: 4222.1216 - z_loss: 6.5649e-05 - REG_loss: 1.7611e-10\n",
      "Epoch 20/100\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 4215.9179 - reconstruction_loss: 4222.2173 - z_loss: 6.1253e-05 - REG_loss: 1.9273e-10\n",
      "Epoch 21/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4214.8677 - reconstruction_loss: 4222.2354 - z_loss: 5.7411e-05 - REG_loss: 2.7156e-10\n",
      "Epoch 22/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4207.7560 - reconstruction_loss: 4222.1265 - z_loss: 5.4011e-05 - REG_loss: 1.3972e-10\n",
      "Epoch 23/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4215.9915 - reconstruction_loss: 4222.1011 - z_loss: 5.0970e-05 - REG_loss: 2.3743e-10\n",
      "Epoch 24/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4214.7004 - reconstruction_loss: 4222.2192 - z_loss: 4.8226e-05 - REG_loss: 2.0898e-10\n",
      "Epoch 25/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4234.0441 - reconstruction_loss: 4222.1357 - z_loss: 4.5732e-05 - REG_loss: 1.7551e-10\n",
      "Epoch 26/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4226.2037 - reconstruction_loss: 4222.2129 - z_loss: 4.3450e-05 - REG_loss: 1.3822e-10\n",
      "Epoch 27/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4233.5237 - reconstruction_loss: 4222.1699 - z_loss: 4.1351e-05 - REG_loss: 1.5167e-10\n",
      "Epoch 28/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4226.9728 - reconstruction_loss: 4222.2280 - z_loss: 3.9413e-05 - REG_loss: 3.2292e-10\n",
      "Epoch 29/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4219.4157 - reconstruction_loss: 4222.2104 - z_loss: 3.7614e-05 - REG_loss: 2.2866e-10\n",
      "Epoch 30/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4208.4465 - reconstruction_loss: 4222.4067 - z_loss: 3.5941e-05 - REG_loss: 9.9390e-11\n",
      "Epoch 31/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4219.9696 - reconstruction_loss: 4222.3887 - z_loss: 3.4378e-05 - REG_loss: 1.8257e-10\n",
      "Epoch 32/100\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 4209.2415 - reconstruction_loss: 4222.3760 - z_loss: 3.2916e-05 - REG_loss: 9.2182e-11\n",
      "Epoch 33/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4209.7395 - reconstruction_loss: 4222.4888 - z_loss: 3.1545e-05 - REG_loss: 1.9568e-10\n",
      "Epoch 34/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4225.4685 - reconstruction_loss: 4222.2842 - z_loss: 3.0255e-05 - REG_loss: 8.7875e-11\n",
      "Epoch 35/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4212.3744 - reconstruction_loss: 4222.2417 - z_loss: 2.9040e-05 - REG_loss: 1.3701e-10\n",
      "Epoch 36/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4186.8962 - reconstruction_loss: 4222.2988 - z_loss: 2.7894e-05 - REG_loss: 1.6640e-10\n",
      "Epoch 37/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4222.2013 - reconstruction_loss: 4222.3169 - z_loss: 2.6811e-05 - REG_loss: 1.7612e-10\n",
      "Epoch 38/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4217.0191 - reconstruction_loss: 4222.3726 - z_loss: 2.5786e-05 - REG_loss: 3.3189e-10\n",
      "Epoch 39/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4222.5250 - reconstruction_loss: 4222.3618 - z_loss: 2.4814e-05 - REG_loss: 1.2689e-10\n",
      "Epoch 40/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4231.4772 - reconstruction_loss: 4222.4868 - z_loss: 2.3891e-05 - REG_loss: 1.3667e-10\n",
      "Epoch 41/100\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 4220.7771 - reconstruction_loss: 4222.4478 - z_loss: 2.3015e-05 - REG_loss: 9.2456e-11\n",
      "Epoch 42/100\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 4224.4455 - reconstruction_loss: 4222.3989 - z_loss: 2.2181e-05 - REG_loss: 3.1833e-10\n",
      "Epoch 43/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4211.0012 - reconstruction_loss: 4222.3784 - z_loss: 2.1386e-05 - REG_loss: 3.2427e-10\n",
      "Epoch 44/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4227.7503 - reconstruction_loss: 4222.4536 - z_loss: 2.0628e-05 - REG_loss: 1.5842e-10\n",
      "Epoch 45/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4224.2164 - reconstruction_loss: 4222.4546 - z_loss: 1.9904e-05 - REG_loss: 2.9271e-10\n",
      "Epoch 46/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4213.8552 - reconstruction_loss: 4222.4463 - z_loss: 1.9213e-05 - REG_loss: 1.9921e-10\n",
      "Epoch 47/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4217.8110 - reconstruction_loss: 4222.3555 - z_loss: 1.8551e-05 - REG_loss: 2.9884e-10\n",
      "Epoch 48/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4209.1439 - reconstruction_loss: 4222.5024 - z_loss: 1.7918e-05 - REG_loss: 3.0997e-10\n",
      "Epoch 49/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4232.8043 - reconstruction_loss: 4222.4678 - z_loss: 1.7311e-05 - REG_loss: 5.2576e-10\n",
      "Epoch 50/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4237.9605 - reconstruction_loss: 4222.5615 - z_loss: 1.6729e-05 - REG_loss: 3.0663e-10\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 1s 17ms/step - loss: 4234.2116 - reconstruction_loss: 4222.3110 - z_loss: 1.6170e-05 - REG_loss: 2.6557e-10\n",
      "Epoch 52/100\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 4207.4328 - reconstruction_loss: 4222.4355 - z_loss: 1.5634e-05 - REG_loss: 2.1744e-10\n",
      "Epoch 53/100\n",
      "56/56 [==============================] - 1s 16ms/step - loss: 4224.6712 - reconstruction_loss: 4222.5254 - z_loss: 1.5119e-05 - REG_loss: 9.7779e-10\n",
      "Epoch 54/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4216.4795 - reconstruction_loss: 4222.6304 - z_loss: 1.4623e-05 - REG_loss: 3.3339e-10\n",
      "Epoch 55/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4215.0980 - reconstruction_loss: 4222.4893 - z_loss: 1.4147e-05 - REG_loss: 3.6504e-10\n",
      "Epoch 56/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4209.8094 - reconstruction_loss: 4222.6201 - z_loss: 1.3689e-05 - REG_loss: 5.4081e-10\n",
      "Epoch 57/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4215.2367 - reconstruction_loss: 4222.4712 - z_loss: 1.3247e-05 - REG_loss: 1.1305e-09\n",
      "Epoch 58/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4214.2540 - reconstruction_loss: 4222.5171 - z_loss: 1.2823e-05 - REG_loss: 5.0785e-10\n",
      "Epoch 59/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4216.0342 - reconstruction_loss: 4222.4229 - z_loss: 1.2413e-05 - REG_loss: 2.3979e-10\n",
      "Epoch 60/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4231.7018 - reconstruction_loss: 4222.4590 - z_loss: 1.2019e-05 - REG_loss: 1.3957e-10\n",
      "Epoch 61/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4219.5268 - reconstruction_loss: 4222.5659 - z_loss: 1.1639e-05 - REG_loss: 4.3759e-10\n",
      "Epoch 62/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4216.6378 - reconstruction_loss: 4222.4922 - z_loss: 1.1272e-05 - REG_loss: 2.0325e-10\n",
      "Epoch 63/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4211.9465 - reconstruction_loss: 4222.5010 - z_loss: 1.0919e-05 - REG_loss: 6.1736e-10\n",
      "Epoch 64/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4233.0116 - reconstruction_loss: 4222.6489 - z_loss: 1.0578e-05 - REG_loss: 3.3275e-10\n",
      "Epoch 65/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4229.2722 - reconstruction_loss: 4222.6602 - z_loss: 1.0249e-05 - REG_loss: 4.0616e-10\n",
      "Epoch 66/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4229.8719 - reconstruction_loss: 4222.5962 - z_loss: 9.9317e-06 - REG_loss: 1.3463e-09\n",
      "Epoch 67/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4230.0115 - reconstruction_loss: 4222.5684 - z_loss: 9.6253e-06 - REG_loss: 5.4259e-10\n",
      "Epoch 68/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4223.4386 - reconstruction_loss: 4222.5024 - z_loss: 9.3295e-06 - REG_loss: 2.9523e-10\n",
      "Epoch 69/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4231.3617 - reconstruction_loss: 4222.8530 - z_loss: 9.0438e-06 - REG_loss: 1.0009e-09\n",
      "Epoch 70/100\n",
      "56/56 [==============================] - 2s 27ms/step - loss: 4216.4641 - reconstruction_loss: 4222.6675 - z_loss: 8.7679e-06 - REG_loss: 1.4816e-09\n",
      "Epoch 71/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 4221.6247 - reconstruction_loss: 4222.7163 - z_loss: 8.5013e-06 - REG_loss: 6.4476e-10\n",
      "Epoch 72/100\n",
      "56/56 [==============================] - 1s 22ms/step - loss: 4218.0005 - reconstruction_loss: 4222.6626 - z_loss: 8.2436e-06 - REG_loss: 7.8407e-10\n",
      "Epoch 73/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4202.5125 - reconstruction_loss: 4222.7466 - z_loss: 7.9945e-06 - REG_loss: 7.3239e-10\n",
      "Epoch 74/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4215.7393 - reconstruction_loss: 4222.5762 - z_loss: 7.7537e-06 - REG_loss: 7.8116e-10\n",
      "Epoch 75/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4212.5025 - reconstruction_loss: 4222.6904 - z_loss: 7.5208e-06 - REG_loss: 7.4874e-10\n",
      "Epoch 76/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4229.3981 - reconstruction_loss: 4222.6084 - z_loss: 7.2956e-06 - REG_loss: 1.6249e-09\n",
      "Epoch 77/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4222.6709 - reconstruction_loss: 4222.7466 - z_loss: 7.0776e-06 - REG_loss: 7.3516e-10\n",
      "Epoch 78/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4222.2865 - reconstruction_loss: 4222.7344 - z_loss: 6.8668e-06 - REG_loss: 5.2121e-10\n",
      "Epoch 79/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4216.2536 - reconstruction_loss: 4222.7886 - z_loss: 6.6627e-06 - REG_loss: 9.5411e-10\n",
      "Epoch 80/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4236.0357 - reconstruction_loss: 4222.5483 - z_loss: 6.4652e-06 - REG_loss: 3.5074e-10\n",
      "Epoch 81/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4211.3460 - reconstruction_loss: 4222.6089 - z_loss: 6.2740e-06 - REG_loss: 1.1595e-09\n",
      "Epoch 82/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4229.8147 - reconstruction_loss: 4222.5518 - z_loss: 6.0888e-06 - REG_loss: 7.5780e-10\n",
      "Epoch 83/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4218.4133 - reconstruction_loss: 4222.7212 - z_loss: 5.9095e-06 - REG_loss: 1.1408e-09\n",
      "Epoch 84/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4225.1416 - reconstruction_loss: 4222.8335 - z_loss: 5.7358e-06 - REG_loss: 5.7384e-10\n",
      "Epoch 85/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4216.9651 - reconstruction_loss: 4222.3608 - z_loss: 5.5675e-06 - REG_loss: 1.0089e-09\n",
      "Epoch 86/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4221.4346 - reconstruction_loss: 4222.3604 - z_loss: 5.4045e-06 - REG_loss: 1.6390e-09\n",
      "Epoch 87/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4227.4574 - reconstruction_loss: 4222.5137 - z_loss: 5.2465e-06 - REG_loss: 9.5580e-10\n",
      "Epoch 88/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4222.3206 - reconstruction_loss: 4222.6064 - z_loss: 5.0934e-06 - REG_loss: 4.2850e-09\n",
      "Epoch 89/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4244.0781 - reconstruction_loss: 4222.4702 - z_loss: 4.9450e-06 - REG_loss: 7.3590e-09\n",
      "Epoch 90/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4231.9064 - reconstruction_loss: 4222.8647 - z_loss: 4.8012e-06 - REG_loss: 2.1222e-09\n",
      "Epoch 91/100\n",
      "56/56 [==============================] - 1s 17ms/step - loss: 4221.4753 - reconstruction_loss: 4222.7231 - z_loss: 4.6617e-06 - REG_loss: 4.1820e-09\n",
      "Epoch 92/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 4220.0779 - reconstruction_loss: 4222.7178 - z_loss: 4.5265e-06 - REG_loss: 6.3388e-09\n",
      "Epoch 93/100\n",
      "56/56 [==============================] - 1s 21ms/step - loss: 4232.1535 - reconstruction_loss: 4222.6021 - z_loss: 4.3954e-06 - REG_loss: 1.9770e-09\n",
      "Epoch 94/100\n",
      "56/56 [==============================] - 1s 24ms/step - loss: 4208.1411 - reconstruction_loss: 4222.4185 - z_loss: 4.2683e-06 - REG_loss: 3.6814e-09\n",
      "Epoch 95/100\n",
      "56/56 [==============================] - 1s 22ms/step - loss: 4221.0838 - reconstruction_loss: 4222.5977 - z_loss: 4.1450e-06 - REG_loss: 2.5036e-09\n",
      "Epoch 96/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4247.1386 - reconstruction_loss: 4222.6465 - z_loss: 4.0254e-06 - REG_loss: 4.8189e-09\n",
      "Epoch 97/100\n",
      "56/56 [==============================] - 1s 19ms/step - loss: 4223.7990 - reconstruction_loss: 4222.6357 - z_loss: 3.9093e-06 - REG_loss: 2.0937e-09\n",
      "Epoch 98/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4222.5297 - reconstruction_loss: 4222.7598 - z_loss: 3.7968e-06 - REG_loss: 4.2847e-09\n",
      "Epoch 99/100\n",
      "56/56 [==============================] - 1s 20ms/step - loss: 4225.5227 - reconstruction_loss: 4222.6484 - z_loss: 3.6876e-06 - REG_loss: 6.0372e-09\n",
      "Epoch 100/100\n",
      "56/56 [==============================] - 1s 18ms/step - loss: 4242.2370 - reconstruction_loss: 4222.6528 - z_loss: 3.5816e-06 - REG_loss: 3.3328e-09\n"
     ]
    }
   ],
   "source": [
    "rae, history = train_RAE(data, num_dims=784, hidden_layer_n=[512, 256, 128], z_loss_w = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "674da7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = data[-1, :].reshape([1, 784])\n",
    "\n",
    "z = rae.encoder(sample.astype('float32'))\n",
    "reconstruction = rae.decoder(z)\n",
    "\n",
    "reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample.astype('float32'), reconstruction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a1faa92a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6912.768"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57b5c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rae_detect_outliers(data,\n",
    "                        rae_model,\n",
    "                        num_dims\n",
    "                        ):\n",
    "\n",
    "    data_mean = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "\n",
    "        z = rae_model.encoder(sample)\n",
    "        reconstruction = rae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample, reconstruction)\n",
    "        \n",
    "        data_mean.append(reconstruction_loss)\n",
    "    \n",
    "    data_mean = np.array(data_mean)\n",
    "    data_std = np.std(data_mean)\n",
    "\n",
    "    threshold = data_mean + 3*data_std\n",
    "\n",
    "    classes = []\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "            \n",
    "        sample = data[i,:].reshape([1,num_dims])\n",
    "        sample = sample.astype('float32')\n",
    "        \n",
    "        z = rae_model.encoder(sample)\n",
    "        reconstruction = rae_model.decoder(z)\n",
    "\n",
    "        reconstruction_loss = tf.keras.losses.MeanSquaredError()(sample, reconstruction)\n",
    "        \n",
    "        if any(reconstruction_loss > threshold):\n",
    "            \n",
    "            classes.append(1)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            classes.append(0)\n",
    "\n",
    "    classes = np.array(classes)\n",
    "\n",
    "\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c955089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = rae_detect_outliers(data, rae, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149a70d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "# sklearn.metrics.confusion_matrix(lables, 1-classes, labels=None, sample_weight=None, normalize=None)\n",
    "true_labels = 1 - lables\n",
    "predicted_labels = classes\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Normalize the confusion matrix to get percentages\n",
    "cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"Confusion Matrix (Percentage):\")\n",
    "print(cm_percentage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
